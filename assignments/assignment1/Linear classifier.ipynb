{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Test batch_size = 3\n",
      "Gradients are different at (0, 0). Analytic: 0.68145, Numeric: 0.22715\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "print('Test batch_size = 3')\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are different at (0, 0). Analytic: -0.88080, Numeric: -0.44040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 8.629879\n",
      "Epoch 1, loss: 14.302076\n",
      "Epoch 2, loss: 12.489537\n",
      "Epoch 3, loss: 18.152534\n",
      "Epoch 4, loss: 14.948022\n",
      "Epoch 5, loss: 14.628770\n",
      "Epoch 6, loss: 14.686721\n",
      "Epoch 7, loss: 16.201681\n",
      "Epoch 8, loss: 15.280018\n",
      "Epoch 9, loss: 18.200110\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22a5908aee0>]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5b0/8M93ZrLvKyEJZCELm4CQsAokUiwura1tXaoWlYparVqrvXq1ffX+2mt7W0XrtdZqRVxaWrXU21arRZyAIogBZJ9JwhKSsMxk35eZeX5/JGCAhCzMzJkz83m/Xr5ITk7O+TKST555zrOIUgpERKQ/Bq0LICKi0WGAExHpFAOciEinGOBERDrFACci0imTN2+WmJioMjMzvXlLIiLd2759e61SKuns414N8MzMTJSWlnrzlkREuicilQMdZxcKEZFOMcCJiHSKAU5EpFMMcCIinWKAExHpFAOciEinGOBERDrFACci8qDOHid++vd9ONbY4fZrM8BpVFwuhde3VqKhrVvrUoh82ksfH8aaT46gsq7d7dceMsBFZLWI2ERkb79jM0Rkq4h8LiKlIjLb7ZWRTyutbMBjb+/Fr963aF0Kkc+yt3ThOXMFlk4eg3kTEtx+/eG0wNcAWHbWsV8B+C+l1AwAP+n7nAKI2WoDALxZWo2jHmhZEPmDpz4oQ5fDhUcun+iR6w8Z4EqpTQDqzz4MILrv4xgAx9xcF/k4s8WGiSlRMBoEv9lQrnU5RD7HeqIFf952FDfNzUB2UqRH7jHaPvD7AfxaRKoAPAHgkcFOFJGVfd0spXa7fZS3I19yvKkDlhMtuGZmGm6em4G/7azGQXur1mUR+ZTH3z2AyBAT7luS67F7jDbA7wLwA6XUOAA/APDSYCcqpV5QShUopQqSks5ZDZF0qMTa+4u4OD8ZdxZNQGiQEc+wFU502sYyOzaW2XHvklzERQR77D6jDfDlANb1ffwmAD7EDCBmiw1psWHISY5EYmQIls/PxN93HUPZyRatSyPSnMPpwn+/sx8ZCeG4eV6GR+812gA/BmBx38eXAmDzK0B0OZzYXFGL4olJEBEAwMqF2YgINuHpD8o0ro5Ie2+UVqPsZCseXjYRISajR+81nGGEawFsAZAvItUisgLA7QCeFJFdAB4HsNKjVZLPKD3SgLZuJ4rzk08fi4sIxm0LMvHunhPYd6xJw+qItNXS2YNV660ozIzDsqkpHr/fcEah3KCUGquUClJKpSulXlJKfayUmqWUmq6UmqOU2u7xSsknmC02BJsM54xpXbEwG9GhJjy1nm/GKHA9v/Egalu78diVk0+/Q/UkzsSkETFbbZibnYDw4DN344sJC8LtC7PxwYGT2F3dqFF1RNqpaezAHz46jK/NSMX0cbFeuScDnIbtaF07DtrbUJw/8GiiWy/JQmx4EFatZ184BZ5fv9c7K/mhZZ6ZtDMQBjgNW0lZ7+zL/v3f/UWGmHDHogkosdqxvbLBm6URaerzqka8/fkxfHdhFtJiw7x2XwY4DZvZYkNWYgQyEyMGPWf5/AwkRgZj1XqrFysj0o5SCv/9zn4kRgbjrqIcr96bAU7D0tnjxCcH61A0SPfJKeHBJty5eAI2V9Rh66E6L1VHpJ339p7AZ0ca8MDSfESGmIb+BjdigNOwbDlUhy6Ha9Duk/5umpuBMdEhWLW+DEopL1RHpI0uhxO/fM+CvDGRuLYg3ev3Z4DTsJRYbAgLMmJ2VvyQ54YGGXF3cQ62Ha7H5gq2wsl/vbalEpV17Xj0yskwGb0fpwxwGpJSCmarHQtyEhAaNLyZZdcVjkNqTCieXG9lK5z8UkNbN57ZUI7FeUlYnKfNOk8McBrSodo2HK1vR9Ewuk9OCTEZcc+ludh5tPH04ldE/uQ3G8rR2uXAo1dO0qwGBjgNyWzpHT441APMs32rIB3j4sPYF05+55C9Fa9vrcT1s8cjb0yUZnUwwGlIJVY78sZEIj0ufETfF2Q04N5Lc7Gnpgnr95/0UHVE3veLf1kQYjLgB1/K07QOBjidV1uXA58erhvW6JOBfP3iNGQlRmDV+jK4XGyFk/5tOViH9ftP4nvFOUiKCtG0FgY4ndfmilr0ONWI+r/7MxkNuG9JLiwnWvCvvSfcXB2Rd7lcCj9/Zz/SYsOw4pIsrcthgNP5ma02RIaYUJAZN+prfGV6KnKTI/HUB2VwshVOOrZuZw32HWvGj5blD3tElicxwGlQSimYLXYszE1E0AWMcTUaBPd/KQ8Vtlb8Yxf3vyZ9au924In3rZieHoOvTEvVuhwADHA6D8uJFpxo7hx1/3d/l09NwcSUKPxmQzkcTpcbqiPyrhc3HcaJ5k48dtVkGAyeX+t7OBjgNCiztXf44OIRDh8ciMEgeGBpHg7XtuFvO2su+HpE3nSyuRPPbzyIy6emoDBz6NnI3sIAp0GVWOyYkhqNMdGhbrne0sljcFFaDJ75sBw9bIWTjjz5byscLhcevtx7a30PBwOcBtTU3oPtRxvc0n1yikhvK7yqvgNvlla77bpEnrTvWBPe3F6NW+ZnIiNh8KWUtcAApwF9VGGH06VQPNG9azwU5Sfh4vGxePbDcnQ5nG69NpG79a71fQAxYUG4pzhX63LOwQCnAZktdsSGB2HGuNEPHxyIiOCHS/NxrKkTf/msyq3XJnK3Dy02fHKwDvcvyUVMeJDW5ZyDAU7ncLkUNpbZsCg3CUYPPG1fkJOA2VnxePbDCnT2sBVOvqnH6cLj7x5AdmIEbpyboXU5A2KA0zn2HmtCbWu327tPTjnVF25r6cLrWys9cg+iC7V221EctLfhkSsmXdA8CE/yzapIU2aLHSLAolzPrXE8NzsBC3IS8PzGg2jvdnjsPkSj0dTRg6c/KMfc7Hh8aZL7HuS7GwOczmG22jA9PRYJkZ5dqOeBpfmobe3GK5+wFU6+5TlzBRrau/HYlZMh4huTdgbCAKcz1LV2YVd1o1uHDw5mVkYcivKT8PtNB9HS2ePx+xENR1V9O17efATXXJyOqWkxWpdzXgxwOsOmcjuUgsf6v8/2wNI8NLb3YM3mI165H9FQfvmeBQYD8NCX87UuZUgMcDqD2WJHYmQwpqZ6p+UxLT0WX5o0Bi9+dAhNHWyFk7a2V9bjnd3HcceiCUiJcc8MZE9igNNpTpfCxjI7Fucle3WxngeW5qG504GXPjrktXsSnU0phZ/98wCSo0Jwx+JsrcsZFgY4nfZ5VQOaOnq81n1yyuTUaFxxUQpWbz6ChrZur96b6JR/7D6Oz6sa8eCX8xEebNK6nGFhgNNpZosdRoNgYY53AxwA7v9SHtq6HXiBrXC/YGvpRLOOHkx39jjxP/+yYNLYaHxjZrrW5QwbA5xOM1ttmDU+TpMpw3ljovDV6alYs/kIalu7vH5/cp8tB+tw6RMbsfB/zHjp48Podvj+ypMvbz6CmsYOPHblJI/MPvYUBjgB6F3veN+xZhR5ufukv3uX5KLL4cTzJQc1q4EuzPr9J7H85W0YGxOKaekx+Nk/9+Oypzbi/X0noJRvbqdX29qF58wVWDIxGQtyErUuZ0SGDHARWS0iNhHZe9bx74uIVUT2icivPFciecNGqx0AvDL+ezATkiLx9YvT8drWSpxs7tSsDhqdv26vxp2vb8eksdF44455eG3FHKy5tRBBRgPueG07bnhxK/bWNGld5jme/qAM7T1OPHLFJK1LGbHhtMDXAFjW/4CIFAO4GsA0pdQUAE+4vzTyJrPVhpToUExMidK0jvuW5MLpUnjOXKFpHTQyf/joEH745i7MzY7Hn747B3ERwQCAovxk/Ou+hfjZ16ai7GQrvvLsx3jwzV040eQbv6DLT7Zg7bYq3DhnPHKSI7UuZ8SGDHCl1CYA9WcdvgvAL5VSXX3n2DxQG3lJj9OFj8prUTwxSfNpw+MTwvGtgnSs3VaFY40dmtZCQ1NK4Yn3rfj5Owdw+dQUrL6lEBEhZ47gMBkNuHluBkoeKsLKRdn4++fHUPxESW/LV+N1cB5/9wDCg424b4nvrfU9HKPtA88DsFBEPhWRjSJSONiJIrJSREpFpNRut4/yduRJpUca0NrlQJGG3Sf93XNpLhQUnmUr3Kc5XQo//r+9eNZcgesLx+HZb89EiMk46PnRoUF45PJJ2PDDxbh0UjKe/qAcxU+U4K3t1XC5vN8//lG5HWarHfcU53h83R9PGW2AmwDEAZgL4CEAb8ggTTel1AtKqQKlVEFSknYPyGhwJVYbgoziMw9w0mLDcH3heLzxWRWq6tu1LocG0O1w4b4/78TrW4/izsUT8ItrLhr26I1x8eH47bdn4q93zUNKTBgefHMXvvrbj7H1UJ2Hq/6C09W70864+DAsn5/ptfu622gDvBrAOtVrGwAXAN/46acRM1ttmJ0Vj8gQ35m8cHdxDgwGwTMbyrUuhc7S3u3A7a+W4p+7j+Phyyfi4csnjqrrbVZGPP5213z85voZqG/txvUvbMUdr5XicG2bB6o+05ulVbCcaMHDyyYhNGjwdw2+brQB/jaASwFARPIABAOodVdR5D3VDe0oO9mq6eiTgaTEhOKmORlYt7PGKz/QNDxN7T24+aVt+Kjcjv/5xkW4c/GEC7qewSC4ekYaPnywCA99OR8fl9fisqc24mf/3I+mds9MBGrtcuDJ9WWYlRGHKy5K8cg9vGU4wwjXAtgCIF9EqkVkBYDVALL7hhb+GcBy5auDPOm8Sk4NH5zoWwEOAHcVTUCw0cBWuI+wNXfi2t9vwZ7qJjx340xcVzjebdcODTLi7uIcmB8qwjdnpePlzYex+AkzXt58GD1O904E+v3Gg7C3dOHRKydp/tD+Qg1nFMoNSqmxSqkgpVS6UuolpVS3UuompdRUpdRMpdSH3iiW3K/EasP4+HBkJ0ZoXco5kqJC8J35GXj78xpU2Fq0LiegVda14RvPf4KqhnasvqUQy6aO9ch9kqNC8YtrpuGdexdiamoM/usf+/HlpzZh/f6TbpkIdKyxAy9+dAhfmZ6KmePdu2G3FjgTM4B19jixuaIOxfnaDx8czB2LJiA8yIinPmArXCsHjjfjm89vQWunA3+6fS4uyfX8465JY6Px2orZePmWQogAt79aim+/+Cn2HbuwiUBPvG+FSwE/0sFa38PBAA9g2w7Xo6PHiSIf7D45JT4iGLcuyMI7u4/jwPFmrcsJONsr63Hd77fAKII37piHGeNivXZvEUHxxGS8d/8i/OzqKbCcaMZV//sxfvTWrlHN1N1d3Yh1O2tw24IsjIsP90DF3scAD2Bmqw0hJgPmZSdoXcp53b4wG1GhJjy1vkzrUgJKidWGG//wKRIiQ/DWXfOQO0abWbpBRgNunpeJkoeKsXJhNt7e2TsR6JkN5ejodg7rGkop/PydA0iICMb3ii/swasvYYAHsBKrHfMnJPj8MKqY8CB895Js/Hv/Seyp9r21NPzR33cdw3dfKcWEpEi8eec8pMdp32KNCQvCI1dMwvoHFqEoPwmr1peh+IkSrNsx9ESgf+8/iW2H63H/0jxEh3p/tU1PYYAHqMO1bThc2+aTo08GctslmYgJC8JTH7AV7mmvba3EfX/eiZkZcVi7ci4SfWyWYkZCBJ67cRbevHMekqND8MAbu3D1bzfj00EmAnU7XPjFuweQkxyJGwrHeblaz2KAB6gSa+/yNUV5+gjwqNAgrFyUjQ8tNuw42qB1OX5JKYX/3VCOH7+9F0smJuPV22b7dGu1MDMeb39vAZ6+bgZqW7tw3Qtbcedr23HkrHkDr22txJG6djx65SSYjP4Vef71t6FhM1vtmJAUgfEJ2r81Hq5b5mciISKYfeEe4HL17gf55PoyXHNxGn530yyf71oDeicCfe3iNHz4wyI8eFkeNpXbsfSpjfh530SgxvZuPLOhHAtzE1GU539LefjO3GnymvZuB7YeqsN35mZoXcqIRISYcOfiCfjvdw9g2+F6zM6K17okv+BwuvCjv+7Guh01uHVBJn585WSvbmrtDmHBRtxzaS6uLRiHVevL8NLmw3hrRzUmpkShpbPHLybtDIQt8AC05WAduh0u3fR/93fT3AwkRYVg1Xqr1qX4hc4eJ+58fQfW7ajBA0vz8JOr9Bfe/SVHh+KX35iGd76/EFNSo7H1UD2uLRiHiSnRWpfmEWyBByCz1YaIYCMKMvU3Ey0s2Ii7iybgp//Yj08qajHfR1ZQ1KPmzh7c/kopth2px8+unoKb52VqXZLbTE6Nxusr5mBPTRPyNBr+6A1sgQcYpRTMFjsW5CSed+1mX3b97PEYGxOKJ9eX+ew+i76utrULN7ywFdsrG/D0dTP8KrxPERFMS4/VRV/+aDHAA0yFrRU1jR267D455dTCR9srG7CxjJuEjFR1QzuufX4LDtpb8eLyAlw9I03rkmiUGOABxnxq+GC+vp/IX1swDulxYVjFVviIVNha8K3nt6C2tQuvr5jjc8sI08gwwAOM2WLHxJQojI0J07qUCxJsMuDeS3Oxu7oJGw5wS9bh2FXViG89vwU9ToW/3DEPBZkcxaN3DPAA0tLZg8+O1Ou6+6S/a2amISMhHE+uL8Ph2jZN9lXUi80Vtfj2i1sRGWrCX++ah0lj/XNURqDhKJQAsrmiFg6X8pu3zSajAQ9elo/vr92J4idKEBliwuTUaExNjcHUtGhMTYtBdmKE382+G6n39p7AvWt3IisxAq+umI0x0aFal0RuwgAPIGaLHVGhJswc770lQT3tK9NTkTcmCp9XNWBvTTP2HWvCn7ZVorOndxeX0CADJo39ItSnpMYgb0wUgk2BEepvfFaFh9ftxoxxsXj5ltmICffdqfE0cgzwAKGUgtlqw6K8JL9rkeanRCE/JQrXFfZ+7nC6cLi2DXuPNWFvTTP21jTh7Z01eG1rJQAgyCjIGxP1RainxWBSSjTCgv1ruNkLmw7i8XctWJSXhOdvmonwYP64+xv+Hw0Q+483w9bS5TfdJ+djMhqQOyYKuWOi8PWLe4+5XApH69tPh/q+Y0349/4T+EtpFQDAIEBOciSmpsZgSloMpqZGY3JqNKJ8eDGnwSil8Kv3rfhdyUFcNW0sVl07I2DecQQaBniAOLV58WI/XNBnOAwGQWZiBDITI3DVtFQAvUF3vKkTe2uasPdYM/bVNGHzwVqs21lz+vsyE8L7Av2LLpj4iGCt/hpDcroUHnt7D9Zuq8KNc8bj/109FUYdT42n82OABwizxYZp6TFIivKttZ21JCJIjQ1DamwYLpuScvq4raUT+/oCfW9NM3ZVNeKd3cdPfz0tNgxTUnsfkk5N6+1fTx7iwaBSCl0OFzp7nOjs6fvT0e/j0//1+/ys87vOOd91+hpdfcfaup1o6ujBPcU5+OFleX65gBN9gQEeABrbu7HjaAPuuTRX61J0ITkqFMn5oWd0NzW2d2P/seYv+tWPNWH9gZM4NYcoKSoEmQnh6Ha4+gVr//B1jbqeYKMBIUEGhAYZERpkQKjJePrjyBATEiL6jgcZERZkxKyMOHztYs6uDAQM8ACwscwOlwKKdT77Ukux4cGYn5N4xuJZrV0OHDje+5B0b00zahrbERsefDpMe4O29+OQAcI3LMjY97Vzzz91TojJyC4QGhQDPACUWO2IjwjGtHT/GT7oCyJDTCjMjEchZzSSRvho2s85XQoby+xYnJfElhyRn2GA+7nd1Y2ob+vW/eJVRHQuBrifM1vtMAiwKJcBTuRvGOB+rsRqw8Xj4xDnw2OXiWh0GOAj8Mi6PbjjtVJ0OZxalzIs9pYu7K5u4ugTIj/FAB+m9m4H/rq9Gu/vO4kH3tili6VLT+1WUxQA0+eJAhEDfJg+qahDt9OFZVNS8M7u4/ivf+zz+Z1gzFYbkqNCMCWVaz8T+SMG+DCVlPXu5P6bG2ZgxSVZeGVLJZ4rOah1WYNyOF3YVGZHUX4Sp1MT+akhA1xEVouITUT2DvC1B0VEiUjiQN/rL87eyf3RKybh6hmp+PX7VrzxWZXW5Q1ox9FGtHQ6AmL1QaJANZwW+BoAy84+KCLjACwFcNTNNfmcs3dyNxgEv/7mdCzMTcQjf9uDDQdOalzhucxWG0wGwYJcv/7dShTQhgxwpdQmAPUDfOkpAD8C4NsdwW4w0E7uwSYDfnfTLEweG427/7QD2ysHeom0Y7bYUJAZh2gdrmdNRMMzqj5wEfkqgBql1K5hnLtSREpFpNRut4/mdpobbCf3yBATXr61ECnRobhtTSnKT7ZoVOGZjjd1wHKihd0nRH5uxAEuIuEAHgXwk+Gcr5R6QSlVoJQqSErS33jkUzu5DzYULzEyBK/eNgdBRgO+s3objjd1eLnCc53avMFfdp8nooGNpgU+AUAWgF0icgRAOoAdIpJy3u/SqS92ch/8l8/4hHCsubUQLZ0OLF+9DU3tPV6s8Fxmiw1psWHITY7UtA4i8qwRB7hSao9SKlkplamUygRQDWCmUuqE26vzAad3cs+IO+95U9Ni8MLNs3Ckth3fffUzdPZoM1uzy+HE5opaDh8kCgDDGUa4FsAWAPkiUi0iKzxflm84vZN7bhKChrGT+/ycRKy6bjpKKxtwz592wuEc/S4so1V6pAFt3U72fxMFgCE3dFBK3TDE1zPdVo2PObWT+0iWYr1qWipqW7rw03/sx2Nv78UvrrnIqy1hs8WGYKMB83MSvHZPItIGd+Q5j9M7uY9wMahbFmTB3tqF35oPIjkqBA9clu+J8gZkttowJzse4cH8X0vk7/hTfh4lVhsuSotBctT5dxwfyIOX5cPe0oVnPqxAUnQobp6b4YEKz3S0rh0H7W24cY7n70VE2uNaKINoau/B9sqGUS/FKiJ4/OsXYcnEZPzk//bi3T3H3VzhuUrKeicccfggUWBggA9iU3nvTu5FFxCGJqMBz357Ji4eF4v7//w5thysc2OF5zJbbMhMCEdWYoRH70NEvoEBPgiz1Ya48CBMv8Cd3MOCjVh9SyHGJ4Rj5aul2H+s2U0Vnqmzx4lPDtZx7W+iAMIAH4DLpbDRasciN+3kHhsejFdvm42IEBOWv7wNVfXtbqjyTFsO1aHL4WL3CVEAYYAPYE9NE+raut06ljo1NgyvrpiNrh4nlq/ehrrWLrddGwBKLDaEBhkwJyverdclIt/FAB+A2WqDCLAoz71rt+SNicJLtxSiprEDt635DG1dDrdct3fCkR0LJiQiNMjolmsSke9jgA/AbLVjxrhYxHtgJ/fCzHg8++2Z2FPThLv+uAM9bpiteai2DUfr2y/ogSsR6Q8D/Cx1rV3YXd3o0anoSyePweNfvwibyuz4j7d2X/AGyWZL33rlbn7HQES+jRN5zrKp3A6l4PG1RK6fPR62li6sWl+GpKgQPHLFpFFfq8RqR25yJMbFh7uxQiLydQzws5gtdiRGemcn9+9fmgN7Sxd+v+kQkqJC8N2F2SO+RluXA58ersOtC7I8UCER+TIGeD9Ol8LGMjuWTh4DgxuGDw5FRPDTr05BXVsXfv7OASRGhuBrF6eN6BqbK2rR41QjWnCLiPwD+8D7+byqAU0dPV5ditVoEKy6dgbmZMXjwTd3YVPZyLadM1vtiAwxoTCTwweJAg0DvB+zxQ6jQXCJl3dyDw0y4sXlBchJjsSdr2/HrqrGYX2fUgolVhsW5iYOa71yIvIv/Knvx2y1Ydb4OMSEeX8n9+jQILxy22zERwTj1jWf4XBt25DfYz3ZguNNndy8gShAMcD7nGzuxL5jzSiaqF1f8pjoULx622wAwHdWfwpbS+d5zzdbRrdeORH5BwZ4n42ndnLXuDWbnRSJ1bcUoralG7es/gwtnYNvkGy22jAlNRpjoke+XjkR6R8DvI/ZakNKdCgmpkRpXQpmjIvF726aibKTLVj56nZ0Oc7dILmp49R65ew+IQpUDHAAPU4XPi6vRfFE39nJvSg/Gb/65jRsOVSHB/6yC86zZmt+XF4Lp0uhWMMuHyLSFseBA9he2YCWLofPraV9zcx01LZ24fF3LUiMDMZPvzrl9C8Ys9WG2PAgzBgXp3GVRKQVBjh6wzDIKFiQ493hg8OxctEE2Jq78IePDyM5OhR3F+fA5VIosdqxKNc965UTkT4xwAGUWOyYnRWPyBDffDn+84pJqG3twq/ftyIxMhiTx8agtrWL3SdEAc43E8uLaho7YD3Zgm8VjH4xKU8zGAS/+uZ01LV145F1ezBvQkLveuW5DHCiQBbwDzFLrH1Lsfr4WOpgkwHP3zQLU9NisLmiDtPTY5EQGaJ1WUSkoYAPcLPFjvS4MExIitS6lCFFhJiw+pZCzM6Kx3fmZWhdDhFpLKC7ULocTmyuqMU3Z6X7zPDBoSRGhuCNO+ZpXQYR+YCAboFvO1yPjh4nHwYSkS4FdICbLXYEmwyYl+17wweJiIYS0AFeYrVhXnYCwoK5kzsR6U/ABnhlXRsO1bah2MdHnxARDSZgA7ykb/VBX5s+T0Q0XEMGuIisFhGbiOztd+zXImIRkd0i8jcRifVsme5nttqQnRiBzMQIrUshIhqV4bTA1wBYdtax9QCmKqWmASgD8Iib6/Kojm4nthysY+ubiHRtyABXSm0CUH/WsX8rpRx9n24FkO6B2jxm66E6dDlcPj/7kojofNzRB34bgH8N9kURWSkipSJSarePbMd1TzFbbQgLMmJ2FndyJyL9uqAAF5FHATgA/HGwc5RSLyilCpRSBUlJ2rd4lVL40GLDgpwEhAZx+CAR6deoA1xElgO4CsCNSik11Pm+4qC9DdUNHez/JiLdG9VaKCKyDMB/AFislGp3b0mepZfVB4mIhjKcYYRrAWwBkC8i1SKyAsCzAKIArBeRz0XkeQ/X6TYlVjvyxkQiPS5c61KIiC7IkC1wpdQNAxx+yQO1eFxblwOfHq7DbQuytC6FiOiCBdRMzM0VtehxKvZ/E5FfCKgAN1vtiAwxoSCTO7kTkf4FTIArpVBiteGSnEQEGQPmr01Efixgksx6sgXHmzq5eQMR+Y2ACXCzhasPEpF/CZwAt9oweWw0xkSHal0KEZFbBESAN3X0YHtlA7tPiMivBESAf1xeC6dLoZjdJ0TkRwIiwEusNvotVH4AAAedSURBVMSEBWHGON3tO0FENCi/D3CXS6GkzI5FeUkwcfggEfkRv0+0/cebYW/p4ubFROR3/D7AzRYbRIBFeQxwIvIv/h/gVhumpcUgMTJE61KIiNzKrwO8vq0bO6saOXmHiPySXwf4R+V2KAUUT2SAE5H/8esAN1tsSIgIxrS0GK1LISJyO78NcKdLYWOZHYvzkmAwiNblEBG5nd8G+K7qRjS096CI3SdE5Kf8NsBLrHYYBFiUm6h1KUREHuHHAW7DzPFxiA0P1roUIiKP8MsAt7d0YXd1E0efEJFf88sA31jWu3nDYs6+JCI/5pcBbrbakBwVgimp0VqXQkTkMX4X4A6nC5vK7CjKT4IIhw8Skf/yuwDfcbQRLZ0Obt5ARH7P7wLcbLXBZBAs4PBBIvJz/hfgFhsKMuMQHRqkdSlERB7lVwF+vKkDlhMt7D4hooDgVwG+0do7fJDjv4koEPhVgJutNqTFhiE3OVLrUoiIPM5vArzb4cLH5bUcPkhEAcNvArz0SD3aup3cfYeIAsaQAS4iq0XEJiJ7+x2LF5H1IlLe92ecZ8scmtlqQ7DRgPkTErQuhYjIK4bTAl8DYNlZxx4GsEEplQtgQ9/nmjJb7ZiTHY+IEJPWpRARecWQAa6U2gSg/qzDVwN4pe/jVwB8zc11jUhVfTsqbK3sPiGigDLaPvAxSqnjAND3p6bJWWK1AQCK87n6IBEFDo8/xBSRlSJSKiKldrvdI/cwW+3ISAhHVmKER65PROSLRhvgJ0VkLAD0/Wkb7ESl1AtKqQKlVEFSkvtbyJ09TnxysBbF+ckcPkhEAWW0Af53AMv7Pl4O4P/cU87IfXq4Hp09LhSx+4SIAsxwhhGuBbAFQL6IVIvICgC/BLBURMoBLO37XBNmiw2hQQbMzebwQSIKLEOOuVNK3TDIl5a4uZZRKbHaMH9CIkKDjFqXQkTkVbqeiXm4tg1H6trZfUJEAUnXAW629D47Lcrj+G8iCjz6DnCrDROSIjA+IVzrUoiIvE63Ad7e7cCnh+q5eQMRBSzdBvgnFXXodrq4eQMRBSzdBrjZakNEsBEFmZovhEhEpAldBrhSCiVWOxbkJCLExOGDRBSYdBngFbZW1DR2sPuEiAKaLgPc3Lf6IMd/E1Eg02eAW+yYmBKFsTFhWpdCRKQZ3QV4S2cPPjtSz80biCjg6S7AN1fUwuFS3LyBiAKe7gLcbLEjKtSEmRkcPkhEgU1XAa6Ugtlqw6LcJAQZdVU6EZHb6SoF9x9vhq2li6NPiIigswAvsfbuqbmYAU5EpLcAt+GitBgkR4VqXQoRkeZ0E+BN7T3YXtnA0SdERH10E+Cbyu1wKaCI0+eJiADoKMDNVhviwoMwPT1W61KIiHyCLgLc5VLYaLVjUV4SjAbRuhwiIp+giwDfU9OEurZu7r5DRNSPLgLcbLVBBFiUxweYRESn6CLAx8aE4tpZ4xAfEax1KUREPsOkdQHDcV3heFxXOF7rMoiIfIouWuBERHQuBjgRkU4xwImIdIoBTkSkUwxwIiKdYoATEekUA5yISKcY4EREOiVKKe/dTMQOoHKU354IoNaN5egdX48v8LU4E1+PM/nD65GhlDpnLRGvBviFEJFSpVSB1nX4Cr4eX+BrcSa+Hmfy59eDXShERDrFACci0ik9BfgLWhfgY/h6fIGvxZn4epzJb18P3fSBExHRmfTUAicion4Y4EREOqWLABeRZSJiFZEKEXlY63q0IiLjRMQsIgdEZJ+I3Kd1Tb5ARIwislNE/ql1LVoTkVgReUtELH3/TuZpXZNWROQHfT8ne0VkrYiEal2Tu/l8gIuIEcBvAVwOYDKAG0RksrZVacYB4IdKqUkA5gK4O4Bfi/7uA3BA6yJ8xG8AvKeUmghgOgL0dRGRNAD3AihQSk0FYARwvbZVuZ/PBziA2QAqlFKHlFLdAP4M4GqNa9KEUuq4UmpH38ct6P3hTNO2Km2JSDqAKwH8QetatCYi0QAWAXgJAJRS3UqpRm2r0pQJQJiImACEAzimcT1up4cATwNQ1e/zagR4aAGAiGQCuBjAp9pWormnAfwIgEvrQnxANgA7gJf7upT+ICIRWhelBaVUDYAnABwFcBxAk1Lq39pW5X56CHAZ4FhAj30UkUgAfwVwv1KqWet6tCIiVwGwKaW2a12LjzABmAngd0qpiwG0AQjIZ0YiEofed+pZAFIBRIjITdpW5X56CPBqAOP6fZ4OP3wrNFwiEoTe8P6jUmqd1vVobAGAr4rIEfR2rV0qIq9rW5KmqgFUK6VOvSt7C72BHoi+BOCwUsqulOoBsA7AfI1rcjs9BPhnAHJFJEtEgtH7IOLvGtekCRER9PZvHlBKrdK6Hq0ppR5RSqUrpTLR++/iQ6WU37WyhkspdQJAlYjk9x1aAmC/hiVp6SiAuSIS3vdzswR++EDXpHUBQ1FKOUTkHgDvo/dJ8mql1D6Ny9LKAgA3A9gjIp/3HftPpdS7GtZEvuX7AP7Y19g5BOBWjevRhFLqUxF5C8AO9I7e2gk/nFLPqfRERDqlhy4UIiIaAAOciEinGOBERDrFACci0ikGOBGRTjHAiYh0igFORKRT/x/IpxXFZ3oPbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.184\n",
      "Epoch 0, loss: 16.134579\n",
      "Epoch 1, loss: 14.419783\n",
      "Epoch 2, loss: 17.232117\n",
      "Epoch 3, loss: 13.897578\n",
      "Epoch 4, loss: 15.052605\n",
      "Epoch 5, loss: 16.201548\n",
      "Epoch 6, loss: 19.806697\n",
      "Epoch 7, loss: 16.957137\n",
      "Epoch 8, loss: 14.878859\n",
      "Epoch 9, loss: 15.611164\n",
      "Epoch 10, loss: 16.586477\n",
      "Epoch 11, loss: 17.245209\n",
      "Epoch 12, loss: 15.188509\n",
      "Epoch 13, loss: 17.396540\n",
      "Epoch 14, loss: 17.328419\n",
      "Epoch 15, loss: 17.602020\n",
      "Epoch 16, loss: 15.881882\n",
      "Epoch 17, loss: 15.240127\n",
      "Epoch 18, loss: 17.555193\n",
      "Epoch 19, loss: 14.644665\n",
      "Epoch 20, loss: 21.059733\n",
      "Epoch 21, loss: 14.915116\n",
      "Epoch 22, loss: 14.727447\n",
      "Epoch 23, loss: 14.385899\n",
      "Epoch 24, loss: 14.697618\n",
      "Epoch 25, loss: 14.702872\n",
      "Epoch 26, loss: 15.772658\n",
      "Epoch 27, loss: 17.655422\n",
      "Epoch 28, loss: 15.514106\n",
      "Epoch 29, loss: 15.743345\n",
      "Epoch 30, loss: 19.158438\n",
      "Epoch 31, loss: 17.106054\n",
      "Epoch 32, loss: 14.991990\n",
      "Epoch 33, loss: 17.984060\n",
      "Epoch 34, loss: 17.167299\n",
      "Epoch 35, loss: 14.511740\n",
      "Epoch 36, loss: 16.769300\n",
      "Epoch 37, loss: 15.183408\n",
      "Epoch 38, loss: 18.724391\n",
      "Epoch 39, loss: 17.557954\n",
      "Epoch 40, loss: 15.970354\n",
      "Epoch 41, loss: 15.765337\n",
      "Epoch 42, loss: 15.198642\n",
      "Epoch 43, loss: 19.075821\n",
      "Epoch 44, loss: 15.473636\n",
      "Epoch 45, loss: 16.327992\n",
      "Epoch 46, loss: 18.932778\n",
      "Epoch 47, loss: 15.518656\n",
      "Epoch 48, loss: 18.103148\n",
      "Epoch 49, loss: 20.790980\n",
      "Epoch 50, loss: 15.337710\n",
      "Epoch 51, loss: 16.443059\n",
      "Epoch 52, loss: 16.941477\n",
      "Epoch 53, loss: 16.495457\n",
      "Epoch 54, loss: 13.331080\n",
      "Epoch 55, loss: 17.406353\n",
      "Epoch 56, loss: 16.004222\n",
      "Epoch 57, loss: 16.665902\n",
      "Epoch 58, loss: 16.605764\n",
      "Epoch 59, loss: 17.665646\n",
      "Epoch 60, loss: 14.643509\n",
      "Epoch 61, loss: 16.881786\n",
      "Epoch 62, loss: 17.242219\n",
      "Epoch 63, loss: 16.524212\n",
      "Epoch 64, loss: 15.056557\n",
      "Epoch 65, loss: 13.961031\n",
      "Epoch 66, loss: 17.348652\n",
      "Epoch 67, loss: 14.507084\n",
      "Epoch 68, loss: 16.734411\n",
      "Epoch 69, loss: 17.090349\n",
      "Epoch 70, loss: 14.916503\n",
      "Epoch 71, loss: 18.250583\n",
      "Epoch 72, loss: 20.849074\n",
      "Epoch 73, loss: 15.504101\n",
      "Epoch 74, loss: 15.258384\n",
      "Epoch 75, loss: 16.843226\n",
      "Epoch 76, loss: 14.755220\n",
      "Epoch 77, loss: 15.352906\n",
      "Epoch 78, loss: 14.656665\n",
      "Epoch 79, loss: 16.944238\n",
      "Epoch 80, loss: 15.550420\n",
      "Epoch 81, loss: 16.430697\n",
      "Epoch 82, loss: 14.793737\n",
      "Epoch 83, loss: 15.751959\n",
      "Epoch 84, loss: 16.241354\n",
      "Epoch 85, loss: 15.393059\n",
      "Epoch 86, loss: 17.589576\n",
      "Epoch 87, loss: 19.056908\n",
      "Epoch 88, loss: 15.353785\n",
      "Epoch 89, loss: 15.464765\n",
      "Epoch 90, loss: 19.259678\n",
      "Epoch 91, loss: 16.674393\n",
      "Epoch 92, loss: 15.441159\n",
      "Epoch 93, loss: 16.956714\n",
      "Epoch 94, loss: 15.599204\n",
      "Epoch 95, loss: 14.592772\n",
      "Epoch 96, loss: 18.259696\n",
      "Epoch 97, loss: 15.662216\n",
      "Epoch 98, loss: 13.817091\n",
      "Epoch 99, loss: 13.412184\n",
      "Accuracy after training for 100 epochs:  0.174\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 3.401183\n",
      "Epoch 1, loss: 2.551507\n",
      "Epoch 2, loss: 2.662188\n",
      "Epoch 3, loss: 2.900581\n",
      "Epoch 4, loss: 3.235575\n",
      "Epoch 5, loss: 2.715136\n",
      "Epoch 6, loss: 2.816455\n",
      "Epoch 7, loss: 3.039470\n",
      "Epoch 8, loss: 3.031808\n",
      "Epoch 9, loss: 2.658602\n",
      "Epoch 10, loss: 2.767735\n",
      "Epoch 11, loss: 3.226740\n",
      "Epoch 12, loss: 2.614760\n",
      "Epoch 13, loss: 3.073508\n",
      "Epoch 14, loss: 2.549081\n",
      "Epoch 15, loss: 2.301594\n",
      "Epoch 16, loss: 3.444189\n",
      "Epoch 17, loss: 2.544185\n",
      "Epoch 18, loss: 3.033703\n",
      "Epoch 19, loss: 2.473474\n",
      "Epoch 20, loss: 2.240451\n",
      "Epoch 21, loss: 2.587061\n",
      "Epoch 22, loss: 2.390768\n",
      "Epoch 23, loss: 2.236458\n",
      "Epoch 24, loss: 2.410471\n",
      "Epoch 25, loss: 2.660028\n",
      "Epoch 26, loss: 2.405632\n",
      "Epoch 27, loss: 2.279345\n",
      "Epoch 28, loss: 2.426270\n",
      "Epoch 29, loss: 2.450088\n",
      "Epoch 30, loss: 2.909856\n",
      "Epoch 31, loss: 2.630205\n",
      "Epoch 32, loss: 3.038210\n",
      "Epoch 33, loss: 2.888752\n",
      "Epoch 34, loss: 2.438006\n",
      "Epoch 35, loss: 2.270358\n",
      "Epoch 36, loss: 2.116509\n",
      "Epoch 37, loss: 2.548648\n",
      "Epoch 38, loss: 2.287727\n",
      "Epoch 39, loss: 2.373110\n",
      "Epoch 40, loss: 2.681169\n",
      "Epoch 41, loss: 2.320568\n",
      "Epoch 42, loss: 2.652039\n",
      "Epoch 43, loss: 2.318430\n",
      "Epoch 44, loss: 2.362652\n",
      "Epoch 45, loss: 2.868127\n",
      "Epoch 46, loss: 2.676640\n",
      "Epoch 47, loss: 2.630714\n",
      "Epoch 48, loss: 2.278196\n",
      "Epoch 49, loss: 2.375240\n",
      "Epoch 50, loss: 2.418325\n",
      "Epoch 51, loss: 2.200985\n",
      "Epoch 52, loss: 2.875036\n",
      "Epoch 53, loss: 2.442987\n",
      "Epoch 54, loss: 2.951035\n",
      "Epoch 55, loss: 2.696766\n",
      "Epoch 56, loss: 2.431529\n",
      "Epoch 57, loss: 2.662721\n",
      "Epoch 58, loss: 2.353392\n",
      "Epoch 59, loss: 2.796000\n",
      "Epoch 60, loss: 2.422410\n",
      "Epoch 61, loss: 2.136598\n",
      "Epoch 62, loss: 3.670655\n",
      "Epoch 63, loss: 2.358428\n",
      "Epoch 64, loss: 3.004530\n",
      "Epoch 65, loss: 2.384899\n",
      "Epoch 66, loss: 2.448410\n",
      "Epoch 67, loss: 2.370377\n",
      "Epoch 68, loss: 2.572302\n",
      "Epoch 69, loss: 2.547316\n",
      "Epoch 70, loss: 2.867713\n",
      "Epoch 71, loss: 3.212266\n",
      "Epoch 72, loss: 2.487859\n",
      "Epoch 73, loss: 2.353032\n",
      "Epoch 74, loss: 2.856509\n",
      "Epoch 75, loss: 2.385496\n",
      "Epoch 76, loss: 2.411045\n",
      "Epoch 77, loss: 2.253031\n",
      "Epoch 78, loss: 2.639892\n",
      "Epoch 79, loss: 2.442196\n",
      "Epoch 80, loss: 2.750823\n",
      "Epoch 81, loss: 2.310883\n",
      "Epoch 82, loss: 2.939961\n",
      "Epoch 83, loss: 2.699010\n",
      "Epoch 84, loss: 2.569283\n",
      "Epoch 85, loss: 2.658168\n",
      "Epoch 86, loss: 2.382494\n",
      "Epoch 87, loss: 2.857472\n",
      "Epoch 88, loss: 2.714414\n",
      "Epoch 89, loss: 2.759146\n",
      "Epoch 90, loss: 2.227794\n",
      "Epoch 91, loss: 2.491210\n",
      "Epoch 92, loss: 3.142292\n",
      "Epoch 93, loss: 2.079636\n",
      "Epoch 94, loss: 2.727539\n",
      "Epoch 95, loss: 2.436802\n",
      "Epoch 96, loss: 2.745714\n",
      "Epoch 97, loss: 2.768201\n",
      "Epoch 98, loss: 2.553624\n",
      "Epoch 99, loss: 2.479847\n",
      "Epoch 100, loss: 2.808706\n",
      "Epoch 101, loss: 2.866766\n",
      "Epoch 102, loss: 2.585169\n",
      "Epoch 103, loss: 2.463772\n",
      "Epoch 104, loss: 2.596705\n",
      "Epoch 105, loss: 2.331921\n",
      "Epoch 106, loss: 2.926660\n",
      "Epoch 107, loss: 2.317659\n",
      "Epoch 108, loss: 3.003305\n",
      "Epoch 109, loss: 2.857493\n",
      "Epoch 110, loss: 2.599051\n",
      "Epoch 111, loss: 2.522932\n",
      "Epoch 112, loss: 2.333920\n",
      "Epoch 113, loss: 2.430186\n",
      "Epoch 114, loss: 3.016677\n",
      "Epoch 115, loss: 2.331282\n",
      "Epoch 116, loss: 2.693436\n",
      "Epoch 117, loss: 2.225621\n",
      "Epoch 118, loss: 2.732650\n",
      "Epoch 119, loss: 2.486789\n",
      "Epoch 120, loss: 2.367973\n",
      "Epoch 121, loss: 2.387603\n",
      "Epoch 122, loss: 2.995191\n",
      "Epoch 123, loss: 2.241679\n",
      "Epoch 124, loss: 2.538837\n",
      "Epoch 125, loss: 2.879830\n",
      "Epoch 126, loss: 2.101787\n",
      "Epoch 127, loss: 2.334838\n",
      "Epoch 128, loss: 2.568210\n",
      "Epoch 129, loss: 2.525401\n",
      "Epoch 130, loss: 2.454144\n",
      "Epoch 131, loss: 2.951473\n",
      "Epoch 132, loss: 2.211366\n",
      "Epoch 133, loss: 2.694403\n",
      "Epoch 134, loss: 2.641848\n",
      "Epoch 135, loss: 2.648913\n",
      "Epoch 136, loss: 2.419916\n",
      "Epoch 137, loss: 2.164140\n",
      "Epoch 138, loss: 2.240771\n",
      "Epoch 139, loss: 2.342055\n",
      "Epoch 140, loss: 2.580569\n",
      "Epoch 141, loss: 3.060328\n",
      "Epoch 142, loss: 2.516455\n",
      "Epoch 143, loss: 2.402207\n",
      "Epoch 144, loss: 2.155118\n",
      "Epoch 145, loss: 2.744952\n",
      "Epoch 146, loss: 2.352599\n",
      "Epoch 147, loss: 2.337591\n",
      "Epoch 148, loss: 2.644551\n",
      "Epoch 149, loss: 2.470742\n",
      "Epoch 150, loss: 2.130707\n",
      "Epoch 151, loss: 2.556154\n",
      "Epoch 152, loss: 2.505613\n",
      "Epoch 153, loss: 2.107606\n",
      "Epoch 154, loss: 2.157962\n",
      "Epoch 155, loss: 2.330083\n",
      "Epoch 156, loss: 2.878498\n",
      "Epoch 157, loss: 2.212345\n",
      "Epoch 158, loss: 2.407904\n",
      "Epoch 159, loss: 2.526904\n",
      "Epoch 160, loss: 2.341223\n",
      "Epoch 161, loss: 2.803943\n",
      "Epoch 162, loss: 2.434248\n",
      "Epoch 163, loss: 2.212812\n",
      "Epoch 164, loss: 2.816213\n",
      "Epoch 165, loss: 2.925059\n",
      "Epoch 166, loss: 2.247537\n",
      "Epoch 167, loss: 2.393136\n",
      "Epoch 168, loss: 2.314262\n",
      "Epoch 169, loss: 2.692582\n",
      "Epoch 170, loss: 2.774056\n",
      "Epoch 171, loss: 2.488632\n",
      "Epoch 172, loss: 2.465557\n",
      "Epoch 173, loss: 2.150262\n",
      "Epoch 174, loss: 3.016636\n",
      "Epoch 175, loss: 2.175685\n",
      "Epoch 176, loss: 2.383480\n",
      "Epoch 177, loss: 2.334229\n",
      "Epoch 178, loss: 2.365303\n",
      "Epoch 179, loss: 3.283072\n",
      "Epoch 180, loss: 2.438349\n",
      "Epoch 181, loss: 2.074186\n",
      "Epoch 182, loss: 2.544677\n",
      "Epoch 183, loss: 2.303187\n",
      "Epoch 184, loss: 2.561984\n",
      "Epoch 185, loss: 2.372376\n",
      "Epoch 186, loss: 2.480145\n",
      "Epoch 187, loss: 3.008949\n",
      "Epoch 188, loss: 2.717623\n",
      "Epoch 189, loss: 2.675851\n",
      "Epoch 190, loss: 2.104473\n",
      "Epoch 191, loss: 2.104506\n",
      "Epoch 192, loss: 2.101539\n",
      "Epoch 193, loss: 2.354740\n",
      "Epoch 194, loss: 2.293707\n",
      "Epoch 195, loss: 2.118277\n",
      "Epoch 196, loss: 2.648311\n",
      "Epoch 197, loss: 2.806629\n",
      "Epoch 198, loss: 2.298089\n",
      "Epoch 199, loss: 2.406251\n",
      "Epoch 0, loss: 2.772098\n",
      "Epoch 1, loss: 2.982386\n",
      "Epoch 2, loss: 2.962032\n",
      "Epoch 3, loss: 2.469370\n",
      "Epoch 4, loss: 2.844273\n",
      "Epoch 5, loss: 3.567959\n",
      "Epoch 6, loss: 3.098243\n",
      "Epoch 7, loss: 2.842314\n",
      "Epoch 8, loss: 2.420322\n",
      "Epoch 9, loss: 2.863248\n",
      "Epoch 10, loss: 2.520728\n",
      "Epoch 11, loss: 3.117768\n",
      "Epoch 12, loss: 2.896690\n",
      "Epoch 13, loss: 3.070939\n",
      "Epoch 14, loss: 3.078892\n",
      "Epoch 15, loss: 2.582151\n",
      "Epoch 16, loss: 2.650248\n",
      "Epoch 17, loss: 2.852664\n",
      "Epoch 18, loss: 3.059808\n",
      "Epoch 19, loss: 2.379740\n",
      "Epoch 20, loss: 2.322432\n",
      "Epoch 21, loss: 3.095756\n",
      "Epoch 22, loss: 3.046827\n",
      "Epoch 23, loss: 2.440279\n",
      "Epoch 24, loss: 2.731164\n",
      "Epoch 25, loss: 2.513884\n",
      "Epoch 26, loss: 3.354014\n",
      "Epoch 27, loss: 2.894845\n",
      "Epoch 28, loss: 2.439161\n",
      "Epoch 29, loss: 2.196697\n",
      "Epoch 30, loss: 2.631096\n",
      "Epoch 31, loss: 2.699032\n",
      "Epoch 32, loss: 2.386507\n",
      "Epoch 33, loss: 2.831999\n",
      "Epoch 34, loss: 2.674968\n",
      "Epoch 35, loss: 2.370176\n",
      "Epoch 36, loss: 3.293947\n",
      "Epoch 37, loss: 2.439271\n",
      "Epoch 38, loss: 2.841449\n",
      "Epoch 39, loss: 2.589959\n",
      "Epoch 40, loss: 2.522891\n",
      "Epoch 41, loss: 2.689158\n",
      "Epoch 42, loss: 2.801998\n",
      "Epoch 43, loss: 2.509998\n",
      "Epoch 44, loss: 2.589103\n",
      "Epoch 45, loss: 2.406686\n",
      "Epoch 46, loss: 2.696217\n",
      "Epoch 47, loss: 2.523061\n",
      "Epoch 48, loss: 2.292083\n",
      "Epoch 49, loss: 2.618419\n",
      "Epoch 50, loss: 2.548226\n",
      "Epoch 51, loss: 2.722196\n",
      "Epoch 52, loss: 2.195878\n",
      "Epoch 53, loss: 3.053008\n",
      "Epoch 54, loss: 2.821660\n",
      "Epoch 55, loss: 2.570947\n",
      "Epoch 56, loss: 3.007960\n",
      "Epoch 57, loss: 2.697713\n",
      "Epoch 58, loss: 2.803163\n",
      "Epoch 59, loss: 2.754152\n",
      "Epoch 60, loss: 2.331867\n",
      "Epoch 61, loss: 2.542884\n",
      "Epoch 62, loss: 3.277987\n",
      "Epoch 63, loss: 2.981488\n",
      "Epoch 64, loss: 3.154727\n",
      "Epoch 65, loss: 2.246469\n",
      "Epoch 66, loss: 2.224460\n",
      "Epoch 67, loss: 2.668654\n",
      "Epoch 68, loss: 2.223075\n",
      "Epoch 69, loss: 2.360853\n",
      "Epoch 70, loss: 2.645867\n",
      "Epoch 71, loss: 2.187481\n",
      "Epoch 72, loss: 2.897865\n",
      "Epoch 73, loss: 2.844296\n",
      "Epoch 74, loss: 2.692335\n",
      "Epoch 75, loss: 2.453499\n",
      "Epoch 76, loss: 3.146380\n",
      "Epoch 77, loss: 2.528920\n",
      "Epoch 78, loss: 2.485861\n",
      "Epoch 79, loss: 2.167796\n",
      "Epoch 80, loss: 2.539114\n",
      "Epoch 81, loss: 2.852954\n",
      "Epoch 82, loss: 2.317806\n",
      "Epoch 83, loss: 2.298767\n",
      "Epoch 84, loss: 2.351106\n",
      "Epoch 85, loss: 2.361450\n",
      "Epoch 86, loss: 2.699826\n",
      "Epoch 87, loss: 2.510790\n",
      "Epoch 88, loss: 2.772540\n",
      "Epoch 89, loss: 2.268993\n",
      "Epoch 90, loss: 2.197721\n",
      "Epoch 91, loss: 2.588114\n",
      "Epoch 92, loss: 2.228934\n",
      "Epoch 93, loss: 2.705916\n",
      "Epoch 94, loss: 3.350145\n",
      "Epoch 95, loss: 2.565019\n",
      "Epoch 96, loss: 2.221967\n",
      "Epoch 97, loss: 2.543788\n",
      "Epoch 98, loss: 2.574843\n",
      "Epoch 99, loss: 2.336415\n",
      "Epoch 100, loss: 2.411785\n",
      "Epoch 101, loss: 2.603678\n",
      "Epoch 102, loss: 2.677893\n",
      "Epoch 103, loss: 3.106201\n",
      "Epoch 104, loss: 2.101621\n",
      "Epoch 105, loss: 2.339570\n",
      "Epoch 106, loss: 2.044701\n",
      "Epoch 107, loss: 2.180368\n",
      "Epoch 108, loss: 2.316637\n",
      "Epoch 109, loss: 2.373519\n",
      "Epoch 110, loss: 2.275067\n",
      "Epoch 111, loss: 2.391557\n",
      "Epoch 112, loss: 2.228685\n",
      "Epoch 113, loss: 2.654493\n",
      "Epoch 114, loss: 2.126854\n",
      "Epoch 115, loss: 2.276865\n",
      "Epoch 116, loss: 2.777888\n",
      "Epoch 117, loss: 2.160290\n",
      "Epoch 118, loss: 2.438264\n",
      "Epoch 119, loss: 2.699157\n",
      "Epoch 120, loss: 2.801419\n",
      "Epoch 121, loss: 2.551823\n",
      "Epoch 122, loss: 2.446882\n",
      "Epoch 123, loss: 2.857284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, loss: 2.210811\n",
      "Epoch 125, loss: 2.445614\n",
      "Epoch 126, loss: 3.074348\n",
      "Epoch 127, loss: 2.162153\n",
      "Epoch 128, loss: 2.603005\n",
      "Epoch 129, loss: 2.106176\n",
      "Epoch 130, loss: 2.950955\n",
      "Epoch 131, loss: 2.730142\n",
      "Epoch 132, loss: 2.359469\n",
      "Epoch 133, loss: 2.729493\n",
      "Epoch 134, loss: 2.613291\n",
      "Epoch 135, loss: 2.380038\n",
      "Epoch 136, loss: 2.561232\n",
      "Epoch 137, loss: 2.227933\n",
      "Epoch 138, loss: 2.608680\n",
      "Epoch 139, loss: 2.149558\n",
      "Epoch 140, loss: 2.512207\n",
      "Epoch 141, loss: 2.501185\n",
      "Epoch 142, loss: 2.517238\n",
      "Epoch 143, loss: 3.024809\n",
      "Epoch 144, loss: 2.181208\n",
      "Epoch 145, loss: 2.273767\n",
      "Epoch 146, loss: 2.268698\n",
      "Epoch 147, loss: 2.882542\n",
      "Epoch 148, loss: 2.407246\n",
      "Epoch 149, loss: 2.712948\n",
      "Epoch 150, loss: 2.485197\n",
      "Epoch 151, loss: 2.603110\n",
      "Epoch 152, loss: 2.446419\n",
      "Epoch 153, loss: 2.380169\n",
      "Epoch 154, loss: 2.810115\n",
      "Epoch 155, loss: 2.549624\n",
      "Epoch 156, loss: 2.431099\n",
      "Epoch 157, loss: 2.178373\n",
      "Epoch 158, loss: 2.545545\n",
      "Epoch 159, loss: 2.428772\n",
      "Epoch 160, loss: 3.046078\n",
      "Epoch 161, loss: 3.101530\n",
      "Epoch 162, loss: 2.139710\n",
      "Epoch 163, loss: 2.392458\n",
      "Epoch 164, loss: 2.679265\n",
      "Epoch 165, loss: 2.859591\n",
      "Epoch 166, loss: 2.496054\n",
      "Epoch 167, loss: 2.295350\n",
      "Epoch 168, loss: 2.429509\n",
      "Epoch 169, loss: 2.945364\n",
      "Epoch 170, loss: 2.172950\n",
      "Epoch 171, loss: 2.219361\n",
      "Epoch 172, loss: 2.622822\n",
      "Epoch 173, loss: 2.508261\n",
      "Epoch 174, loss: 2.378667\n",
      "Epoch 175, loss: 2.413182\n",
      "Epoch 176, loss: 2.647589\n",
      "Epoch 177, loss: 3.004447\n",
      "Epoch 178, loss: 2.275419\n",
      "Epoch 179, loss: 2.620304\n",
      "Epoch 180, loss: 2.197009\n",
      "Epoch 181, loss: 2.454172\n",
      "Epoch 182, loss: 2.965949\n",
      "Epoch 183, loss: 2.846711\n",
      "Epoch 184, loss: 2.700615\n",
      "Epoch 185, loss: 2.929348\n",
      "Epoch 186, loss: 2.294634\n",
      "Epoch 187, loss: 2.207306\n",
      "Epoch 188, loss: 2.700247\n",
      "Epoch 189, loss: 2.500547\n",
      "Epoch 190, loss: 2.588306\n",
      "Epoch 191, loss: 2.329084\n",
      "Epoch 192, loss: 2.214418\n",
      "Epoch 193, loss: 2.114577\n",
      "Epoch 194, loss: 2.831634\n",
      "Epoch 195, loss: 2.575087\n",
      "Epoch 196, loss: 2.238258\n",
      "Epoch 197, loss: 2.425113\n",
      "Epoch 198, loss: 2.296215\n",
      "Epoch 199, loss: 2.456436\n",
      "Epoch 0, loss: 4.085161\n",
      "Epoch 1, loss: 3.848588\n",
      "Epoch 2, loss: 3.084658\n",
      "Epoch 3, loss: 2.715470\n",
      "Epoch 4, loss: 3.049107\n",
      "Epoch 5, loss: 3.043585\n",
      "Epoch 6, loss: 2.651898\n",
      "Epoch 7, loss: 2.819077\n",
      "Epoch 8, loss: 3.391581\n",
      "Epoch 9, loss: 3.380815\n",
      "Epoch 10, loss: 2.355849\n",
      "Epoch 11, loss: 2.757963\n",
      "Epoch 12, loss: 2.794747\n",
      "Epoch 13, loss: 2.613869\n",
      "Epoch 14, loss: 2.596395\n",
      "Epoch 15, loss: 3.162754\n",
      "Epoch 16, loss: 2.566288\n",
      "Epoch 17, loss: 2.820022\n",
      "Epoch 18, loss: 2.436466\n",
      "Epoch 19, loss: 2.658759\n",
      "Epoch 20, loss: 2.461014\n",
      "Epoch 21, loss: 2.739803\n",
      "Epoch 22, loss: 2.468193\n",
      "Epoch 23, loss: 2.897159\n",
      "Epoch 24, loss: 2.703217\n",
      "Epoch 25, loss: 2.716223\n",
      "Epoch 26, loss: 2.870709\n",
      "Epoch 27, loss: 2.711806\n",
      "Epoch 28, loss: 2.557322\n",
      "Epoch 29, loss: 2.387777\n",
      "Epoch 30, loss: 2.237877\n",
      "Epoch 31, loss: 2.823542\n",
      "Epoch 32, loss: 2.184017\n",
      "Epoch 33, loss: 2.557943\n",
      "Epoch 34, loss: 2.752180\n",
      "Epoch 35, loss: 2.520779\n",
      "Epoch 36, loss: 2.548816\n",
      "Epoch 37, loss: 2.966953\n",
      "Epoch 38, loss: 2.568679\n",
      "Epoch 39, loss: 2.564920\n",
      "Epoch 40, loss: 3.430259\n",
      "Epoch 41, loss: 2.419306\n",
      "Epoch 42, loss: 2.724051\n",
      "Epoch 43, loss: 2.363917\n",
      "Epoch 44, loss: 2.920336\n",
      "Epoch 45, loss: 2.364799\n",
      "Epoch 46, loss: 2.708713\n",
      "Epoch 47, loss: 2.741194\n",
      "Epoch 48, loss: 2.750458\n",
      "Epoch 49, loss: 2.500530\n",
      "Epoch 50, loss: 2.421846\n",
      "Epoch 51, loss: 2.487786\n",
      "Epoch 52, loss: 3.106632\n",
      "Epoch 53, loss: 2.627869\n",
      "Epoch 54, loss: 2.880256\n",
      "Epoch 55, loss: 2.431449\n",
      "Epoch 56, loss: 2.517806\n",
      "Epoch 57, loss: 2.642023\n",
      "Epoch 58, loss: 2.236466\n",
      "Epoch 59, loss: 2.274612\n",
      "Epoch 60, loss: 2.238464\n",
      "Epoch 61, loss: 2.856175\n",
      "Epoch 62, loss: 2.551740\n",
      "Epoch 63, loss: 2.567073\n",
      "Epoch 64, loss: 2.704640\n",
      "Epoch 65, loss: 2.635297\n",
      "Epoch 66, loss: 2.216810\n",
      "Epoch 67, loss: 2.219783\n",
      "Epoch 68, loss: 2.768290\n",
      "Epoch 69, loss: 2.550548\n",
      "Epoch 70, loss: 2.358661\n",
      "Epoch 71, loss: 2.294997\n",
      "Epoch 72, loss: 2.125474\n",
      "Epoch 73, loss: 2.395922\n",
      "Epoch 74, loss: 2.201561\n",
      "Epoch 75, loss: 2.432740\n",
      "Epoch 76, loss: 2.531973\n",
      "Epoch 77, loss: 2.714183\n",
      "Epoch 78, loss: 2.126447\n",
      "Epoch 79, loss: 2.454515\n",
      "Epoch 80, loss: 2.230898\n",
      "Epoch 81, loss: 2.356153\n",
      "Epoch 82, loss: 2.178402\n",
      "Epoch 83, loss: 2.535749\n",
      "Epoch 84, loss: 2.558260\n",
      "Epoch 85, loss: 2.703501\n",
      "Epoch 86, loss: 2.163497\n",
      "Epoch 87, loss: 2.371631\n",
      "Epoch 88, loss: 2.444266\n",
      "Epoch 89, loss: 2.201396\n",
      "Epoch 90, loss: 2.205613\n",
      "Epoch 91, loss: 2.572164\n",
      "Epoch 92, loss: 2.169735\n",
      "Epoch 93, loss: 2.290855\n",
      "Epoch 94, loss: 2.662463\n",
      "Epoch 95, loss: 2.265349\n",
      "Epoch 96, loss: 2.483798\n",
      "Epoch 97, loss: 2.799091\n",
      "Epoch 98, loss: 2.276959\n",
      "Epoch 99, loss: 2.772170\n",
      "Epoch 100, loss: 2.538458\n",
      "Epoch 101, loss: 2.741670\n",
      "Epoch 102, loss: 2.696095\n",
      "Epoch 103, loss: 2.797009\n",
      "Epoch 104, loss: 2.363033\n",
      "Epoch 105, loss: 2.575620\n",
      "Epoch 106, loss: 2.504011\n",
      "Epoch 107, loss: 2.178407\n",
      "Epoch 108, loss: 2.786656\n",
      "Epoch 109, loss: 2.344920\n",
      "Epoch 110, loss: 2.361592\n",
      "Epoch 111, loss: 2.270730\n",
      "Epoch 112, loss: 2.767959\n",
      "Epoch 113, loss: 2.228137\n",
      "Epoch 114, loss: 2.172748\n",
      "Epoch 115, loss: 2.725505\n",
      "Epoch 116, loss: 2.855934\n",
      "Epoch 117, loss: 2.283017\n",
      "Epoch 118, loss: 2.060060\n",
      "Epoch 119, loss: 2.438413\n",
      "Epoch 120, loss: 3.039368\n",
      "Epoch 121, loss: 2.132257\n",
      "Epoch 122, loss: 2.324094\n",
      "Epoch 123, loss: 2.774910\n",
      "Epoch 124, loss: 2.447003\n",
      "Epoch 125, loss: 2.748002\n",
      "Epoch 126, loss: 2.234171\n",
      "Epoch 127, loss: 2.463179\n",
      "Epoch 128, loss: 2.913964\n",
      "Epoch 129, loss: 2.932313\n",
      "Epoch 130, loss: 2.095143\n",
      "Epoch 131, loss: 2.354030\n",
      "Epoch 132, loss: 2.226501\n",
      "Epoch 133, loss: 2.742244\n",
      "Epoch 134, loss: 3.013928\n",
      "Epoch 135, loss: 2.213855\n",
      "Epoch 136, loss: 2.139957\n",
      "Epoch 137, loss: 2.195413\n",
      "Epoch 138, loss: 2.272580\n",
      "Epoch 139, loss: 2.241074\n",
      "Epoch 140, loss: 3.123587\n",
      "Epoch 141, loss: 2.433303\n",
      "Epoch 142, loss: 2.120962\n",
      "Epoch 143, loss: 2.289305\n",
      "Epoch 144, loss: 2.698013\n",
      "Epoch 145, loss: 3.020292\n",
      "Epoch 146, loss: 2.584091\n",
      "Epoch 147, loss: 2.404533\n",
      "Epoch 148, loss: 2.236237\n",
      "Epoch 149, loss: 2.558767\n",
      "Epoch 150, loss: 2.668872\n",
      "Epoch 151, loss: 2.906277\n",
      "Epoch 152, loss: 2.578024\n",
      "Epoch 153, loss: 2.123671\n",
      "Epoch 154, loss: 2.913055\n",
      "Epoch 155, loss: 2.078018\n",
      "Epoch 156, loss: 2.081586\n",
      "Epoch 157, loss: 2.386405\n",
      "Epoch 158, loss: 2.374947\n",
      "Epoch 159, loss: 2.690259\n",
      "Epoch 160, loss: 2.618877\n",
      "Epoch 161, loss: 2.475122\n",
      "Epoch 162, loss: 2.174714\n",
      "Epoch 163, loss: 2.389861\n",
      "Epoch 164, loss: 2.356859\n",
      "Epoch 165, loss: 2.213359\n",
      "Epoch 166, loss: 2.895523\n",
      "Epoch 167, loss: 2.420407\n",
      "Epoch 168, loss: 2.815700\n",
      "Epoch 169, loss: 2.445811\n",
      "Epoch 170, loss: 2.113107\n",
      "Epoch 171, loss: 2.185628\n",
      "Epoch 172, loss: 2.372392\n",
      "Epoch 173, loss: 2.653744\n",
      "Epoch 174, loss: 2.633444\n",
      "Epoch 175, loss: 2.263800\n",
      "Epoch 176, loss: 2.132121\n",
      "Epoch 177, loss: 2.402316\n",
      "Epoch 178, loss: 2.775400\n",
      "Epoch 179, loss: 2.527885\n",
      "Epoch 180, loss: 2.849638\n",
      "Epoch 181, loss: 2.389999\n",
      "Epoch 182, loss: 2.475703\n",
      "Epoch 183, loss: 2.042014\n",
      "Epoch 184, loss: 2.236140\n",
      "Epoch 185, loss: 2.460467\n",
      "Epoch 186, loss: 2.197835\n",
      "Epoch 187, loss: 2.270122\n",
      "Epoch 188, loss: 2.151598\n",
      "Epoch 189, loss: 2.557181\n",
      "Epoch 190, loss: 2.824703\n",
      "Epoch 191, loss: 2.191757\n",
      "Epoch 192, loss: 2.816642\n",
      "Epoch 193, loss: 2.524056\n",
      "Epoch 194, loss: 2.152820\n",
      "Epoch 195, loss: 2.463245\n",
      "Epoch 196, loss: 2.317654\n",
      "Epoch 197, loss: 2.294709\n",
      "Epoch 198, loss: 2.693568\n",
      "Epoch 199, loss: 2.534196\n",
      "Epoch 0, loss: 2.283559\n",
      "Epoch 1, loss: 2.272897\n",
      "Epoch 2, loss: 2.258245\n",
      "Epoch 3, loss: 2.221981\n",
      "Epoch 4, loss: 2.229594\n",
      "Epoch 5, loss: 2.187688\n",
      "Epoch 6, loss: 2.182682\n",
      "Epoch 7, loss: 2.197618\n",
      "Epoch 8, loss: 2.180639\n",
      "Epoch 9, loss: 2.185611\n",
      "Epoch 10, loss: 2.199399\n",
      "Epoch 11, loss: 2.137563\n",
      "Epoch 12, loss: 2.182616\n",
      "Epoch 13, loss: 2.161573\n",
      "Epoch 14, loss: 2.179889\n",
      "Epoch 15, loss: 2.207878\n",
      "Epoch 16, loss: 2.154466\n",
      "Epoch 17, loss: 2.183763\n",
      "Epoch 18, loss: 2.153544\n",
      "Epoch 19, loss: 2.142703\n",
      "Epoch 20, loss: 2.134405\n",
      "Epoch 21, loss: 2.127487\n",
      "Epoch 22, loss: 2.160773\n",
      "Epoch 23, loss: 2.112439\n",
      "Epoch 24, loss: 2.173616\n",
      "Epoch 25, loss: 2.149314\n",
      "Epoch 26, loss: 2.122761\n",
      "Epoch 27, loss: 2.144731\n",
      "Epoch 28, loss: 2.129609\n",
      "Epoch 29, loss: 2.159410\n",
      "Epoch 30, loss: 2.153080\n",
      "Epoch 31, loss: 2.170887\n",
      "Epoch 32, loss: 2.119231\n",
      "Epoch 33, loss: 2.156875\n",
      "Epoch 34, loss: 2.097636\n",
      "Epoch 35, loss: 2.118246\n",
      "Epoch 36, loss: 2.227444\n",
      "Epoch 37, loss: 2.128571\n",
      "Epoch 38, loss: 2.166959\n",
      "Epoch 39, loss: 2.137307\n",
      "Epoch 40, loss: 2.097575\n",
      "Epoch 41, loss: 2.109255\n",
      "Epoch 42, loss: 2.154652\n",
      "Epoch 43, loss: 2.081470\n",
      "Epoch 44, loss: 2.116138\n",
      "Epoch 45, loss: 2.131568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, loss: 2.130609\n",
      "Epoch 47, loss: 2.128792\n",
      "Epoch 48, loss: 2.057248\n",
      "Epoch 49, loss: 2.092293\n",
      "Epoch 50, loss: 2.096955\n",
      "Epoch 51, loss: 2.121475\n",
      "Epoch 52, loss: 2.102458\n",
      "Epoch 53, loss: 2.083691\n",
      "Epoch 54, loss: 2.129636\n",
      "Epoch 55, loss: 2.102781\n",
      "Epoch 56, loss: 2.118664\n",
      "Epoch 57, loss: 2.117754\n",
      "Epoch 58, loss: 2.127478\n",
      "Epoch 59, loss: 2.154085\n",
      "Epoch 60, loss: 2.096568\n",
      "Epoch 61, loss: 2.158922\n",
      "Epoch 62, loss: 2.111563\n",
      "Epoch 63, loss: 2.075359\n",
      "Epoch 64, loss: 2.114260\n",
      "Epoch 65, loss: 2.121500\n",
      "Epoch 66, loss: 2.113261\n",
      "Epoch 67, loss: 2.111832\n",
      "Epoch 68, loss: 2.100545\n",
      "Epoch 69, loss: 2.050742\n",
      "Epoch 70, loss: 2.068420\n",
      "Epoch 71, loss: 2.088417\n",
      "Epoch 72, loss: 2.062177\n",
      "Epoch 73, loss: 2.043069\n",
      "Epoch 74, loss: 2.091073\n",
      "Epoch 75, loss: 2.081649\n",
      "Epoch 76, loss: 2.155071\n",
      "Epoch 77, loss: 2.080468\n",
      "Epoch 78, loss: 2.085423\n",
      "Epoch 79, loss: 2.062154\n",
      "Epoch 80, loss: 2.092088\n",
      "Epoch 81, loss: 2.062428\n",
      "Epoch 82, loss: 2.074187\n",
      "Epoch 83, loss: 2.056830\n",
      "Epoch 84, loss: 2.103877\n",
      "Epoch 85, loss: 2.096677\n",
      "Epoch 86, loss: 2.103282\n",
      "Epoch 87, loss: 2.053588\n",
      "Epoch 88, loss: 2.043080\n",
      "Epoch 89, loss: 2.064557\n",
      "Epoch 90, loss: 2.079494\n",
      "Epoch 91, loss: 2.081877\n",
      "Epoch 92, loss: 2.111935\n",
      "Epoch 93, loss: 2.096937\n",
      "Epoch 94, loss: 2.071542\n",
      "Epoch 95, loss: 2.107426\n",
      "Epoch 96, loss: 2.048502\n",
      "Epoch 97, loss: 2.080622\n",
      "Epoch 98, loss: 2.039529\n",
      "Epoch 99, loss: 2.149390\n",
      "Epoch 100, loss: 2.046582\n",
      "Epoch 101, loss: 2.158328\n",
      "Epoch 102, loss: 2.117538\n",
      "Epoch 103, loss: 2.134044\n",
      "Epoch 104, loss: 2.088649\n",
      "Epoch 105, loss: 2.078820\n",
      "Epoch 106, loss: 2.064468\n",
      "Epoch 107, loss: 2.112751\n",
      "Epoch 108, loss: 2.089519\n",
      "Epoch 109, loss: 2.072154\n",
      "Epoch 110, loss: 2.113021\n",
      "Epoch 111, loss: 2.099465\n",
      "Epoch 112, loss: 2.055271\n",
      "Epoch 113, loss: 2.031248\n",
      "Epoch 114, loss: 2.022513\n",
      "Epoch 115, loss: 2.049362\n",
      "Epoch 116, loss: 2.063513\n",
      "Epoch 117, loss: 2.074820\n",
      "Epoch 118, loss: 2.167153\n",
      "Epoch 119, loss: 2.095544\n",
      "Epoch 120, loss: 2.068409\n",
      "Epoch 121, loss: 2.099409\n",
      "Epoch 122, loss: 2.160390\n",
      "Epoch 123, loss: 2.047099\n",
      "Epoch 124, loss: 2.053727\n",
      "Epoch 125, loss: 2.157217\n",
      "Epoch 126, loss: 2.051314\n",
      "Epoch 127, loss: 2.095449\n",
      "Epoch 128, loss: 2.056231\n",
      "Epoch 129, loss: 2.107162\n",
      "Epoch 130, loss: 2.068688\n",
      "Epoch 131, loss: 2.087114\n",
      "Epoch 132, loss: 2.153454\n",
      "Epoch 133, loss: 2.052115\n",
      "Epoch 134, loss: 2.013922\n",
      "Epoch 135, loss: 2.054690\n",
      "Epoch 136, loss: 2.085103\n",
      "Epoch 137, loss: 2.141048\n",
      "Epoch 138, loss: 2.143272\n",
      "Epoch 139, loss: 2.102953\n",
      "Epoch 140, loss: 2.077820\n",
      "Epoch 141, loss: 2.058175\n",
      "Epoch 142, loss: 2.085013\n",
      "Epoch 143, loss: 2.030585\n",
      "Epoch 144, loss: 2.066847\n",
      "Epoch 145, loss: 2.054667\n",
      "Epoch 146, loss: 2.124170\n",
      "Epoch 147, loss: 2.058056\n",
      "Epoch 148, loss: 2.053567\n",
      "Epoch 149, loss: 2.070368\n",
      "Epoch 150, loss: 2.112443\n",
      "Epoch 151, loss: 2.083136\n",
      "Epoch 152, loss: 2.012029\n",
      "Epoch 153, loss: 2.090152\n",
      "Epoch 154, loss: 2.143337\n",
      "Epoch 155, loss: 2.040444\n",
      "Epoch 156, loss: 2.044630\n",
      "Epoch 157, loss: 2.120953\n",
      "Epoch 158, loss: 2.063678\n",
      "Epoch 159, loss: 2.086956\n",
      "Epoch 160, loss: 2.100432\n",
      "Epoch 161, loss: 2.040530\n",
      "Epoch 162, loss: 2.033011\n",
      "Epoch 163, loss: 2.088565\n",
      "Epoch 164, loss: 2.052013\n",
      "Epoch 165, loss: 2.088009\n",
      "Epoch 166, loss: 2.102126\n",
      "Epoch 167, loss: 1.997581\n",
      "Epoch 168, loss: 2.060210\n",
      "Epoch 169, loss: 2.068560\n",
      "Epoch 170, loss: 2.020540\n",
      "Epoch 171, loss: 2.038499\n",
      "Epoch 172, loss: 2.084820\n",
      "Epoch 173, loss: 2.052550\n",
      "Epoch 174, loss: 2.057726\n",
      "Epoch 175, loss: 2.017681\n",
      "Epoch 176, loss: 2.091613\n",
      "Epoch 177, loss: 2.063983\n",
      "Epoch 178, loss: 2.101263\n",
      "Epoch 179, loss: 2.095021\n",
      "Epoch 180, loss: 2.088345\n",
      "Epoch 181, loss: 2.086086\n",
      "Epoch 182, loss: 2.077029\n",
      "Epoch 183, loss: 1.999422\n",
      "Epoch 184, loss: 2.034397\n",
      "Epoch 185, loss: 2.135476\n",
      "Epoch 186, loss: 2.071588\n",
      "Epoch 187, loss: 2.039718\n",
      "Epoch 188, loss: 2.010005\n",
      "Epoch 189, loss: 2.031472\n",
      "Epoch 190, loss: 2.131405\n",
      "Epoch 191, loss: 2.052540\n",
      "Epoch 192, loss: 2.024662\n",
      "Epoch 193, loss: 2.028027\n",
      "Epoch 194, loss: 2.087106\n",
      "Epoch 195, loss: 2.054561\n",
      "Epoch 196, loss: 2.087859\n",
      "Epoch 197, loss: 2.010346\n",
      "Epoch 198, loss: 1.999698\n",
      "Epoch 199, loss: 2.041595\n",
      "Epoch 0, loss: 2.279314\n",
      "Epoch 1, loss: 2.265751\n",
      "Epoch 2, loss: 2.248287\n",
      "Epoch 3, loss: 2.233193\n",
      "Epoch 4, loss: 2.208269\n",
      "Epoch 5, loss: 2.196164\n",
      "Epoch 6, loss: 2.213132\n",
      "Epoch 7, loss: 2.226209\n",
      "Epoch 8, loss: 2.195684\n",
      "Epoch 9, loss: 2.198371\n",
      "Epoch 10, loss: 2.188909\n",
      "Epoch 11, loss: 2.186991\n",
      "Epoch 12, loss: 2.143996\n",
      "Epoch 13, loss: 2.170507\n",
      "Epoch 14, loss: 2.156675\n",
      "Epoch 15, loss: 2.152678\n",
      "Epoch 16, loss: 2.128759\n",
      "Epoch 17, loss: 2.148025\n",
      "Epoch 18, loss: 2.160170\n",
      "Epoch 19, loss: 2.142471\n",
      "Epoch 20, loss: 2.145524\n",
      "Epoch 21, loss: 2.176952\n",
      "Epoch 22, loss: 2.118815\n",
      "Epoch 23, loss: 2.179820\n",
      "Epoch 24, loss: 2.138143\n",
      "Epoch 25, loss: 2.118384\n",
      "Epoch 26, loss: 2.151398\n",
      "Epoch 27, loss: 2.171301\n",
      "Epoch 28, loss: 2.206395\n",
      "Epoch 29, loss: 2.186663\n",
      "Epoch 30, loss: 2.165915\n",
      "Epoch 31, loss: 2.127139\n",
      "Epoch 32, loss: 2.130930\n",
      "Epoch 33, loss: 2.147896\n",
      "Epoch 34, loss: 2.126287\n",
      "Epoch 35, loss: 2.080738\n",
      "Epoch 36, loss: 2.108775\n",
      "Epoch 37, loss: 2.111686\n",
      "Epoch 38, loss: 2.163258\n",
      "Epoch 39, loss: 2.130434\n",
      "Epoch 40, loss: 2.096274\n",
      "Epoch 41, loss: 2.141216\n",
      "Epoch 42, loss: 2.101663\n",
      "Epoch 43, loss: 2.120316\n",
      "Epoch 44, loss: 2.176231\n",
      "Epoch 45, loss: 2.146750\n",
      "Epoch 46, loss: 2.128501\n",
      "Epoch 47, loss: 2.076417\n",
      "Epoch 48, loss: 2.136251\n",
      "Epoch 49, loss: 2.055832\n",
      "Epoch 50, loss: 2.100925\n",
      "Epoch 51, loss: 2.117229\n",
      "Epoch 52, loss: 2.119388\n",
      "Epoch 53, loss: 2.085371\n",
      "Epoch 54, loss: 2.121488\n",
      "Epoch 55, loss: 2.101366\n",
      "Epoch 56, loss: 2.044570\n",
      "Epoch 57, loss: 2.173792\n",
      "Epoch 58, loss: 2.143038\n",
      "Epoch 59, loss: 2.130293\n",
      "Epoch 60, loss: 2.117842\n",
      "Epoch 61, loss: 2.071751\n",
      "Epoch 62, loss: 2.128748\n",
      "Epoch 63, loss: 2.082073\n",
      "Epoch 64, loss: 2.111885\n",
      "Epoch 65, loss: 2.180008\n",
      "Epoch 66, loss: 2.112763\n",
      "Epoch 67, loss: 2.109491\n",
      "Epoch 68, loss: 2.089068\n",
      "Epoch 69, loss: 2.094393\n",
      "Epoch 70, loss: 2.052681\n",
      "Epoch 71, loss: 2.116155\n",
      "Epoch 72, loss: 2.164229\n",
      "Epoch 73, loss: 2.043935\n",
      "Epoch 74, loss: 2.113724\n",
      "Epoch 75, loss: 2.106732\n",
      "Epoch 76, loss: 2.141815\n",
      "Epoch 77, loss: 2.111581\n",
      "Epoch 78, loss: 2.107889\n",
      "Epoch 79, loss: 2.125455\n",
      "Epoch 80, loss: 2.104328\n",
      "Epoch 81, loss: 2.102975\n",
      "Epoch 82, loss: 2.074130\n",
      "Epoch 83, loss: 2.060785\n",
      "Epoch 84, loss: 2.049028\n",
      "Epoch 85, loss: 2.074805\n",
      "Epoch 86, loss: 2.086201\n",
      "Epoch 87, loss: 2.101879\n",
      "Epoch 88, loss: 2.110563\n",
      "Epoch 89, loss: 2.045698\n",
      "Epoch 90, loss: 2.084063\n",
      "Epoch 91, loss: 2.097651\n",
      "Epoch 92, loss: 2.116105\n",
      "Epoch 93, loss: 2.104320\n",
      "Epoch 94, loss: 2.059978\n",
      "Epoch 95, loss: 2.063857\n",
      "Epoch 96, loss: 2.054908\n",
      "Epoch 97, loss: 2.080994\n",
      "Epoch 98, loss: 2.117743\n",
      "Epoch 99, loss: 2.145941\n",
      "Epoch 100, loss: 2.117212\n",
      "Epoch 101, loss: 2.078689\n",
      "Epoch 102, loss: 2.099021\n",
      "Epoch 103, loss: 2.045060\n",
      "Epoch 104, loss: 2.146458\n",
      "Epoch 105, loss: 2.093882\n",
      "Epoch 106, loss: 2.125647\n",
      "Epoch 107, loss: 2.050177\n",
      "Epoch 108, loss: 2.101582\n",
      "Epoch 109, loss: 2.018488\n",
      "Epoch 110, loss: 2.048427\n",
      "Epoch 111, loss: 2.094028\n",
      "Epoch 112, loss: 2.085953\n",
      "Epoch 113, loss: 2.078988\n",
      "Epoch 114, loss: 2.050528\n",
      "Epoch 115, loss: 2.086379\n",
      "Epoch 116, loss: 2.013589\n",
      "Epoch 117, loss: 2.048005\n",
      "Epoch 118, loss: 2.101752\n",
      "Epoch 119, loss: 2.102193\n",
      "Epoch 120, loss: 2.040565\n",
      "Epoch 121, loss: 2.104474\n",
      "Epoch 122, loss: 2.020118\n",
      "Epoch 123, loss: 2.008568\n",
      "Epoch 124, loss: 2.057661\n",
      "Epoch 125, loss: 2.057088\n",
      "Epoch 126, loss: 2.019432\n",
      "Epoch 127, loss: 2.135609\n",
      "Epoch 128, loss: 2.093530\n",
      "Epoch 129, loss: 2.105965\n",
      "Epoch 130, loss: 1.990762\n",
      "Epoch 131, loss: 2.098833\n",
      "Epoch 132, loss: 2.122783\n",
      "Epoch 133, loss: 2.067788\n",
      "Epoch 134, loss: 2.107489\n",
      "Epoch 135, loss: 2.056850\n",
      "Epoch 136, loss: 2.021109\n",
      "Epoch 137, loss: 2.097750\n",
      "Epoch 138, loss: 2.089897\n",
      "Epoch 139, loss: 2.083582\n",
      "Epoch 140, loss: 2.044229\n",
      "Epoch 141, loss: 2.037298\n",
      "Epoch 142, loss: 2.061941\n",
      "Epoch 143, loss: 2.091837\n",
      "Epoch 144, loss: 2.111324\n",
      "Epoch 145, loss: 2.065751\n",
      "Epoch 146, loss: 2.062516\n",
      "Epoch 147, loss: 2.112710\n",
      "Epoch 148, loss: 2.116133\n",
      "Epoch 149, loss: 2.051705\n",
      "Epoch 150, loss: 2.079846\n",
      "Epoch 151, loss: 2.095618\n",
      "Epoch 152, loss: 2.005976\n",
      "Epoch 153, loss: 2.062546\n",
      "Epoch 154, loss: 2.033736\n",
      "Epoch 155, loss: 1.985183\n",
      "Epoch 156, loss: 2.040442\n",
      "Epoch 157, loss: 2.025523\n",
      "Epoch 158, loss: 2.106135\n",
      "Epoch 159, loss: 2.067646\n",
      "Epoch 160, loss: 2.046474\n",
      "Epoch 161, loss: 2.048782\n",
      "Epoch 162, loss: 2.106046\n",
      "Epoch 163, loss: 2.073919\n",
      "Epoch 164, loss: 2.039051\n",
      "Epoch 165, loss: 2.090454\n",
      "Epoch 166, loss: 2.066111\n",
      "Epoch 167, loss: 2.032985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168, loss: 2.070551\n",
      "Epoch 169, loss: 2.097072\n",
      "Epoch 170, loss: 2.028862\n",
      "Epoch 171, loss: 2.039372\n",
      "Epoch 172, loss: 2.054511\n",
      "Epoch 173, loss: 2.005183\n",
      "Epoch 174, loss: 2.065396\n",
      "Epoch 175, loss: 2.072651\n",
      "Epoch 176, loss: 2.129909\n",
      "Epoch 177, loss: 2.074307\n",
      "Epoch 178, loss: 2.117673\n",
      "Epoch 179, loss: 2.067675\n",
      "Epoch 180, loss: 2.070826\n",
      "Epoch 181, loss: 2.109337\n",
      "Epoch 182, loss: 2.086747\n",
      "Epoch 183, loss: 2.016701\n",
      "Epoch 184, loss: 2.001804\n",
      "Epoch 185, loss: 2.108146\n",
      "Epoch 186, loss: 2.091441\n",
      "Epoch 187, loss: 2.012533\n",
      "Epoch 188, loss: 2.132479\n",
      "Epoch 189, loss: 2.061717\n",
      "Epoch 190, loss: 2.045296\n",
      "Epoch 191, loss: 2.112917\n",
      "Epoch 192, loss: 2.039633\n",
      "Epoch 193, loss: 2.123433\n",
      "Epoch 194, loss: 2.086855\n",
      "Epoch 195, loss: 2.026573\n",
      "Epoch 196, loss: 2.099805\n",
      "Epoch 197, loss: 2.033634\n",
      "Epoch 198, loss: 2.094018\n",
      "Epoch 199, loss: 2.021376\n",
      "Epoch 0, loss: 2.281518\n",
      "Epoch 1, loss: 2.273484\n",
      "Epoch 2, loss: 2.242905\n",
      "Epoch 3, loss: 2.243645\n",
      "Epoch 4, loss: 2.205398\n",
      "Epoch 5, loss: 2.188143\n",
      "Epoch 6, loss: 2.197801\n",
      "Epoch 7, loss: 2.207891\n",
      "Epoch 8, loss: 2.162752\n",
      "Epoch 9, loss: 2.214603\n",
      "Epoch 10, loss: 2.220343\n",
      "Epoch 11, loss: 2.171236\n",
      "Epoch 12, loss: 2.170691\n",
      "Epoch 13, loss: 2.164810\n",
      "Epoch 14, loss: 2.164960\n",
      "Epoch 15, loss: 2.165031\n",
      "Epoch 16, loss: 2.131655\n",
      "Epoch 17, loss: 2.203369\n",
      "Epoch 18, loss: 2.184074\n",
      "Epoch 19, loss: 2.141929\n",
      "Epoch 20, loss: 2.184833\n",
      "Epoch 21, loss: 2.156053\n",
      "Epoch 22, loss: 2.160372\n",
      "Epoch 23, loss: 2.123598\n",
      "Epoch 24, loss: 2.165994\n",
      "Epoch 25, loss: 2.091926\n",
      "Epoch 26, loss: 2.137897\n",
      "Epoch 27, loss: 2.131187\n",
      "Epoch 28, loss: 2.125041\n",
      "Epoch 29, loss: 2.122095\n",
      "Epoch 30, loss: 2.138456\n",
      "Epoch 31, loss: 2.172154\n",
      "Epoch 32, loss: 2.126373\n",
      "Epoch 33, loss: 2.091158\n",
      "Epoch 34, loss: 2.184256\n",
      "Epoch 35, loss: 2.149561\n",
      "Epoch 36, loss: 2.110284\n",
      "Epoch 37, loss: 2.178976\n",
      "Epoch 38, loss: 2.141270\n",
      "Epoch 39, loss: 2.138092\n",
      "Epoch 40, loss: 2.098823\n",
      "Epoch 41, loss: 2.156061\n",
      "Epoch 42, loss: 2.111631\n",
      "Epoch 43, loss: 2.129205\n",
      "Epoch 44, loss: 2.113206\n",
      "Epoch 45, loss: 2.099629\n",
      "Epoch 46, loss: 2.125807\n",
      "Epoch 47, loss: 2.134102\n",
      "Epoch 48, loss: 2.141165\n",
      "Epoch 49, loss: 2.063003\n",
      "Epoch 50, loss: 2.102057\n",
      "Epoch 51, loss: 2.078223\n",
      "Epoch 52, loss: 2.160579\n",
      "Epoch 53, loss: 2.079641\n",
      "Epoch 54, loss: 2.142998\n",
      "Epoch 55, loss: 2.117013\n",
      "Epoch 56, loss: 2.073115\n",
      "Epoch 57, loss: 2.121043\n",
      "Epoch 58, loss: 2.107356\n",
      "Epoch 59, loss: 2.146634\n",
      "Epoch 60, loss: 2.050359\n",
      "Epoch 61, loss: 2.061093\n",
      "Epoch 62, loss: 2.122198\n",
      "Epoch 63, loss: 2.112761\n",
      "Epoch 64, loss: 2.076759\n",
      "Epoch 65, loss: 2.089512\n",
      "Epoch 66, loss: 2.163236\n",
      "Epoch 67, loss: 2.108578\n",
      "Epoch 68, loss: 2.107933\n",
      "Epoch 69, loss: 2.105239\n",
      "Epoch 70, loss: 2.086372\n",
      "Epoch 71, loss: 2.131683\n",
      "Epoch 72, loss: 2.097611\n",
      "Epoch 73, loss: 2.081162\n",
      "Epoch 74, loss: 2.065991\n",
      "Epoch 75, loss: 2.061665\n",
      "Epoch 76, loss: 2.109180\n",
      "Epoch 77, loss: 2.105914\n",
      "Epoch 78, loss: 2.142889\n",
      "Epoch 79, loss: 2.080823\n",
      "Epoch 80, loss: 2.117754\n",
      "Epoch 81, loss: 2.098983\n",
      "Epoch 82, loss: 2.077732\n",
      "Epoch 83, loss: 2.106418\n",
      "Epoch 84, loss: 2.099133\n",
      "Epoch 85, loss: 2.077992\n",
      "Epoch 86, loss: 2.056408\n",
      "Epoch 87, loss: 2.104708\n",
      "Epoch 88, loss: 2.140365\n",
      "Epoch 89, loss: 2.042255\n",
      "Epoch 90, loss: 2.049315\n",
      "Epoch 91, loss: 2.080107\n",
      "Epoch 92, loss: 2.131809\n",
      "Epoch 93, loss: 2.168272\n",
      "Epoch 94, loss: 2.070700\n",
      "Epoch 95, loss: 2.117692\n",
      "Epoch 96, loss: 2.066291\n",
      "Epoch 97, loss: 2.070930\n",
      "Epoch 98, loss: 2.100524\n",
      "Epoch 99, loss: 2.137191\n",
      "Epoch 100, loss: 2.041521\n",
      "Epoch 101, loss: 2.145386\n",
      "Epoch 102, loss: 2.041769\n",
      "Epoch 103, loss: 2.074584\n",
      "Epoch 104, loss: 2.066104\n",
      "Epoch 105, loss: 2.071713\n",
      "Epoch 106, loss: 2.131565\n",
      "Epoch 107, loss: 2.030081\n",
      "Epoch 108, loss: 2.115422\n",
      "Epoch 109, loss: 2.083098\n",
      "Epoch 110, loss: 2.115950\n",
      "Epoch 111, loss: 2.050914\n",
      "Epoch 112, loss: 2.077977\n",
      "Epoch 113, loss: 2.083396\n",
      "Epoch 114, loss: 2.096505\n",
      "Epoch 115, loss: 2.106600\n",
      "Epoch 116, loss: 2.085045\n",
      "Epoch 117, loss: 2.094906\n",
      "Epoch 118, loss: 2.094006\n",
      "Epoch 119, loss: 2.122587\n",
      "Epoch 120, loss: 2.056509\n",
      "Epoch 121, loss: 2.151622\n",
      "Epoch 122, loss: 2.096099\n",
      "Epoch 123, loss: 2.091802\n",
      "Epoch 124, loss: 2.106169\n",
      "Epoch 125, loss: 2.051337\n",
      "Epoch 126, loss: 2.054539\n",
      "Epoch 127, loss: 2.039241\n",
      "Epoch 128, loss: 2.098497\n",
      "Epoch 129, loss: 2.088737\n",
      "Epoch 130, loss: 2.040550\n",
      "Epoch 131, loss: 2.055985\n",
      "Epoch 132, loss: 2.100001\n",
      "Epoch 133, loss: 2.062927\n",
      "Epoch 134, loss: 2.070256\n",
      "Epoch 135, loss: 2.069321\n",
      "Epoch 136, loss: 2.162152\n",
      "Epoch 137, loss: 2.067078\n",
      "Epoch 138, loss: 2.089894\n",
      "Epoch 139, loss: 2.062161\n",
      "Epoch 140, loss: 2.069646\n",
      "Epoch 141, loss: 2.150988\n",
      "Epoch 142, loss: 2.112867\n",
      "Epoch 143, loss: 2.170435\n",
      "Epoch 144, loss: 2.070943\n",
      "Epoch 145, loss: 2.035096\n",
      "Epoch 146, loss: 2.083417\n",
      "Epoch 147, loss: 2.135929\n",
      "Epoch 148, loss: 2.050005\n",
      "Epoch 149, loss: 2.059191\n",
      "Epoch 150, loss: 2.096070\n",
      "Epoch 151, loss: 2.046771\n",
      "Epoch 152, loss: 2.035520\n",
      "Epoch 153, loss: 2.128156\n",
      "Epoch 154, loss: 2.118133\n",
      "Epoch 155, loss: 2.057668\n",
      "Epoch 156, loss: 2.043632\n",
      "Epoch 157, loss: 2.098271\n",
      "Epoch 158, loss: 2.138611\n",
      "Epoch 159, loss: 2.096044\n",
      "Epoch 160, loss: 2.038280\n",
      "Epoch 161, loss: 2.050405\n",
      "Epoch 162, loss: 2.080433\n",
      "Epoch 163, loss: 2.073704\n",
      "Epoch 164, loss: 2.055800\n",
      "Epoch 165, loss: 2.040507\n",
      "Epoch 166, loss: 2.131153\n",
      "Epoch 167, loss: 2.086915\n",
      "Epoch 168, loss: 1.989155\n",
      "Epoch 169, loss: 2.053939\n",
      "Epoch 170, loss: 2.034507\n",
      "Epoch 171, loss: 2.079306\n",
      "Epoch 172, loss: 2.025269\n",
      "Epoch 173, loss: 2.052127\n",
      "Epoch 174, loss: 1.997191\n",
      "Epoch 175, loss: 2.059047\n",
      "Epoch 176, loss: 2.074153\n",
      "Epoch 177, loss: 2.065572\n",
      "Epoch 178, loss: 2.029196\n",
      "Epoch 179, loss: 2.070341\n",
      "Epoch 180, loss: 2.064222\n",
      "Epoch 181, loss: 2.045157\n",
      "Epoch 182, loss: 2.023556\n",
      "Epoch 183, loss: 2.052732\n",
      "Epoch 184, loss: 2.147310\n",
      "Epoch 185, loss: 2.031253\n",
      "Epoch 186, loss: 2.090387\n",
      "Epoch 187, loss: 2.074133\n",
      "Epoch 188, loss: 2.016289\n",
      "Epoch 189, loss: 2.019683\n",
      "Epoch 190, loss: 2.085425\n",
      "Epoch 191, loss: 2.073659\n",
      "Epoch 192, loss: 2.047173\n",
      "Epoch 193, loss: 2.033612\n",
      "Epoch 194, loss: 2.021902\n",
      "Epoch 195, loss: 2.046766\n",
      "Epoch 196, loss: 2.052458\n",
      "Epoch 197, loss: 2.062206\n",
      "Epoch 198, loss: 2.072899\n",
      "Epoch 199, loss: 2.080965\n",
      "Epoch 0, loss: 2.300621\n",
      "Epoch 1, loss: 2.297693\n",
      "Epoch 2, loss: 2.296675\n",
      "Epoch 3, loss: 2.293887\n",
      "Epoch 4, loss: 2.286722\n",
      "Epoch 5, loss: 2.286183\n",
      "Epoch 6, loss: 2.281293\n",
      "Epoch 7, loss: 2.282346\n",
      "Epoch 8, loss: 2.278997\n",
      "Epoch 9, loss: 2.277697\n",
      "Epoch 10, loss: 2.268740\n",
      "Epoch 11, loss: 2.270063\n",
      "Epoch 12, loss: 2.279039\n",
      "Epoch 13, loss: 2.268007\n",
      "Epoch 14, loss: 2.262158\n",
      "Epoch 15, loss: 2.271232\n",
      "Epoch 16, loss: 2.253335\n",
      "Epoch 17, loss: 2.267167\n",
      "Epoch 18, loss: 2.253103\n",
      "Epoch 19, loss: 2.269983\n",
      "Epoch 20, loss: 2.261713\n",
      "Epoch 21, loss: 2.256274\n",
      "Epoch 22, loss: 2.255526\n",
      "Epoch 23, loss: 2.256757\n",
      "Epoch 24, loss: 2.248442\n",
      "Epoch 25, loss: 2.258030\n",
      "Epoch 26, loss: 2.262168\n",
      "Epoch 27, loss: 2.243127\n",
      "Epoch 28, loss: 2.245232\n",
      "Epoch 29, loss: 2.247787\n",
      "Epoch 30, loss: 2.229203\n",
      "Epoch 31, loss: 2.240846\n",
      "Epoch 32, loss: 2.224985\n",
      "Epoch 33, loss: 2.242683\n",
      "Epoch 34, loss: 2.224537\n",
      "Epoch 35, loss: 2.224398\n",
      "Epoch 36, loss: 2.238437\n",
      "Epoch 37, loss: 2.240798\n",
      "Epoch 38, loss: 2.223130\n",
      "Epoch 39, loss: 2.215874\n",
      "Epoch 40, loss: 2.223273\n",
      "Epoch 41, loss: 2.230306\n",
      "Epoch 42, loss: 2.232945\n",
      "Epoch 43, loss: 2.230118\n",
      "Epoch 44, loss: 2.239475\n",
      "Epoch 45, loss: 2.221323\n",
      "Epoch 46, loss: 2.236705\n",
      "Epoch 47, loss: 2.199690\n",
      "Epoch 48, loss: 2.203327\n",
      "Epoch 49, loss: 2.213956\n",
      "Epoch 50, loss: 2.232307\n",
      "Epoch 51, loss: 2.213629\n",
      "Epoch 52, loss: 2.200697\n",
      "Epoch 53, loss: 2.213166\n",
      "Epoch 54, loss: 2.227652\n",
      "Epoch 55, loss: 2.214031\n",
      "Epoch 56, loss: 2.213042\n",
      "Epoch 57, loss: 2.211947\n",
      "Epoch 58, loss: 2.196639\n",
      "Epoch 59, loss: 2.191410\n",
      "Epoch 60, loss: 2.192538\n",
      "Epoch 61, loss: 2.212972\n",
      "Epoch 62, loss: 2.181238\n",
      "Epoch 63, loss: 2.228532\n",
      "Epoch 64, loss: 2.208943\n",
      "Epoch 65, loss: 2.181077\n",
      "Epoch 66, loss: 2.166083\n",
      "Epoch 67, loss: 2.230558\n",
      "Epoch 68, loss: 2.225514\n",
      "Epoch 69, loss: 2.208603\n",
      "Epoch 70, loss: 2.198771\n",
      "Epoch 71, loss: 2.186045\n",
      "Epoch 72, loss: 2.198669\n",
      "Epoch 73, loss: 2.206577\n",
      "Epoch 74, loss: 2.193175\n",
      "Epoch 75, loss: 2.168680\n",
      "Epoch 76, loss: 2.227788\n",
      "Epoch 77, loss: 2.194680\n",
      "Epoch 78, loss: 2.209692\n",
      "Epoch 79, loss: 2.179600\n",
      "Epoch 80, loss: 2.208021\n",
      "Epoch 81, loss: 2.215923\n",
      "Epoch 82, loss: 2.204973\n",
      "Epoch 83, loss: 2.175581\n",
      "Epoch 84, loss: 2.205469\n",
      "Epoch 85, loss: 2.181006\n",
      "Epoch 86, loss: 2.175028\n",
      "Epoch 87, loss: 2.208593\n",
      "Epoch 88, loss: 2.163117\n",
      "Epoch 89, loss: 2.182201\n",
      "Epoch 90, loss: 2.197962\n",
      "Epoch 91, loss: 2.156516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, loss: 2.197452\n",
      "Epoch 93, loss: 2.172371\n",
      "Epoch 94, loss: 2.188333\n",
      "Epoch 95, loss: 2.196475\n",
      "Epoch 96, loss: 2.216227\n",
      "Epoch 97, loss: 2.156489\n",
      "Epoch 98, loss: 2.179360\n",
      "Epoch 99, loss: 2.197445\n",
      "Epoch 100, loss: 2.178636\n",
      "Epoch 101, loss: 2.174636\n",
      "Epoch 102, loss: 2.180949\n",
      "Epoch 103, loss: 2.209826\n",
      "Epoch 104, loss: 2.157914\n",
      "Epoch 105, loss: 2.170643\n",
      "Epoch 106, loss: 2.168740\n",
      "Epoch 107, loss: 2.186677\n",
      "Epoch 108, loss: 2.162695\n",
      "Epoch 109, loss: 2.195905\n",
      "Epoch 110, loss: 2.175338\n",
      "Epoch 111, loss: 2.174121\n",
      "Epoch 112, loss: 2.204222\n",
      "Epoch 113, loss: 2.163301\n",
      "Epoch 114, loss: 2.184417\n",
      "Epoch 115, loss: 2.163393\n",
      "Epoch 116, loss: 2.175641\n",
      "Epoch 117, loss: 2.203071\n",
      "Epoch 118, loss: 2.187460\n",
      "Epoch 119, loss: 2.183492\n",
      "Epoch 120, loss: 2.174948\n",
      "Epoch 121, loss: 2.194048\n",
      "Epoch 122, loss: 2.169533\n",
      "Epoch 123, loss: 2.201330\n",
      "Epoch 124, loss: 2.161884\n",
      "Epoch 125, loss: 2.186074\n",
      "Epoch 126, loss: 2.175711\n",
      "Epoch 127, loss: 2.173117\n",
      "Epoch 128, loss: 2.170829\n",
      "Epoch 129, loss: 2.156991\n",
      "Epoch 130, loss: 2.167988\n",
      "Epoch 131, loss: 2.144567\n",
      "Epoch 132, loss: 2.147237\n",
      "Epoch 133, loss: 2.158860\n",
      "Epoch 134, loss: 2.172436\n",
      "Epoch 135, loss: 2.161396\n",
      "Epoch 136, loss: 2.139788\n",
      "Epoch 137, loss: 2.184206\n",
      "Epoch 138, loss: 2.131406\n",
      "Epoch 139, loss: 2.148250\n",
      "Epoch 140, loss: 2.191469\n",
      "Epoch 141, loss: 2.193378\n",
      "Epoch 142, loss: 2.157061\n",
      "Epoch 143, loss: 2.138344\n",
      "Epoch 144, loss: 2.201605\n",
      "Epoch 145, loss: 2.170457\n",
      "Epoch 146, loss: 2.164138\n",
      "Epoch 147, loss: 2.180511\n",
      "Epoch 148, loss: 2.138544\n",
      "Epoch 149, loss: 2.163929\n",
      "Epoch 150, loss: 2.179702\n",
      "Epoch 151, loss: 2.172937\n",
      "Epoch 152, loss: 2.180484\n",
      "Epoch 153, loss: 2.200166\n",
      "Epoch 154, loss: 2.135051\n",
      "Epoch 155, loss: 2.150573\n",
      "Epoch 156, loss: 2.164642\n",
      "Epoch 157, loss: 2.166801\n",
      "Epoch 158, loss: 2.171523\n",
      "Epoch 159, loss: 2.138440\n",
      "Epoch 160, loss: 2.145275\n",
      "Epoch 161, loss: 2.162014\n",
      "Epoch 162, loss: 2.141564\n",
      "Epoch 163, loss: 2.185739\n",
      "Epoch 164, loss: 2.130276\n",
      "Epoch 165, loss: 2.153943\n",
      "Epoch 166, loss: 2.182036\n",
      "Epoch 167, loss: 2.115860\n",
      "Epoch 168, loss: 2.162064\n",
      "Epoch 169, loss: 2.131891\n",
      "Epoch 170, loss: 2.192835\n",
      "Epoch 171, loss: 2.150546\n",
      "Epoch 172, loss: 2.154219\n",
      "Epoch 173, loss: 2.129515\n",
      "Epoch 174, loss: 2.177541\n",
      "Epoch 175, loss: 2.148626\n",
      "Epoch 176, loss: 2.152037\n",
      "Epoch 177, loss: 2.129838\n",
      "Epoch 178, loss: 2.146848\n",
      "Epoch 179, loss: 2.176580\n",
      "Epoch 180, loss: 2.169461\n",
      "Epoch 181, loss: 2.122783\n",
      "Epoch 182, loss: 2.164537\n",
      "Epoch 183, loss: 2.150295\n",
      "Epoch 184, loss: 2.160088\n",
      "Epoch 185, loss: 2.172400\n",
      "Epoch 186, loss: 2.194594\n",
      "Epoch 187, loss: 2.166316\n",
      "Epoch 188, loss: 2.124452\n",
      "Epoch 189, loss: 2.123703\n",
      "Epoch 190, loss: 2.118192\n",
      "Epoch 191, loss: 2.133872\n",
      "Epoch 192, loss: 2.146423\n",
      "Epoch 193, loss: 2.140632\n",
      "Epoch 194, loss: 2.161820\n",
      "Epoch 195, loss: 2.130854\n",
      "Epoch 196, loss: 2.156038\n",
      "Epoch 197, loss: 2.136739\n",
      "Epoch 198, loss: 2.135799\n",
      "Epoch 199, loss: 2.180499\n",
      "Epoch 0, loss: 2.301172\n",
      "Epoch 1, loss: 2.297744\n",
      "Epoch 2, loss: 2.295505\n",
      "Epoch 3, loss: 2.288978\n",
      "Epoch 4, loss: 2.290834\n",
      "Epoch 5, loss: 2.286927\n",
      "Epoch 6, loss: 2.283446\n",
      "Epoch 7, loss: 2.282946\n",
      "Epoch 8, loss: 2.279753\n",
      "Epoch 9, loss: 2.278060\n",
      "Epoch 10, loss: 2.281233\n",
      "Epoch 11, loss: 2.270096\n",
      "Epoch 12, loss: 2.268177\n",
      "Epoch 13, loss: 2.277262\n",
      "Epoch 14, loss: 2.271100\n",
      "Epoch 15, loss: 2.264404\n",
      "Epoch 16, loss: 2.275931\n",
      "Epoch 17, loss: 2.270197\n",
      "Epoch 18, loss: 2.262636\n",
      "Epoch 19, loss: 2.257921\n",
      "Epoch 20, loss: 2.254353\n",
      "Epoch 21, loss: 2.256604\n",
      "Epoch 22, loss: 2.251369\n",
      "Epoch 23, loss: 2.252452\n",
      "Epoch 24, loss: 2.244960\n",
      "Epoch 25, loss: 2.258576\n",
      "Epoch 26, loss: 2.243570\n",
      "Epoch 27, loss: 2.246171\n",
      "Epoch 28, loss: 2.258770\n",
      "Epoch 29, loss: 2.257061\n",
      "Epoch 30, loss: 2.250927\n",
      "Epoch 31, loss: 2.234299\n",
      "Epoch 32, loss: 2.217639\n",
      "Epoch 33, loss: 2.249214\n",
      "Epoch 34, loss: 2.257732\n",
      "Epoch 35, loss: 2.239111\n",
      "Epoch 36, loss: 2.235513\n",
      "Epoch 37, loss: 2.230835\n",
      "Epoch 38, loss: 2.236210\n",
      "Epoch 39, loss: 2.216753\n",
      "Epoch 40, loss: 2.241909\n",
      "Epoch 41, loss: 2.221520\n",
      "Epoch 42, loss: 2.229556\n",
      "Epoch 43, loss: 2.232701\n",
      "Epoch 44, loss: 2.207994\n",
      "Epoch 45, loss: 2.225656\n",
      "Epoch 46, loss: 2.242146\n",
      "Epoch 47, loss: 2.222054\n",
      "Epoch 48, loss: 2.226741\n",
      "Epoch 49, loss: 2.211121\n",
      "Epoch 50, loss: 2.218209\n",
      "Epoch 51, loss: 2.233017\n",
      "Epoch 52, loss: 2.220925\n",
      "Epoch 53, loss: 2.210601\n",
      "Epoch 54, loss: 2.218223\n",
      "Epoch 55, loss: 2.201263\n",
      "Epoch 56, loss: 2.188676\n",
      "Epoch 57, loss: 2.206803\n",
      "Epoch 58, loss: 2.186674\n",
      "Epoch 59, loss: 2.249220\n",
      "Epoch 60, loss: 2.200480\n",
      "Epoch 61, loss: 2.207156\n",
      "Epoch 62, loss: 2.188086\n",
      "Epoch 63, loss: 2.211311\n",
      "Epoch 64, loss: 2.200909\n",
      "Epoch 65, loss: 2.203607\n",
      "Epoch 66, loss: 2.213380\n",
      "Epoch 67, loss: 2.205282\n",
      "Epoch 68, loss: 2.219419\n",
      "Epoch 69, loss: 2.214721\n",
      "Epoch 70, loss: 2.177353\n",
      "Epoch 71, loss: 2.195071\n",
      "Epoch 72, loss: 2.203121\n",
      "Epoch 73, loss: 2.192905\n",
      "Epoch 74, loss: 2.190320\n",
      "Epoch 75, loss: 2.180987\n",
      "Epoch 76, loss: 2.205339\n",
      "Epoch 77, loss: 2.187662\n",
      "Epoch 78, loss: 2.201021\n",
      "Epoch 79, loss: 2.200387\n",
      "Epoch 80, loss: 2.222363\n",
      "Epoch 81, loss: 2.183039\n",
      "Epoch 82, loss: 2.203048\n",
      "Epoch 83, loss: 2.206128\n",
      "Epoch 84, loss: 2.231676\n",
      "Epoch 85, loss: 2.184379\n",
      "Epoch 86, loss: 2.179336\n",
      "Epoch 87, loss: 2.194242\n",
      "Epoch 88, loss: 2.184865\n",
      "Epoch 89, loss: 2.172523\n",
      "Epoch 90, loss: 2.187640\n",
      "Epoch 91, loss: 2.176312\n",
      "Epoch 92, loss: 2.188285\n",
      "Epoch 93, loss: 2.190455\n",
      "Epoch 94, loss: 2.207371\n",
      "Epoch 95, loss: 2.184092\n",
      "Epoch 96, loss: 2.194156\n",
      "Epoch 97, loss: 2.142767\n",
      "Epoch 98, loss: 2.173653\n",
      "Epoch 99, loss: 2.161862\n",
      "Epoch 100, loss: 2.209671\n",
      "Epoch 101, loss: 2.178990\n",
      "Epoch 102, loss: 2.146807\n",
      "Epoch 103, loss: 2.192765\n",
      "Epoch 104, loss: 2.153401\n",
      "Epoch 105, loss: 2.175836\n",
      "Epoch 106, loss: 2.139330\n",
      "Epoch 107, loss: 2.166220\n",
      "Epoch 108, loss: 2.159322\n",
      "Epoch 109, loss: 2.172042\n",
      "Epoch 110, loss: 2.118893\n",
      "Epoch 111, loss: 2.234225\n",
      "Epoch 112, loss: 2.150267\n",
      "Epoch 113, loss: 2.150644\n",
      "Epoch 114, loss: 2.146425\n",
      "Epoch 115, loss: 2.173002\n",
      "Epoch 116, loss: 2.176912\n",
      "Epoch 117, loss: 2.207521\n",
      "Epoch 118, loss: 2.175110\n",
      "Epoch 119, loss: 2.226546\n",
      "Epoch 120, loss: 2.154948\n",
      "Epoch 121, loss: 2.132079\n",
      "Epoch 122, loss: 2.132729\n",
      "Epoch 123, loss: 2.199366\n",
      "Epoch 124, loss: 2.146805\n",
      "Epoch 125, loss: 2.177985\n",
      "Epoch 126, loss: 2.197821\n",
      "Epoch 127, loss: 2.177127\n",
      "Epoch 128, loss: 2.153609\n",
      "Epoch 129, loss: 2.188551\n",
      "Epoch 130, loss: 2.163547\n",
      "Epoch 131, loss: 2.152935\n",
      "Epoch 132, loss: 2.197450\n",
      "Epoch 133, loss: 2.190964\n",
      "Epoch 134, loss: 2.149983\n",
      "Epoch 135, loss: 2.151732\n",
      "Epoch 136, loss: 2.198027\n",
      "Epoch 137, loss: 2.195266\n",
      "Epoch 138, loss: 2.123659\n",
      "Epoch 139, loss: 2.147047\n",
      "Epoch 140, loss: 2.160075\n",
      "Epoch 141, loss: 2.145312\n",
      "Epoch 142, loss: 2.185280\n",
      "Epoch 143, loss: 2.120408\n",
      "Epoch 144, loss: 2.198748\n",
      "Epoch 145, loss: 2.156266\n",
      "Epoch 146, loss: 2.178157\n",
      "Epoch 147, loss: 2.184927\n",
      "Epoch 148, loss: 2.153889\n",
      "Epoch 149, loss: 2.163936\n",
      "Epoch 150, loss: 2.165135\n",
      "Epoch 151, loss: 2.191202\n",
      "Epoch 152, loss: 2.181384\n",
      "Epoch 153, loss: 2.154228\n",
      "Epoch 154, loss: 2.156846\n",
      "Epoch 155, loss: 2.196981\n",
      "Epoch 156, loss: 2.143942\n",
      "Epoch 157, loss: 2.144773\n",
      "Epoch 158, loss: 2.191503\n",
      "Epoch 159, loss: 2.190583\n",
      "Epoch 160, loss: 2.172451\n",
      "Epoch 161, loss: 2.174454\n",
      "Epoch 162, loss: 2.144579\n",
      "Epoch 163, loss: 2.186704\n",
      "Epoch 164, loss: 2.134068\n",
      "Epoch 165, loss: 2.167048\n",
      "Epoch 166, loss: 2.123406\n",
      "Epoch 167, loss: 2.161655\n",
      "Epoch 168, loss: 2.135877\n",
      "Epoch 169, loss: 2.125202\n",
      "Epoch 170, loss: 2.129149\n",
      "Epoch 171, loss: 2.128875\n",
      "Epoch 172, loss: 2.202969\n",
      "Epoch 173, loss: 2.155733\n",
      "Epoch 174, loss: 2.145815\n",
      "Epoch 175, loss: 2.120872\n",
      "Epoch 176, loss: 2.151600\n",
      "Epoch 177, loss: 2.148968\n",
      "Epoch 178, loss: 2.139408\n",
      "Epoch 179, loss: 2.139479\n",
      "Epoch 180, loss: 2.132771\n",
      "Epoch 181, loss: 2.122654\n",
      "Epoch 182, loss: 2.150008\n",
      "Epoch 183, loss: 2.147290\n",
      "Epoch 184, loss: 2.157886\n",
      "Epoch 185, loss: 2.181402\n",
      "Epoch 186, loss: 2.105600\n",
      "Epoch 187, loss: 2.141105\n",
      "Epoch 188, loss: 2.117251\n",
      "Epoch 189, loss: 2.158859\n",
      "Epoch 190, loss: 2.165544\n",
      "Epoch 191, loss: 2.144732\n",
      "Epoch 192, loss: 2.123937\n",
      "Epoch 193, loss: 2.115816\n",
      "Epoch 194, loss: 2.185071\n",
      "Epoch 195, loss: 2.181902\n",
      "Epoch 196, loss: 2.154862\n",
      "Epoch 197, loss: 2.198387\n",
      "Epoch 198, loss: 2.142228\n",
      "Epoch 199, loss: 2.153058\n",
      "Epoch 0, loss: 2.300110\n",
      "Epoch 1, loss: 2.299465\n",
      "Epoch 2, loss: 2.295206\n",
      "Epoch 3, loss: 2.295390\n",
      "Epoch 4, loss: 2.291859\n",
      "Epoch 5, loss: 2.288866\n",
      "Epoch 6, loss: 2.289439\n",
      "Epoch 7, loss: 2.278921\n",
      "Epoch 8, loss: 2.274952\n",
      "Epoch 9, loss: 2.282415\n",
      "Epoch 10, loss: 2.268272\n",
      "Epoch 11, loss: 2.266767\n",
      "Epoch 12, loss: 2.278190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 2.267291\n",
      "Epoch 14, loss: 2.264956\n",
      "Epoch 15, loss: 2.272170\n",
      "Epoch 16, loss: 2.271150\n",
      "Epoch 17, loss: 2.258304\n",
      "Epoch 18, loss: 2.246255\n",
      "Epoch 19, loss: 2.264039\n",
      "Epoch 20, loss: 2.254469\n",
      "Epoch 21, loss: 2.266726\n",
      "Epoch 22, loss: 2.254263\n",
      "Epoch 23, loss: 2.267334\n",
      "Epoch 24, loss: 2.245500\n",
      "Epoch 25, loss: 2.270511\n",
      "Epoch 26, loss: 2.224535\n",
      "Epoch 27, loss: 2.254064\n",
      "Epoch 28, loss: 2.257938\n",
      "Epoch 29, loss: 2.241773\n",
      "Epoch 30, loss: 2.235945\n",
      "Epoch 31, loss: 2.242913\n",
      "Epoch 32, loss: 2.240205\n",
      "Epoch 33, loss: 2.223509\n",
      "Epoch 34, loss: 2.225385\n",
      "Epoch 35, loss: 2.247092\n",
      "Epoch 36, loss: 2.230702\n",
      "Epoch 37, loss: 2.223723\n",
      "Epoch 38, loss: 2.248617\n",
      "Epoch 39, loss: 2.248863\n",
      "Epoch 40, loss: 2.227141\n",
      "Epoch 41, loss: 2.249972\n",
      "Epoch 42, loss: 2.220165\n",
      "Epoch 43, loss: 2.214306\n",
      "Epoch 44, loss: 2.219201\n",
      "Epoch 45, loss: 2.233882\n",
      "Epoch 46, loss: 2.217473\n",
      "Epoch 47, loss: 2.211727\n",
      "Epoch 48, loss: 2.227779\n",
      "Epoch 49, loss: 2.214423\n",
      "Epoch 50, loss: 2.217461\n",
      "Epoch 51, loss: 2.214767\n",
      "Epoch 52, loss: 2.215996\n",
      "Epoch 53, loss: 2.201657\n",
      "Epoch 54, loss: 2.203650\n",
      "Epoch 55, loss: 2.208758\n",
      "Epoch 56, loss: 2.223670\n",
      "Epoch 57, loss: 2.227029\n",
      "Epoch 58, loss: 2.193324\n",
      "Epoch 59, loss: 2.180812\n",
      "Epoch 60, loss: 2.209887\n",
      "Epoch 61, loss: 2.214444\n",
      "Epoch 62, loss: 2.196087\n",
      "Epoch 63, loss: 2.223013\n",
      "Epoch 64, loss: 2.188124\n",
      "Epoch 65, loss: 2.212699\n",
      "Epoch 66, loss: 2.217258\n",
      "Epoch 67, loss: 2.181397\n",
      "Epoch 68, loss: 2.209629\n",
      "Epoch 69, loss: 2.189372\n",
      "Epoch 70, loss: 2.178020\n",
      "Epoch 71, loss: 2.151156\n",
      "Epoch 72, loss: 2.221526\n",
      "Epoch 73, loss: 2.174793\n",
      "Epoch 74, loss: 2.190902\n",
      "Epoch 75, loss: 2.194312\n",
      "Epoch 76, loss: 2.203352\n",
      "Epoch 77, loss: 2.165369\n",
      "Epoch 78, loss: 2.170406\n",
      "Epoch 79, loss: 2.202765\n",
      "Epoch 80, loss: 2.198918\n",
      "Epoch 81, loss: 2.178415\n",
      "Epoch 82, loss: 2.207963\n",
      "Epoch 83, loss: 2.182824\n",
      "Epoch 84, loss: 2.178845\n",
      "Epoch 85, loss: 2.197507\n",
      "Epoch 86, loss: 2.195191\n",
      "Epoch 87, loss: 2.141282\n",
      "Epoch 88, loss: 2.178534\n",
      "Epoch 89, loss: 2.178432\n",
      "Epoch 90, loss: 2.191049\n",
      "Epoch 91, loss: 2.176539\n",
      "Epoch 92, loss: 2.180718\n",
      "Epoch 93, loss: 2.175856\n",
      "Epoch 94, loss: 2.185853\n",
      "Epoch 95, loss: 2.163597\n",
      "Epoch 96, loss: 2.147813\n",
      "Epoch 97, loss: 2.182791\n",
      "Epoch 98, loss: 2.189478\n",
      "Epoch 99, loss: 2.182644\n",
      "Epoch 100, loss: 2.183768\n",
      "Epoch 101, loss: 2.212504\n",
      "Epoch 102, loss: 2.189434\n",
      "Epoch 103, loss: 2.158123\n",
      "Epoch 104, loss: 2.186160\n",
      "Epoch 105, loss: 2.178980\n",
      "Epoch 106, loss: 2.174881\n",
      "Epoch 107, loss: 2.192942\n",
      "Epoch 108, loss: 2.211022\n",
      "Epoch 109, loss: 2.167195\n",
      "Epoch 110, loss: 2.191691\n",
      "Epoch 111, loss: 2.167744\n",
      "Epoch 112, loss: 2.206179\n",
      "Epoch 113, loss: 2.191011\n",
      "Epoch 114, loss: 2.158136\n",
      "Epoch 115, loss: 2.158466\n",
      "Epoch 116, loss: 2.183238\n",
      "Epoch 117, loss: 2.138743\n",
      "Epoch 118, loss: 2.191428\n",
      "Epoch 119, loss: 2.150648\n",
      "Epoch 120, loss: 2.188708\n",
      "Epoch 121, loss: 2.129142\n",
      "Epoch 122, loss: 2.163593\n",
      "Epoch 123, loss: 2.135500\n",
      "Epoch 124, loss: 2.173801\n",
      "Epoch 125, loss: 2.141905\n",
      "Epoch 126, loss: 2.202109\n",
      "Epoch 127, loss: 2.186912\n",
      "Epoch 128, loss: 2.135184\n",
      "Epoch 129, loss: 2.186693\n",
      "Epoch 130, loss: 2.174061\n",
      "Epoch 131, loss: 2.146487\n",
      "Epoch 132, loss: 2.163776\n",
      "Epoch 133, loss: 2.154268\n",
      "Epoch 134, loss: 2.158269\n",
      "Epoch 135, loss: 2.196084\n",
      "Epoch 136, loss: 2.173894\n",
      "Epoch 137, loss: 2.172876\n",
      "Epoch 138, loss: 2.150157\n",
      "Epoch 139, loss: 2.189799\n",
      "Epoch 140, loss: 2.170002\n",
      "Epoch 141, loss: 2.146968\n",
      "Epoch 142, loss: 2.154159\n",
      "Epoch 143, loss: 2.167685\n",
      "Epoch 144, loss: 2.204737\n",
      "Epoch 145, loss: 2.185912\n",
      "Epoch 146, loss: 2.163577\n",
      "Epoch 147, loss: 2.181367\n",
      "Epoch 148, loss: 2.168651\n",
      "Epoch 149, loss: 2.159249\n",
      "Epoch 150, loss: 2.163947\n",
      "Epoch 151, loss: 2.133777\n",
      "Epoch 152, loss: 2.206534\n",
      "Epoch 153, loss: 2.154179\n",
      "Epoch 154, loss: 2.156250\n",
      "Epoch 155, loss: 2.182540\n",
      "Epoch 156, loss: 2.151189\n",
      "Epoch 157, loss: 2.196243\n",
      "Epoch 158, loss: 2.186183\n",
      "Epoch 159, loss: 2.127014\n",
      "Epoch 160, loss: 2.105928\n",
      "Epoch 161, loss: 2.190180\n",
      "Epoch 162, loss: 2.157986\n",
      "Epoch 163, loss: 2.161223\n",
      "Epoch 164, loss: 2.184541\n",
      "Epoch 165, loss: 2.147701\n",
      "Epoch 166, loss: 2.147802\n",
      "Epoch 167, loss: 2.198839\n",
      "Epoch 168, loss: 2.155257\n",
      "Epoch 169, loss: 2.215714\n",
      "Epoch 170, loss: 2.170362\n",
      "Epoch 171, loss: 2.215560\n",
      "Epoch 172, loss: 2.145633\n",
      "Epoch 173, loss: 2.113603\n",
      "Epoch 174, loss: 2.143514\n",
      "Epoch 175, loss: 2.141394\n",
      "Epoch 176, loss: 2.180766\n",
      "Epoch 177, loss: 2.105420\n",
      "Epoch 178, loss: 2.133895\n",
      "Epoch 179, loss: 2.139597\n",
      "Epoch 180, loss: 2.164374\n",
      "Epoch 181, loss: 2.154008\n",
      "Epoch 182, loss: 2.157429\n",
      "Epoch 183, loss: 2.185128\n",
      "Epoch 184, loss: 2.175951\n",
      "Epoch 185, loss: 2.181954\n",
      "Epoch 186, loss: 2.153687\n",
      "Epoch 187, loss: 2.156289\n",
      "Epoch 188, loss: 2.168955\n",
      "Epoch 189, loss: 2.153913\n",
      "Epoch 190, loss: 2.147939\n",
      "Epoch 191, loss: 2.184046\n",
      "Epoch 192, loss: 2.168987\n",
      "Epoch 193, loss: 2.129619\n",
      "Epoch 194, loss: 2.147122\n",
      "Epoch 195, loss: 2.182658\n",
      "Epoch 196, loss: 2.139893\n",
      "Epoch 197, loss: 2.139496\n",
      "Epoch 198, loss: 2.152574\n",
      "Epoch 199, loss: 2.151533\n",
      "best validation accuracy achieved: 0.216000\n",
      "При этом скорость обучения: 0.000100\n",
      "И регуляризация: 0.000001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for learning_rate in learning_rates: # для каждой скорости обучения\n",
    "    for reg in reg_strengths:\n",
    "        model = linear_classifer.LinearSoftmaxClassifier() # создаём модель\n",
    "        model.fit(train_X, train_y, batch_size, learning_rate, reg, num_epochs) # обучаем\n",
    "        preds = model.predict(test_X) # делаем предсказание\n",
    "        accuracy = multiclass_accuracy(preds, test_y) # вычисляем точность работы\n",
    "        if accuracy > best_val_accuracy: # если лучше, чем раньше\n",
    "            best_val_accuracy = accuracy\n",
    "            best_learning_rate = learning_rate\n",
    "            best_reg_strength = reg\n",
    "            best_classifier = model\n",
    "            \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "print('При этом скорость обучения: %f' % best_learning_rate)\n",
    "print('И регуляризация: %f' % best_reg_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.216000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
