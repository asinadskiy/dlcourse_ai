{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2], tol=0.5) # увеличил tolerance - неправильно, но ошибку не нашёл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2], tol=100) # тоже где-то что-то пошло не так... (tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13333333333333333"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DATA\\pytest\\dlcourse_ai\\assignments\\assignment2\\layers.py:66: RuntimeWarning: divide by zero encountered in log\n",
      "  ces[ix]= -1. * np.sum(px[ix] * np.log(qx[ix])) # кросс-энтропия по формуле -1*СУМ(P*logQ)\n",
      "C:\\DATA\\pytest\\dlcourse_ai\\assignments\\assignment2\\layers.py:66: RuntimeWarning: invalid value encountered in multiply\n",
      "  ces[ix]= -1. * np.sum(px[ix] * np.log(qx[ix])) # кросс-энтропия по формуле -1*СУМ(P*logQ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DATA\\pytest\\dlcourse_ai\\assignments\\assignment2\\layers.py:40: RuntimeWarning: overflow encountered in subtract\n",
      "  probs -= np.max(probs, axis = 1).reshape(-1,1) # вычитаем максимальное и строим в одномерный вектор\n",
      "C:\\DATA\\pytest\\dlcourse_ai\\assignments\\assignment2\\layers.py:40: RuntimeWarning: invalid value encountered in subtract\n",
      "  probs -= np.max(probs, axis = 1).reshape(-1,1) # вычитаем максимальное и строим в одномерный вектор\n",
      "C:\\DATA\\pytest\\dlcourse_ai\\assignments\\assignment2\\model.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  param.grad += grad_d # добавляем регуляризацию (штраф за сложность модели)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-1) # увеличил learning_rate (было 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ed81b2daf0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.290870, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240739, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.248810, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236266, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298369, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299265, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277451, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.244998, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.219804, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275266, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270303, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265702, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.326827, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.354108, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221634, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.328727, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260064, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254927, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254530, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.332353, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.319022, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306890, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291762, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299276, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.325432, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.334649, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296353, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271701, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263773, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299354, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298602, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282402, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265409, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255057, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.321626, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298835, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305279, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265578, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.310853, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.339369, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.320178, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.321494, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.294950, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.312134, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.255764, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.303402, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.330338, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.272899, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.132355, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.202404, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.843584, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.150539, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.744951, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.621199, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.947686, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.890011, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.324098, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.920011, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.326019, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.121381, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.505892, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.037406, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.118042, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.312177, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.884122, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.585148, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.836268, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.494721, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.560209, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.207668, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.979541, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.681159, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.513592, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.340330, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.600659, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.377642, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.197531, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.903918, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.707223, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.675082, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.972724, Train accuracy: 0.600000, val accuracy: 0.133333\n",
      "Loss: 1.532268, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.412173, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.182653, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.767629, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.604955, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.549157, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.402291, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.181065, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.830095, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.304635, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.327738, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.114247, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.603383, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.229912, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.285033, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.861838, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.953359, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.989379, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.016580, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.362726, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.648875, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.950918, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.058936, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.415378, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.170820, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.438255, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.291644, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.293232, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.208359, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.332253, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.540215, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.582711, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.618884, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.698509, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.639640, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.661030, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.904225, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.504282, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.501594, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.325673, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.034348, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 2.245041, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.901892, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.534636, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.244399, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.247137, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.028801, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.355384, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.903630, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.134848, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.326245, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.439878, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.050376, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.341115, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.447797, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.966174, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.385000, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.340908, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.533122, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.457669, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.246567, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.054505, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.188355, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.309884, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.209406, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.337928, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.043724, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.105641, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.484152, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.332609, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.434275, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.391809, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.395326, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.360504, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.499265, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.366211, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.414653, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.365685, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.313577, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.161082, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.286043, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.054141, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.411473, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.475162, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.372440, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.339841, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.166890, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.262288, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.125307, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.332023, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.404055, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.387759, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.142705, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.218683, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.084457, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.312233, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.110533, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.329202, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.465255, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.469051, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.426089, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.413535, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267127, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.229001, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.329935, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.319636, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.504706, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.313825, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.336887, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325326, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.275431, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.265588, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.959480, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.405906, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.229707, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.319165, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.137018, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.762367, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.445312, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.059192, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.433963, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 0.773601, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.608405, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.541643, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.836613, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.667667, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.680145, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.293699, Train accuracy: 0.933333, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-2)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=0.25, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.300039, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298307, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294559, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291461, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289544, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290566, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281593, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282586, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263507, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.286621, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304125, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266564, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266484, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263588, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270955, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259905, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271893, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225118, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224972, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.286799, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240439, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257501, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259268, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.244376, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265046, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291921, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275055, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.322807, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287460, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259426, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.258643, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290213, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.258223, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275551, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253579, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250095, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.195359, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273685, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225039, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246096, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.191211, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264627, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.201014, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.232293, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262313, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296382, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259087, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.172290, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245377, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.331745, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293707, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.252106, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.154098, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245804, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292115, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.183494, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.252506, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214380, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256111, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229707, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.181294, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.222578, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237600, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290301, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299164, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223430, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285874, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237405, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250779, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218572, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242391, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.143849, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.194388, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275046, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.195766, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213780, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275018, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.195763, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272245, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.325152, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.168707, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.203983, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.152269, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.328085, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.235947, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.233399, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.180858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.204362, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.206943, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.201491, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292920, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231772, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264707, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253805, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238972, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228869, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265810, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.179332, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249750, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243038, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216005, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301249, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246221, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287329, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218680, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.164342, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239905, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.209650, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242396, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241399, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261895, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.143704, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274347, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300210, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.194418, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280330, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.099791, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275002, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280524, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226391, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257607, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.249646, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237294, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223570, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259675, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.181419, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.402166, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213208, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283123, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.173136, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240977, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261043, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257163, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230407, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288697, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288879, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256888, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270439, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.145137, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.335607, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249925, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256011, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.204115, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.177575, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249020, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.133440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281158, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237739, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.159236, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.310409, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265300, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278972, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242990, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.385068, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227516, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228153, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.199970, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260922, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313364, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.121961, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237825, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.365437, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263512, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.201812, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196804, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.191152, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.169492, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264386, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264902, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.139314, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.233660, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216594, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.164443, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.123095, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.350281, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.143354, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226870, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228218, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.168239, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.248703, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280692, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196738, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.126049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.174224, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237695, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.191056, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256540, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.159214, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.069856, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.153132, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.175089, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.182319, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "next\n",
      "best validation accuracy achieved: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "# нужно сделать больше параметров - пока просто запустил на одном наборе, чтобы сэкономить время (долго обучается)\n",
    "learning_rates = [1e-4]\n",
    "reg_strengths = [1e-3]\n",
    "learning_rate_decay = [0.999]\n",
    "hidden_layer_sizes = [128]\n",
    "num_epochs = [200]\n",
    "batch_sizes = [64]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for reg_strength in reg_strengths:\n",
    "        for hidden_layer_size in hidden_layer_sizes:\n",
    "            for num_epoch in num_epochs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    \n",
    "                    model = TwoLayerNet(n_input = train_X.shape[1], \n",
    "                                    n_output = 10, \n",
    "                                    hidden_layer_size = hidden_layer_size, \n",
    "                                    reg = reg_strength)\n",
    "                    dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "                    trainer = Trainer(model, dataset, MomentumSGD(), \n",
    "                                      learning_rate=learning_rate, \n",
    "                                      num_epochs=num_epoch, batch_size=batch_size)\n",
    "                    loss_history, train_history, val_history = trainer.fit()\n",
    "                    #if best_classifier == None:\n",
    "                    #    best_classifier = model\n",
    "                    #    best_val_accuracy = val_history[-1]\n",
    "\n",
    "                    if val_history[-1] > best_val_accuracy:\n",
    "                        best_classifier = model\n",
    "                        best_val_accuracy = val_history[-1]\n",
    "                        best_learning_rate = learning_rate\n",
    "                        best_reg_strength = reg_strength\n",
    "                        best_hidden_layer_size = hidden_layer_size\n",
    "                        best_num_epoch = num_epoch\n",
    "                    print('next')\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "print('best_val_accuracy', best_val_accuracy)\n",
    "print('best_learning_rate', best_learning_rate)\n",
    "print('best_reg_strength', best_reg_strength)\n",
    "print('best_hidden_layer_size', best_hidden_layer_size)\n",
    "print('best_num_epoch', best_num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.206000\n",
      "best_val_accuracy 0.206\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-28e4bcdb8e9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_learning_rate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_learning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_reg_strength'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_reg_strength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_hidden_layer_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_hidden_layer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "print('best_val_accuracy', best_val_accuracy)\n",
    "print('best_learning_rate', best_learning_rate)\n",
    "print('best_reg_strength', best_reg_strength)\n",
    "print('best_hidden_layer_size', best_hidden_layer_size)\n",
    "print('best_num_epoch', best_num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ed81933490>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAGrCAYAAABjUG5rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxcdZnv8c9T1VXVXb3v6XTSWUgIYV8CYRVwBxd0RAdGcAFEXGHEuTrOHcc7c+eO48w4OiqDGVBRQURZFQFR2fckhCUkkI1snU53et+7uuq5f9Tp2ITupJNeqpfv+/WqV1ed3++ceur0yel68tvM3REREREREZGpLZTpAERERERERGT0lNyJiIiIiIhMA0ruREREREREpgEldyIiIiIiItOAkjsREREREZFpQMmdiIiIiIjINKDkTkREREREZBpQciciIjOWmb1uZm/PdBwiIiJjQcmdiIiIiIjINKDkTkREZBAzi5nZd8ysNnh8x8xiQVmZmf3WzFrMrMnMHjOzUFD2FTPbaWbtZvaqmb0ts59ERERmmqxMByAiIjLJ/B1wKnA84MDdwP8G/h64FtgBlAd1TwXczJYAnwdOdvdaM5sPhCc2bBERmenUciciIvJGHwX+0d3r3b0B+D/ApUFZAqgC5rl7wt0fc3cHkkAMONLMIu7+urtvykj0IiIyYym5ExEReaPZwNZBr7cG2wD+DdgI/N7MNpvZVwHcfSNwDfANoN7MbjWz2YiIiEwgJXciIiJvVAvMG/S6JtiGu7e7+7XuvhB4H/ClgbF17n6Lu58Z7OvAv05s2CIiMtMpuRMRkZkuYmbZAw/gF8D/NrNyMysDvg78HMDM3mtmi8zMgDbS3TGTZrbEzN4aTLzSA3QHZSIiIhNGyZ2IiMx0vyOdjA08soGVwIvAS8Bq4P8GdRcDfwA6gKeA69z9YdLj7b4J7AHqgArgaxP2CURERABLjwMXERERERGRqUwtdyIiIiIiItOAkjsREREREZFpQMmdiIiIiIjINKDkTkREREREZBrIynQAB6OsrMznz5+f6TBEREREREQyYtWqVXvcvXyosimV3M2fP5+VK1dmOgwREREREZGMMLOtw5WpW6aIiIiIiMg0oORORERERERkGlByJyIiIiIiMg0ouRMREREREZkGlNyJiIiIiIhMA0ruRunHT2zhpidfz3QYIiIiIiIywx1ycmdmc83sITNbZ2ZrzezqIepcYGYvmtkaM1tpZmcOKnu3mb1qZhvN7KuHGkcmpVLOU5sa+Yd71vKrldszHY6IiIiIiMxgo2m56weudfelwKnA58zsyH3q/BE4zt2PBy4DbgAwszDwA+A84Ejg4iH2nfRCIeN7f3UCZy0u4yu3v8i9L+7KdEgiIiIiIjJDHXJy5+673H118LwdWAdU71Onw909eJkLDDw/Bdjo7pvdvQ+4FbjgUGPJpFhWmB9eehIn1hRzzS+f56H19ZkOSUREREREZqAxGXNnZvOBE4Bnhij7oJmtB+4l3XoH6SRwcD/GHeyTGE4l8WgWP/rkySyZlc9VP1/FU5saMx2SiIiIiIjMMKNO7swsD7gduMbd2/Ytd/c73f0I4APAPw3sNsShfIhtmNmVwXi9lQ0NDaMNd9wUZEe46ZOnMLckzhU3Pcea7S2ZDklERERERGaQUSV3ZhYhndjd7O537K+uuz8KHGZmZaRb6uYOKp4D1A6z3wp3X+buy8rLy0cT7rgrzYtx8xXLKc2L8fEfPcu6XW/KdUVERERERMbFaGbLNOBGYJ27f3uYOouCepjZiUAUaASeAxab2QIziwIXAfccaiyTSWVBNjdfsZycSJhLb3yGzQ0dmQ5JRERERERmgNG03J0BXAq8NVjqYI2ZnW9mV5nZVUGdDwEvm9ka0rNj/qWn9QOfBx4gPRHLbe6+dhSxTCpzS+L8/IrlpBwuueEZtjd1ZTokERERERGZ5uzPk1lOfsuWLfOVK1dmOowRW1vbysUrniY/O8KtV57K3JJ4pkMSEREREZEpzMxWufuyocrGZLZMGdpRswu5+YpTae9JcNGKp9WCJyIiIiIi40bJ3Tg7Zk46wevo7VeCJyIiIiIi40bJ3QRIJ3jL9yZ42xqV4ImIiIiIyNhScjdBjq5OJ3idff1ctOIpJXgiIiIiIjKmlNxNoIEEryuR5KIVT7G1sTPTIYmIiIiIyDSh5G6CpSdZGUjwnlaCJyIiIiIiY0LJXQYcNbuQW644lZ5Ekr/84dNs0kLnIiIiIiIySkruMuTI2QXc8qlT6U+l+Mj1T7G2tjXTIYmIiIiIyBSm5C6DllYVcNunTyOWFeKiFU+zamtTpkMSEREREZEpSsldhi0sz+NXnzmdsrwYl9zwLI9v2JPpkEREREREZApScjcJVBflcNunT2NeaZzLfvIcD6yty3RIIiIiIiIyxSi5myTK82P88srTOKq6gM/evJo7Vu/IdEgiIiIiIjKFKLmbRArjEX5++XKWLyjhS7e9wM+eej3TIYmIiIiIyBSh5G6SyY1l8aNPnMzbl1by93ev5ft/2oC7ZzosERERERGZ5JTcTULZkTD/fcmJfPCEav7996/x9bvXkkwpwRMRERERkeFlZToAGVokHOI/PnwcFfkxfvjoZhrae/nORceTHQlnOjQREREREZmE1HI3iYVCxt+ev5Svv/dIHniljktvfIbWrkSmwxIRERERkUlIyd0UcNmZC/jexSfwwvZWLrz+SXa2dGc6JBERERERmWSU3E0R7z12Njdddgp1rT186LonWV/XlumQRERERERkElFyN4Wcdlgpv/rMaTjOh69/iqc2NWY6JBERERERmSSU3E0xR8wq4I7PnkFlQTYf/9Gz3Pm8FjsXEREREREld1NSdVEOv77qNE6cV8Rf//IF/t/v1mmpBBERERGRGU7J3RRVFI/ys8uX87HT5rHi0c1c9pPnaO3WTJoiIiIiIjOVkrspLBIO8Y8XHM2//MUxPLlpDx/8wRNsrO/IdFgiIiIiIpIBSu6mgYtPqeGWT51Ka3eCD/7gCR5aX5/pkEREREREZIIpuZsmTp5fwj1fOJOa0jiX3fQc//3wJtw1Dk9EREREZKZQcjeNpCdaOZ33HFPFv96/nmt+uYaeRDLTYYmIiIiIyAQ45OTOzOaa2UNmts7M1prZ1UPU+aiZvRg8njSz4waV/XWw38tm9gszyz7UWOTPcqJhvnfxCfzNu5Zwzwu1fPj6p9jV2p3psEREREREZJyNpuWuH7jW3ZcCpwKfM7Mj96mzBTjb3Y8F/glYAWBm1cAXgWXufjQQBi4aRSwyiJnxuXMX8T+XLmPLnk7e970nWLW1KdNhiYiIiIjIODrk5M7dd7n76uB5O7AOqN6nzpPu3hy8fBqYM6g4C8gxsywgDtQeaiwytLcfWcmdnz2d3FiYi1Y8za3Pbst0SCIiIiIiMk7GZMydmc0HTgCe2U+1y4H7ANx9J/DvwDZgF9Dq7r8f5thXmtlKM1vZ0NAwFuHOKIsr87n7c2dw6sJSvnrHS3z5Vy/Q3adxeCIiIiIi082okzszywNuB65x97Zh6pxLOrn7SvC6GLgAWADMBnLN7JKh9nX3Fe6+zN2XlZeXjzbcGakoHuUnnzyFL751Ebev3sEHfvAEmxq0Hp6IiIiIyHQyquTOzCKkE7ub3f2OYeocC9wAXODujcHmtwNb3L3B3RPAHcDpo4lF9i8cMr70ziX85JOn0NDRy/u/9zi/eUE9YUVEREREpovRzJZpwI3AOnf/9jB1akgnbpe6+2uDirYBp5pZPDjO20iP2ZNxdvbh5dz7xTM5oqqAL/zief7+rpfp7Vc3TRERERGRqW40LXdnAJcCbzWzNcHjfDO7ysyuCup8HSgFrgvKVwK4+zPAr4HVwEtBHCtGEYschKrCHG698lQ+ddYCfvb0Vi7876fY3tSV6bBERERERGQUzN0zHcOILVu2zFeuXJnpMKaVB9bW8eVfvYAB37rwON599KxMhyQiIiIiIsMws1XuvmyosjGZLVOmrncdNYt7v3AW80pzuernq/jbO16kq68/02GJiIiIiMhBUnIn1JTGuf0zp/Ppsxdy63Pbee/3Huflna2ZDktERERERA6CkjsBIJoV4m/PW8rNly+ns7efD173BCse3UQqNXW67YqIiIiIzGRK7uQNTl9Uxv1Xv4W3HlHB//vdej72o2fZ3daT6bBEREREROQAlNzJmxTnRrn+kpP4l784hlVbm3n3dx7l92vrMh2WiIiIiIjsh5I7GZKZcfEpNfzmC2cyuyiHK3+2imtve4HWrkSmQxMRERERkSEouZP9WlSRx52fPYMvvHURd63ZyTu/8wh/Wr8702GJiIiIiMg+lNzJAUWzQlz7ziXc9dkzKMqJctlPVnLtbS/Q3qNWPBERERGRyULJnYzYMXMKuecLZ/D5cxdx5/M7eM9/Pc7z25ozHZaIiIiIiKDkTg5SLCvMl9+1hF9++jSSKefC65/ie3/cQFJLJoiIiIiIZJSSOzkkJ88v4XdXn8V7jqniPx58jY/88Ck21rdnOiwRERERkRlLyZ0cssKcCN+96Hi+85fHs6mhg/O/+zj/9ccN9PWnMh2aiIiIiMiMo+RORsXM+MAJ1Tz412fzrqNn8e0HX+O933uMZ7c0ZTo0EREREZEZRcmdjIny/Bjfu/gEbvz4Mtp7+vnID5/iiptWqqumiIiIiMgEUXInY+ptSyv507Xn8DfvWsLTmxt5538+yldvf5E9Hb2ZDk1EREREZFpTcidjLica5nPnLuKRvzmHj502n9tX7+Ad336Eu9fsxF2zaoqIiIiIjAcldzJuSvNifOP9R3HvF8+ipjSXq29dw6d+uordbT2ZDk1EREREZNpRcifj7vDKfO74zOl87fwjeGxDA2//9iPc+PgWzaopIiIiIjKGlNzJhAiHjCvfchj3XX0Wx88t4p9++wrv/M9HeGBtnbpqioiIiIiMASV3MqEWlufx08tO4cefOJmscIhP/2wVF614mpd3tmY6NBERERGRKU3JnUw4M+PcIyq4/+qz+KcPHM2G+g7e9/3Hufa2F6hr1Xg8EREREZFDYVOpS9yyZct85cqVmQ5DxlhbT4IfPLSRHz/+OuGQ8amzFnD5WQspzIlkOjQRERERkUnFzFa5+7Ihy5TcyWSxvamLb96/nntf3EV+dhaXn7mAy85cQEG2kjwREREREVByJ1PM2tpWvvuHDfz+ld0UZGdx1TmHcfmZC4hlhTMdmoiIiIhIRu0vudOYO5l0jppdyIqPLeO3XziTZfNL+Nb9r/Ku/3yUP63fnenQREREREQmLSV3MmkdXV3Ijz5xMjdddgqhkHHZT1Zy2U+eY8Pu9kyHJiIiIiIy6Si5k0nv7MPLuf/qt/B35y/l2S1NvPM7j/KZn6/S8gkiIiIiIoMccnJnZnPN7CEzW2dma83s6iHqfNTMXgweT5rZcYPKiszs12a2PjjGaYcai0x/0awQn3rLQh79X+fy+XMX8fiGPbz3e49z2U+eY/W25kyHJyIiIiKScYc8oYqZVQFV7r7azPKBVcAH3P2VQXVOB9a5e7OZnQd8w92XB2U3AY+5+w1mFgXi7t6yv/fUhCoyoLU7wc+eep0bH99Cc1eCMxaV8vlzF3PqwhLMLNPhiYiIiIiMiwmZLdPM7ga+7+4PDlNeDLzs7tVmVgC8ACz0gwhAyZ3sq7O3n1ue2caKxzbT0N7LsnnFfP6tizj78HIleSIiIiIy7Yx7cmdm84FHgaPdvW2YOl8GjnD3K8zseGAF8ApwHOlWv6vdvXOI/a4ErgSoqak5aevWraOOV6afnkSS21Zu5/qHN1Hb2sMx1YV8/q2LeMfSSkIhJXkiIiIiMj2Ma3JnZnnAI8A/u/sdw9Q5F7gOONPdG81sGfA0cIa7P2Nm3wXa3P3v9/dearmTA+nrT3HX8zv5wcMb2drYxeGVeXz6LYfx/uNnEwlr/iARERERmdrGbZ07M4sAtwM37yexOxa4AbjA3RuDzTuAHe7+TPD618CJo4lFBNITr3zk5Ln88Utn852/PB7DuPZXL3D2tx7ixse30Nnbn+kQRURERETGxWgmVDHgJqDJ3a8Zpk4N8CfgY+7+5D5ljwFXuPurZvYNINfd/2Z/76mWOzlY7s5Dr9Zz/cObefb1JvJjWbz/+NlcfEoNR1cXZjo8EREREZGDMi7dMs3sTOAx4CUgFWz+GlAD4O7Xm9kNwIeAgYFy/QOBBOPubgCiwGbgk+6+3zntldzJaKza2szNT2/l3pd20duf4ujqAj526nw+cEI10Sx12RQRERGRyW9CZsucCEruZCy0diW4a81OfvHsNtbXtTOrIJsrzlrAxafUkBvLynR4IiIiIiLDUnInMgR359ENe/jvhzfy9OYmCnMifPz0+Xzi9PmU5EYzHZ6IiIiIyJsouRM5gNXbmrn+4U38/pXd5ETCXHTKXK44ayHVRTmZDk1EREREZC8ldyIjtLG+nesf2cxdz+8E4F1HzeKSU+dx6sISLYouIiIiIhmn5E7kIO1s6eYnT2zhtpU7aO1OsLgij48ur+H9x1ery6aIiIiIZIySO5FD1JNI8psXavn501t5YUcrWSHjnCXlXHB8Ne84spLsSDjTIYqIiIjIDKLkTmQMvFLbxt1rdnL3mlrq2nooyM7iI8vm8rHT5lNTGs90eCIiIiIyAyi5ExlDyZTzzOZGfvHcdu57aRdJd966pIKPnlrDWYvLiYS1Zp6IiIiIjA8ldyLjZHdbDzc/vZVbnt3Gno4+SnKjvOeYKj5wwmxOrCnWJCwiIiIiMqaU3ImMs77+FI+81sBda3byh1d209ufYkFZLh9eNocLT5xDRUF2pkMUERERkWlAyZ3IBGrvSXD/y3X8atUOnt3SRDhknLukggtPquacJRWahEVEREREDpmSO5EM2dTQwW0rt3P7qp3s6eglPzuL84+u4gMnVLN8QQmhkLptioiIiMjIKbkTybD+ZIonNzVy1/M7eWBtHZ19SRaU5fLR5TVceNIciuJaO09EREREDkzJncgk0t2X5L6Xd3HzM9tYtbWZWFaIdx89i9MPK2XZ/BIWluVqIhYRERERGZKSO5FJat2uNm5+Ziv3vriL5q4EAKW5UU5fVMaFJ83hzEVlhNV1U0REREQCSu5EJjl3Z1NDJytfb+LZ15v40/p6WroSVBVmc+FJc3j/cbNZVJGnFj0RERGRGU7JncgU09uf5I/r6rlt5XYefa2BlEN1UQ7nLCnnnCUVnLW4TLNuioiIiMxASu5EprC61h7+tL6eh1+t54mNe+jsS5Ify+I9x1bxoZPmsGyeFksXERERmSmU3IlME339KZ7d0sSdz+/kvpd30dWXpKYkzmkLSzlmTiHHVBdyRFU+sSy16omIiIhMR0ruRKahzt5+7n+5jt+8WMsL21v2TsgSywrxjiMr+YsTqzlrcTmRcCjDkYqIiIjIWFFyJzLNuTs7mrt5aWcrT21q5Lcv1tLclaA0N8p5x8zilAWlnDSvmOqinEyHKiIiIiKjoOROZIbp60/x6GsN3Pn8Th56tZ6uviQAVYXZnDivmGXzijlpXjFLqwrUsiciIiIyhewvucua6GBEZPxFs0K8/chK3n5kJf3JFOvr2lm1tXnv494XdwGQEwlz3NxCTppXzLJ5JZxQU0RRPJrh6EVERETkUKjlTmQG2tXa/YZkb21tG8lU+l5w7JxC3nZEJW9bWsFRsws0E6eIiIjIJKJumSKyX119/by4o5XntjTx0Kv1PL+9BXeYVZDN+cdUccHxszl2TqESPREREZEMU3InIgdlT0cvD7/awANr63jk1Qb6kikWlOVy3tGzWFyZR01JnJqSXMryokr4RERERCaQkjsROWStXQnuX7uLe16o5alNjaQG3TLi0TA1JXHmlsSZVxJnaVUBZy4uo7IgO3MBi4iIiExj45Lcmdlc4KfALCAFrHD37+5T56PAV4KXHcBn3P2FQeVhYCWw093fe6D3VHInklk9iSQ7mrvZ3tTF1sZOtjV1s62pk21NXWxr6qInkQJgcUUeZy4u48xFZSxfWEpeTHM3iYiIiIyF8Zotsx+41t1Xm1k+sMrMHnT3VwbV2QKc7e7NZnYesAJYPqj8amAdUDCKOERkgmRHwiyqyGNRRd6bylIpZ31dO49vbOCxDXu45Zlt/PiJ18kKGSfWFHPm4jLecWQlR8zKV1dOERERkXEwZt0yzexu4Pvu/uAw5cXAy+5eHbyeA9wE/DPwJbXciUwvPYkkq7Y289iGPTy+sYG1tW24w8KyXM4/pop3HlXJvNJcCrKzlOyJiIiIjNC4r3NnZvOBE4Bn9lPtcuC+Qa+/A/wvIH8sYhCRySU7EuaMRWWcsagMOIKG9l4eWFvH717axXUPb+T7D20E0mvylefFqCiIMb80l3mlceaVxjlqdiGLK/KU+ImIiIiM0KiTOzPLA24HrnH3tmHqnEs6uTszeP1eoN7dV5nZOQc4/pXAlQA1NTWjDVdEMqQ8P8Ylp87jklPn0djRyxObGtnd2sOejl4a2nvZ1drDs1uauGvNTgY6FCwsy+XdR8/i/GOqtOaeiIiIyAGMqlummUWA3wIPuPu3h6lzLHAncJ67vxZs+xfgUtLj9rJJj7m7w90v2d/7qVumyPSXnrSli2e2NHHfS3U8tbmRZMrJChkFORHys7MoyI6wuCKPE2qKOKGmmCNm5ZMVDmU6dBEREZFxN16zZRrpMXNN7n7NMHVqgD8BH3P3J4epcw7wZY25E5GhNHX28YdXdrOlsZP2ngRt3f20dCd4pbaNPR29QHpJhmPnFHJiTTEn1hRz3NwircEnIiIi09J4jbk7g3Tr20tmtibY9jWgBsDdrwe+DpQC1wVfsvqHC0REZCgluVE+cvLcN213d3Y0d7N6WzPPb2th9bZmVjy6mf5gIb54NMzsohyqi3KCMXwFHF1dyOGV+UTUyiciIiLTkBYxF5Fpo7svyUs7W3l5Zys7mrupbelmZ0s3W/Z00tHbD6QncDm8Mo9F5XksrsxnUUUeR1YVMKc4Ry19IiIiMumN+2yZIiKTQU40zCkLSjhlQckbtqdSztamrr2J3/q69mDyltq9dcryohw/t4jj5xZRU5pLeV6M8vz0LJ4F2ZGJ/igiIiIiB03JnYhMe6GQsaAslwVlubz/uNl7t3f09rOxvoOXdrayZlsLa7Y384d19W/avyQ3ysKyXBaW57KwPI85xTnMKY4zpziH0lyN7RMREZHJQd0yRUQGae9JsLuth/q2Xho6eqlr7eH1xk421XeyeU8Hezr63lA/EjaK4lGKciIUx6OU58eoLk6P9asuyuGIqnyqi9TlU0RERMaGumWKiIxQfnaE/OwIiyryhyxv60mws7mbHc3d7Gzuoq6tl5auPlq6EjR39fHKrjYeXLebvv7U3n0q8mOcWFPMCTVFVBTEiEezyI1mUZCTxcLyPPJiuhWLiIjI6OkbhYjIQSjIjlBQFWFpVcGwdVIpZ09nL9ubullb28rqrc2s3tbC/WvrhqxfUxJnaVU+h1fmM6swm4r8bCoLYswuUrdPERERGTl1yxQRmSADLXwdvf109SVp7urjtbp21tW1sX5XO1saO9n3lpyfnW7dO6wsl+riHEpyo5TmxSjLjbK4Mp/y/FhmPoyIiIhkhLpliohMAkXxKEXx6Bu2veuoWXuf9ydT7OnoS4/5a+9le1MXm/d0sLmhkyc3NbK7vedNyd+80jgnzUsv3l6WFyU7EiYezSKWFSI0qMUvHgszvzSXcEitgCIiItOVkjsRkUkiKxxiVmE2swqzhyxPppyWrj6aOvtoaO/l5dpWVr7ezCOvNnDH6p0HPH5OJMzSqnyOml3IwvJciuNRiuIRiuJRciJhwiEjHDKyQsaswmwt9i4iIjLFqFumiMgU5+7sbOmmvSfd3bMnkX4Mvr23dCdYW9vK2to2Xqlt27uo+3Ci4RCLK/NYWlXAksp8sqNhAAwYaBA0DDMojkc4dWHpm1olRUREZOypW6aIyDRmZswpjh+w3oUnzQHSE760dKdn92zpStDS1UdPIkXSnWQqRV9/is17Olm3q52HX63n16t2jCAGOLa6kDMXl3HErAKyI2GyIyFiWWEqguUhBrcEplJObWs3u9t6mV8apzRPYwdFRERGS8mdiMgMEwoZJblRSnJH1tLW0tVHIuk4DkFroMPelsGdLV08tmEPj2/Yw/WPbCaZenOPkHDI9q7919jZy9bGLnr3WS5iaVUBiyvyKM4NuovmRCnLizK3JE5lQbbGC4qIiByAumWKiMiYaetJsLu1h55Eip7+JN19SeraetjW2MXrjZ3sbOmmNDfGwvJcFpTlUpEfY8ueTl7Z1ca6Xe1s2dNBTyL1puNGwunksCwvRk40TG40i3gsTDx4PrCtICeLwpwIBTkRcqNZdPUl6ejtp70ngTvMKc5RsigiIlOaumWKiMiEKMiOUJAdGdUxehJJWrsTtHQlqG/vYXtTN9ubu9je1EVzVx/tPf3Ut/XS2ZceY9jZ2/+GVsCRiISN4nh07yQy4ZCREwlTkBOhMHgUZA88z3rj9pwIebEsQpYec2hATjRM/ig/t4iIyGgpuRMRkUklPV4vTGVBNktm5Y9on2TK6ezrp607QWvw6OpNEo+FyY9FyMvOwt3Z0ZxOFLc1ddHalSCZ8vTDna6+dFK5vamLtcExOvuSI467PD/GwrJcFpbnUZgToTtIPrsTSaLh0N4Ece/P7HQrY24si87eflq7E7T19JNIpqgsiDGrIIeqwmwKcyKkPB1jKkV6mQu1OoqIyBCU3ImIyJQXDtneVsM5xcPXW1ied1DHTSRTtPf0700YB5LHzt5+Uk56HCLQ1t3P5oYONu/p5P6Xd9HZmyQnmu42mhMJ09ufoq07QfsBZikdiUg4vVTF7ML0GMaqomxmF+UwuyiHyvxs+lPpmNt7+untT1KQHaEoHqE4HiUvO/1n34PYQ2ZEwiFiWSEi4ZC6qoqITHFK7kRERIYRCYcOavKZA0mmnPaeBG3dAy11CTp6+8mLZe3tBhoOG/VtPdS19rCrtYe2ngRhM0IhI2RGW0+C2ryGja8AACAASURBVJZualu6eWZLE3VtPUNOYnMowiEjEh5I+MLMLclhaVUBS6sKOKwsl6auvnTrZ1MX9e29uLO3a2peLIvDKvJYXJHH4ZX55Gdn0RJ0r23t7qO6KM7hlXmYvTmB7E+mCAWfUUREDp0mVBEREZnCkimnvr2H2pb00hKxrBB5sSzysrOIZYVp60kvd9HcmU4kB5IxzHB3+vpTJJJOIpkKnqfoS6boSSTZ3NDJul1ttPW8scWxOB6hsiAbC44B0NKVoK6tZ7+xluVFOe2wMk6eX0xTZx/rdrWxvq6drY1dAHsTy3g0TEV+drp7amF2enKcRJKu3n46+5JEwkZRPEpJPD2zakF2hHgsTG4si9xoFrmxMHmxLOLRLPJiWWRHQkMmlSM1sHxIZ28/1UU5SkJFJKM0oYqIiMg0FQ4ZVYU5VBXmjMvx3Z3a1h62NHRSmhdlTnHOsJPHtPck2FDfwYbd7XT3JSnOjVKYEyE/O8Kmhg6e3LiHJzc18psXajGDBaW5HD27kAuOm42Z0RckmF19/exu66WutYeXdrbS1ZckHiRtOZEwiWQqvUZjd2JErZYhI0j6sohFQiRTjns6MY7HwtSUxKkpiTO3OE5/yve2jO5s6WZPRx/NXX1736c4HuH0RWWctaiME+cV059Mj/fs6O0nEgpxeGUe5fmxUSWTIiKHSi13IiIiMmEGJrYpzYsSj47u/5hTKac9WOpiYNmLrt70z87efrr6+unoTQY/+/fOrDrQzTUcdHPd3tzF1sYu2oMWysKcCLOLcqguyqY8P0ZpboyS3CjZkTArtzbx+IY91Lf3DhtXcTzC4ZX5lORG6ez7c4ujAbFIiGg4RDQr3fU1lpUe8xiLpCfdKY5HKY5HiEezSCRT9Pan6E0kSTl7x3FmR8IUZEf2dhkujkfICodGdS5FZOpQy52IiIhMCmbG3JL4mBwrFLK9S1SMhdauBOGwkRcb/uvRXy2vwd3ZUN/BK7VtZEdC5AZdQHsTSV7b3c6ru9tZX9fOxvqOoCxMUTwdY29/unWyvaefxv4++pIpevuT9CRStHYl6Ese3LIekB73ODApTjQcIjsSZmF5LsdUF3LsnEJqSnLZ1NDB2to21ta20tjRx7zSeLDeZB5VhdnkRMNkZ4XJiYYpiUcpyMlS66PIFKSWOxEREZFJwIMlOZq7+ujqSwateunWPTPoTiTTy2v0JWnrTtDY2UdTZx+NnX30JJJ7x0x29SVZX9fOht3t9A/qthoJG4sr8qkoiLG1Mb0kyHDdWuPRMFWF6ZlYy/JiFMejlOalu9n2J1N0J9LjMgGWzMrn6NmFzC3JwcxIpZyGjt69x68qzKayIJvsSHhCzqPIdKeWOxEREZFJzszSk8IM03JYdJDH60mkk7xtTV0cVp7L4op8oll/7r6ZSKbY1tRFfVsvPf1JehPpdRkbO/qobelhV2s3ta09vN7YSVNH35DrPpqll9YAyM/Oojw/xs7mbnr739wCWRyPUFMSZ0FZusVwXmmc1u4E25q62N7URUNHL6W5UWYXpceQVuTH0uMso1nkRMIU5GQxqyC99qNaFUWGppY7ERERETmgnkS6xTASDpETTbco9iVTvFbXwcu1rby8s5Wmzj7mlsSZG0xSEzajrq2HuiBR3NbYxZY9nexs6d573OxIiLnFcSoKYkFi2f2mGVoHi2WFmFWYTXE8SjwaJh7NIicapqMnQX17Lw3tvbR0JTiiKp/TDyvjzEVlLJtfrJZDmTb213Kn5E5EREREJlR3X5IdzV0UxiOU5715dtHO3n4a2nvT3VAT/XT1JWnpSrC7rYfdbT3UtfXS0tVHd1+6q2pXXz952VmU58WoyM8mLzuLF3e08Py2FvpTjhl7x2cWBhPXVBbEqCxIdxktyIlgpFsiAQzb+zxkxjFzCqkuGp8ZaUUOlrplioiIiMikkRMNs7gyf9jy/XVPPRidvf08u6WJ57e30NLVR2t3gtbuBE2dfayva6OhvZcRrKYBwNHVBbzzyFmcu6QCM4LlOPpo7krQ2pX+2dKVIByCY6oLOW5uEUfMKnhDV1iR8aaWOxERERGZkZIpZ09Hb7AMRnr9w4FvxunnTl9/iqc2NfL7V3azelszw311jkfDFOVE6O1P0djZB0A0HOLwWXksKs9jcWU+h5XnMac4h4qC9BIb4ZDGDsrBU7dMEREREZFRqm/v4enNTcSyQhTlRCjOjVKUE6EwHiGWlR7T5+7sbOnmxR2trNnekl4WY3c7ta09bzhWyKA0L0ZFfozy/PTPivz0DKWzi7KZU5yeqTSZcvpT6SQzmhWiNDeqdQ1nuHFJ7sxsLvBTYBaQAla4+3f3qfNR4CvByw7gM+7+wkj2HYqSOxERERGZijp6+9lU38Gu1h4a2ntoaO/dOwHMwM+Gjt5hl6cYYAaluelkcHFlHqcfVsrph5WN2fqRMvmN15i7fuBad19tZvnAKjN70N1fGVRnC3C2uzeb2XnACmD5CPcVEREREZkW8mJZHDe3iOPmDl8nmXJ2t/Wws6Wbnc3d7OnoJRIOkRU2IuEQff2pIBHsoa61hyc2NnL3mloAqotyKMuPkUo5yZTjQEV+jLklOdQEs5cuqshnfmlcLX/T2CEnd+6+C9gVPG83s3VANfDKoDpPDtrlaWDOSPcVEREREZlJwiELumXmcPL8A9d3dzbWd/DU5kae2dxEe28/WSEjZAY4dW09rNneQmt3Yu8+0XCIheW5LJmVz+GV6ceSynxikRAb6zvYsLudzXs6mV2UwwdPqKayIHvcPq+MvTEZc2dm84FHgaPdvW2YOl8GjnD3Kw5mXzO7ErgSoKam5qStW7eOOl4RERERkZmitTvB1sZONuzu4LXd7cGj4w3rDQ6WGw3T2ZckZHD24eVceNJcFlXkEc0KEQkbsawwJblRTQiTIeM6oYqZ5QGPAP/s7ncMU+dc4DrgTHdvPJh9B9OYOxERERGRsdHek2BDfQev1bXTl0yxqDyPRRV5lOfH2LKnk1+v2sEdq3dS19bzpn0jYaO6KIe5JXHmlcb3Lv+wqDxP3T7H2bgld2YWAX4LPODu3x6mzrHAncB57v7awey7LyV3IiIiIiITJ5lynnu9icaOPhLJFH3JFN19SWpbu9nR1M2O5i427+kMlpOAnEiYheW55EazyI6GyYmEKMyJMKswh1kF2VQVZnNEVT5VhVoU/lCNy4QqZmbAjcC6/SR2NcAdwKX7JHYH3FdERERERDIrHDJOXVi63zqplPN6Y+fe5R+2NnbSnUjS1p1gd2uSlu4+6tt737BGYE1JnOULSli+sJTlC0o02+cYGc1SCGcCjwEvkV7OAOBrQA2Au19vZjcAHwIGBsr1u/uy4fZ199/t7z3VciciIiIiMvX0J1M0dPRS29LNmu2tPLO5kWdfb6KlKz3ZS3VRDssXlnDy/BKqi3Ioz49RlhfT2L4haBFzERERERGZVFIp57X6dp7Z3MQzW9IzfjZ29r2hTsigJDe90Ht5foy5xTmcUFPMSfOKmV8aJ90hcGZRciciIiIiIpOau7O9qZvdwSLvezp63/CzoaOPzQ0de8f3leRGWVyRR0VBNhVB8nfcnCJOnl88rSd1Ga9FzEVERERERMaEmVFTGqemdPjxd6mUs7Ghg1Vbm1m1tZmtjZ28tKOF+vZeuvqSABTmRDhnSTlvW1rJ2YeXU5gTmaiPkHFquRMRERERkSmvtTvBU5v28Id19Ty0vp7Gzj6yQsbJ80t429IKzllSzpziONmRcKZDHRV1yxQRERERkRkjmXLWbG/hD+t288d1u3ltd8fesrxYFmV5UWYVZnPErAKOrCpgaVUBiyvzpkTip+RORERERERmrO1NXTy9uZH6QWP4djR382pdO92JdHfOcMg4rDyXpVXphG/Z/BJOmlec4cjfTGPuRERERERkxppbEh9yLb1kytna2Mm6Xe2s29XGul1tPLelibvX1HLB8bMnZXK3P0ruRERERERkRgqHjIXleSwsz+M9x1bt3d7S1bd3gpapRMmdiIiIiIjIIEXxKEXDT9o5aU3fBSBERERERERmECV3IiIiIiIi04CSOxERERERkWlAyZ2IiIiIiMg0oORORERERERkGphSi5ibWQOwNdNxDKEM2JPpIGYwnf/M0bnPLJ3/zNG5zyyd/8zS+c8cnfvMmiznf567lw9VMKWSu8nKzFYOt0q8jD+d/8zRuc8snf/M0bnPLJ3/zNL5zxyd+8yaCudf3TJFRERERESmASV3IiIiIiIi04CSu7GxItMBzHA6/5mjc59ZOv+Zo3OfWTr/maXznzk695k16c+/xtyJiIiIiIhMA2q5ExERERERmQaU3ImIiIiIiEwDSu5GwczebWavmtlGM/tqpuOZ7sxsrpk9ZGbrzGytmV0dbP+Gme00szXB4/xMxzpdmdnrZvZScJ5XBttKzOxBM9sQ/CzOdJzTjZktGXR9rzGzNjO7Rtf++DGzH5lZvZm9PGjbsNe6mf1t8LfgVTN7V2ainj6GOf//ZmbrzexFM7vTzIqC7fPNrHvQv4PrMxf51DfMuR/2XqNrf2wNc/5/Oejcv25ma4LtuvbH0H6+Z06pe7/G3B0iMwsDrwHvAHYAzwEXu/srGQ1sGjOzKqDK3VebWT6wCvgA8BGgw93/PaMBzgBm9jqwzN33DNr2LaDJ3b8Z/CdHsbt/JVMxTnfBvWcnsBz4JLr2x4WZvQXoAH7q7kcH24a81s3sSOAXwCnAbOAPwOHunsxQ+FPeMOf/ncCf3L3fzP4VIDj/84HfDtST0Rnm3H+DIe41uvbH3lDnf5/y/wBa3f0fde2Prf18z/wEU+jer5a7Q3cKsNHdN7t7H3ArcEGGY5rW3H2Xu68OnrcD64DqzEYlpK/7m4LnN5G+Ecr4eRuwyd23ZjqQ6czdHwWa9tk83LV+AXCru/e6+xZgI+m/EXKIhjr/7v57d+8PXj4NzJnwwGaAYa794ejaH2P7O/9mZqT/Q/sXExrUDLGf75lT6t6v5O7QVQPbB73egRKNCRP8b9UJwDPBps8HXXV+pG6B48qB35vZKjO7MthW6e67IH1jBCoyFt3McBFv/MOua3/iDHet6+/BxLsMuG/Q6wVm9ryZPWJmZ2UqqGluqHuNrv2JdRaw2903DNqma38c7PM9c0rd+5XcHTobYpv6uE4AM8sDbgeucfc24L+Bw4DjgV3Af2QwvOnuDHc/ETgP+FzQfUQmiJlFgfcDvwo26dqfHPT3YAKZ2d8B/cDNwaZdQI27nwB8CbjFzAoyFd80Ndy9Rtf+xLqYN/7nnq79cTDE98xhqw6xLePXv5K7Q7cDmDvo9RygNkOxzBhmFiH9D+5md78DwN13u3vS3VPA/zAJmsSnK3evDX7WA3eSPte7g37qA/3V6zMX4bR3HrDa3XeDrv0MGO5a19+DCWJmHwfeC3zUg0kDgi5RjcHzVcAm4PDMRTn97Odeo2t/gphZFvAXwC8HtunaH3tDfc9kit37ldwduueAxWa2IPjf9IuAezIc07QW9DW/EVjn7t8etL1qULUPAi/vu6+MnpnlBgOMMbNc4J2kz/U9wMeDah8H7s5MhDPCG/7XVtf+hBvuWr8HuMjMYma2AFgMPJuB+KY1M3s38BXg/e7eNWh7eTDREGa2kPT535yZKKen/dxrdO1PnLcD6919x8AGXftja7jvmUyxe39WpgOYqoLZuj4PPACEgR+5+9oMhzXdnQFcCrw0MA0w8DXgYjM7nnRT+OvApzMT3rRXCdyZvveRBdzi7veb2XPAbWZ2ObAN+HAGY5y2zCxOenbewdf3t3Ttjw8z+wVwDlBmZjuAfwC+yRDXuruvNbPbgFdIdxf8XKZnS5vqhjn/fwvEgAeD+9DT7n4V8BbgH82sH0gCV7n7SCcEkX0Mc+7PGepeo2t/7A11/t39Rt483hp07Y+14b5nTql7v5ZCEBERERERmQbULVNERERERGQaUHInIiIiIiIyDSi5ExERERERmQaU3ImIyJgxs/uC6eon8j3nm5kHU4XvN4Z96x7Ce33NzG4YTbwiIiLjRROqiIjMcGbWMehlHOglPfMawKfd/eY37zVm7x0lvS7QfHfvOFD9YY4xH9gCRNy9fwzrngP83N3nHEpcIiIiE01LIYiIzHDunjfw3MxeB65w9z/sW8/Msg6UEB2CtwBrDjWxk7ExTr9bERGZYOqWKSIiQzKzc8xsh5l9xczqgB+bWbGZ/dbMGsysOXg+Z9A+D5vZFcHzT5jZ42b270HdLWZ23j5vcz7wOzO7yMxW7vP+f21m9wTP32Nmz5tZm5ltN7Nv7CfuwTGEg/ffY2abgffsU/eTZrbOzNrNbLOZfTrYngvcB8w2s47gMdvMvmFmPx+0//vNbK2ZtQTvu3RQ2etm9mUze9HMWs3sl2aWPUzMh5nZn8ysMYj1ZjMrGlQ+18zuCM57o5l9f1DZpwZ9hlfM7MRgu5vZokH1fmJm/3cUv9sSM/uxmdUG5XcF2182s/cNqhcJPsPxw/2ORERkfCi5ExGR/ZkFlADzgCtJ/934cfC6BugGvj/s3rAceBUoA74F3GiWXoE6cD5wL3APsMTMFg8q+yvgluB5J/AxoIh0gvYZM/vACOL/FPBe4ARgGXDhPuX1QXkB8EngP83sRHfvBM4Dat09L3jUDt7RzA4nvajwNUA58DvgN0FX0wEfAd4NLACOBT4xTJwG/AswG1gKzAW+EbxPGPgtsBWYD1QDtwZlHw7qfSz4DO8HGkdwXuDgf7c/I91t9yigAvjPYPtPgUsG1Tsf2OXuaxARkQml5E5ERPYnBfyDu/e6e7e7N7r77e7e5e7twD8DZ+9n/63u/j/ungRuAqqASgAzW0h67Nur7t4F3A1cHJQtBo4gnfTh7g+7+0vunnL3F0knVft73wEfAb7j7tvdvYl0ArWXu9/r7ps87RHg98BZIzw3fwnc6+4PunsC+HcgBzh9UJ3/cvfa4L1/AwzZmuXuG4Pj9Lp7A/DtQZ/vFNJJ39+4e6e797j740HZFcC33P254DNsdPetI4x/xL9bM6sinexe5e7N7p4IzhfAz4HzzawgeH0p6URQREQmmJI7ERHZnwZ37xl4YWZxM/uhmW01szbgUaAoaF0aSt3AkyCBAxgY4/ce0q1dA24hSO5It9rdNbCPmS03s4eCLoOtwFWkWwMPZDawfdDrNyQ+ZnaemT1tZk1m1kK61Wkkxx049t7juXsqeK/qQXXqBj3v4s+f/Q3MrMLMbjWzncF5/fmgOOaSTpKHGhM3F9g0wnj3dTC/27lAk7s373uQoEXzCeBDQVfS84Bxm4RHRESGp+RORET2Z98pla8FlgDL3b2A9IQokO5WeLAGumQO+D1QFozVupg/d8kkeH4PMNfdC4HrR/ieu0gnJgNqBp6YWQy4nXSLW6W7F5FONgeOe6DppGtJd2EcOJ4F77VzBHHt61+C9zs2OK+XDIpjO1BjQy/fsB04bJhjdpHuRjlg1j7lB/O73Q6UDB4HuI+bgpg/DDzl7odyDkREZJSU3ImIyMHIJz0Wq8XMSoB/OJSDmFkO6e6GDw9sC1qmfg38G+mxYA/u875N7t5jZqeQbtkbiduAL5rZHDMrBr46qCwKxIAGoD+Y7OWdg8p3A6VmVrifY7/HzN5mZhHSyVEv8OQIYxssH+ggfV6rgb8ZVPYs6ST1m2aWa2bZZnZGUHYD8GUzO8nSFpnZQMK5BvgrS08q824O3I112N+tu+8iPcHMdcHEKxEze8ugfe8CTgSuJj0GT0REMkDJnYiIHIzvkB5Xtgd4Grj/EI/zNtItPD37bL8FeDvwq326IX4W+Eczawe+TjqxGon/AR4AXgBWA3cMFATjyr4YHKuZdMJ4z6Dy9aTH9m0OZsOcPfjA7v4q6daq75E+H+8D3ufufSOMbbD/Qzo5aiXdmjk4zmRw7EXANmAH6fF+uPuvSI+NuwVoJ51klQS7Xh3s1wJ8NCjbnwP9bi8FEsB60hPRXDMoxm7SraALBscuIiITS4uYi4jIhDOz64CX3f26TMciY8PMvg4c7u6XHLCyiIiMCy1iLiIimbCG9OyRMg0E3TgvJ926JyIiGaJumSIiMuHcfUUwjkumODP7FOkJV+5z90czHY+IyEymbpkiIiIiIiLTgFruREREREREpoEpNeaurKzM58+fn+kwREREREREMmLVqlV73L18qLIpldzNnz+flStXZjoMERERERGRjDCzrcOVqVumiIiIiIjINKDkTkREREREZBpQciciIiIiIjINjCi5M7N3m9mrZrbRzL46RPlHzezF4PGkmR03kn3N7AtB2Voz+9boP46IiIiIiMjMdMAJVcwsDPwAeAewA3jOzO5x91cGVdsCnO3uzWZ2HrACWL6/fc3sXOAC4Fh37zWzirH9aCIiIiIiIjPHSGbLPAXY6O6bAczsVtJJ2d7kzt2fHFT/aWDOCPb9DPBNd+8NjlE/uo+SIfd9FepeynQUIiIiIiIylmYdA+d9M9NRHJSRdMusBrYPer0j2Dacy4H7RrDv4cBZZvaMmT1iZicPdTAzu9LMVprZyoaGhhGEKyIiIiIiMvOMpOXOhtjmQ1ZMd7W8HDhzBPtmAcXAqcDJwG1mttDd33Bsd19Bupsny5YtG/J9M2qKZfMiIiIiIjI9jaTlbgcwd9DrOUDtvpXM7FjgBuACd28cwb47gDs87VkgBZQdXPgiIiIiIiICI0vungMWm9kCM4sCFwH3DK5gZjXAHcCl7v7aCPe9C3hrsP/hQBTYM5oPIyIiIiIiMlMdsFumu/eb2eeBB4Aw8CN3X2tmVwXl1wNfB0qB68wMoN/dlw23b3DoHwH/v717j7XsqusA/v1lRkh8IK8Ba6eVSoo6koLkpCY+QESwJcggiaaVkCoNtYZG0RgpkoBKTBBRYyLYVGiCCVAxlFBNoNRH9I9a7S3y6AAtQyl0aNMOj4gGtQ78/OPuxst4bu+ZR2fPXffzSW7O2Wuvdc7aK+vuc75n733ONVV1W5IHklxy9CmZAAAArKa2U55aLBa9trY2dzcAAABmUVW3dvdi2bqVfsQcAACA05twBwAAMADhDgAAYADCHQAAwACEOwAAgAEIdwAAAAMQ7gAAAAYg3AEAAAxAuAMAABiAcAcAADAA4Q4AAGAAwh0AAMAAhDsAAIABCHcAAAADEO4AAAAGINwBAAAMQLgDAAAYgHAHAAAwAOEOAABgAMIdAADAAIQ7AACAAQh3AAAAAxDuAAAABiDcAQAADGClcFdVF1TV7VV1sKquXLL+JVX10envpqp62jG0/fWq6qp6/IltCgAAwM61Zbirql1J3pzkwiT7klxcVfuOqvaZJM/q7vOSvD7J1au0raqzkjw3yedOfFMAAAB2rlWO3J2f5GB339ndDyS5Nsn+jRW6+6bu/vK0eHOSvSu2/aMkv5GkT2AbAAAAdrxVwt2ZSe7esHxoKtvMpUnev1Xbqnphks9390ce6smr6rKqWquqtcOHD6/QXQAAgJ1n9wp1aknZ0iNtVfXsrIe7H3motlX1zUlek+R5Wz15d1+d6TTPxWLhCB8AAMASqxy5O5TkrA3Le5Pcc3SlqjovyVuT7O/uL27R9slJzknykaq6ayr/UFV9x7FuAAAAAKsdubslyblVdU6Szye5KMnPbaxQVWcnuS7JS7v7jq3adveBJE/Y0P6uJIvu/sIJbAsAAMCOtWW46+4jVXVFkhuS7EpyTXcfqKrLp/VXJXltkscleUtVJcmR7l5s1vZh2hYAAIAdq7q3z2Vsi8Wi19bW5u4GAADALKrq1u5eLFu30o+YAwAAcHoT7gAAAAYg3AEAAAxAuAMAABiAcAcAADAA4Q4AAGAAwh0AAMAAhDsAAIABCHcAAAADEO4AAAAGINwBAAAMQLgDAAAYgHAHAAAwAOEOAABgAMIdAADAAIQ7AACAAQh3AAAAAxDuAAAABiDcAQAADEC4AwAAGIBwBwAAMADhDgAAYADCHQAAwACEOwAAgAEIdwAAAANYKdxV1QVVdXtVHayqK5esf0lVfXT6u6mqnrZV26r6/ar65NTmvVX16JOzSQAAADvPluGuqnYleXOSC5PsS3JxVe07qtpnkjyru89L8vokV6/Q9sYkT53a3JHk1Se+OQAAADvTKkfuzk9ysLvv7O4HklybZP/GCt19U3d/eVq8Ocnerdp29we7+8iSNgAAAByjVcLdmUnu3rB8aCrbzKVJ3n+MbV+2oc03qKrLqmqtqtYOHz68QncBAAB2nlXCXS0p66UVq56d9XD3qlXbVtVrkhxJ8o5lj9ndV3f3orsXe/bsWaG7AAAAO8/uFeocSnLWhuW9Se45ulJVnZfkrUku7O4vrtK2qi5J8oIkz+nupYERAACAra1y5O6WJOdW1TlV9YgkFyW5fmOFqjo7yXVJXtrdd6zStqouyPoRvhd291dPfFMAAAB2ri2P3HX3kaq6IskNSXYluaa7D1TV5dP6q5K8NsnjkrylqpLkyHQq5dK200P/SZJHJrlxanNzd19+cjcPAABgZ6jtdDbkYrHotbW1ubsBAAAwi6q6tbsXy9at9CPmAAAAnN6EOwAAgAEIdwAAAAMQ7gAAAAYg3AEAAAxAuAMAABiAcAcAADAA4Q4AAGAAwh0AAMAAhDsAAIABCHcAAAADEO4AAAAGINwBAAAMQLgDAAAYgHAHAAAwAOEOAABgAMIdAADAAIQ7AACAAQh3AAAAAxDuAAAABiDcAQAADEC4AwAAGIBwBwAAMADhDgAAYADCHQAAwABWCndVdUFV3V5VB6vqyiXrX1JVH53+bqqqp23VtqoeW1U3VtWnptvHnJxNAgAA2Hm2DHdVtSvJm5NcmGRfkourat9R1T6T5FndfV6S1ye5eoW2Vyb52+4+N8nfTssAAAAch1WO3J2f5GB339ndDyS5H7/57gAACjRJREFUNsn+jRW6+6bu/vK0eHOSvSu03Z/k7dP9tyd50fFvBgAAwM62Srg7M8ndG5YPTWWbuTTJ+1do+8TuvjdJptsnLHuwqrqsqtaqau3w4cMrdBcAAGDnWSXc1ZKyXlqx6tlZD3evOta2m+nuq7t70d2LPXv2HEtTAACAHWOVcHcoyVkblvcmuefoSlV1XpK3Jtnf3V9coe19VXXG1PaMJPcfW9cBAAB40Crh7pYk51bVOVX1iCQXJbl+Y4WqOjvJdUle2t13rNj2+iSXTPcvSfK+498MAACAnW33VhW6+0hVXZHkhiS7klzT3Qeq6vJp/VVJXpvkcUneUlVJcmQ6lXJp2+mh35Dk3VV1aZLPJfmZk7xtAAAAO0Z1H9MlcLNaLBa9trY2dzcAAABmUVW3dvdi2bqVfsQcAACA05twBwAAMADhDgAAYADCHQAAwACEOwAAgAEIdwAAAAMQ7gAAAAYg3AEAAAxAuAMAABiAcAcAADAA4Q4AAGAAwh0AAMAAhDsAAIABCHcAAAADEO4AAAAGINwBAAAMQLgDAAAYgHAHAAAwAOEOAABgAMIdAADAAIQ7AACAAQh3AAAAAxDuAAAABiDcAQAADGClcFdVF1TV7VV1sKquXLL+e6vqn6rqv6vq149a9ytVdVtVHaiqV24of3pV3VxVH66qtao6/8Q3BwAAYGfaMtxV1a4kb05yYZJ9SS6uqn1HVftSkl9O8qaj2j41ycuTnJ/kaUleUFXnTqvfmOS3u/vpSV47LQMAAHAcVjlyd36Sg919Z3c/kOTaJPs3Vuju+7v7liT/c1Tb70tyc3d/tbuPJPmHJD/9YLMkj5ruf3uSe45zGwAAAHa83SvUOTPJ3RuWDyX5wRUf/7Ykv1tVj0vyn0men2RtWvfKJDdU1ZuyHjJ/aNkDVNVlSS5LkrPPPnvFpwUAANhZVjlyV0vKepUH7+5PJPm9JDcm+UCSjyQ5Mq3+pSS/2t1nJfnVJG/b5DGu7u5Fdy/27NmzytMCAADsOKuEu0NJztqwvDfHcApld7+tu5/R3c/M+rV5n5pWXZLkuun+X2b99E8AAACOwyrh7pYk51bVOVX1iCQXJbl+1SeoqidMt2cneXGSd02r7knyrOn+j+f/Qh8AAADHaMtr7rr7SFVdkeSGJLuSXNPdB6rq8mn9VVX1HVm/lu5RSb4+/eTBvu7+SpL3TNfc/U+SV3T3l6eHfnmSP66q3Un+K9N1dQAAABy76l7p8rnTwmKx6LW1ta0rAgAADKiqbu3uxbJ1K/2IOQAAAKc34Q4AAGAAwh0AAMAAhDsAAIABCHcAAAADEO4AAAAGINwBAAAMQLgDAAAYgHAHAAAwAOEOAABgAMIdAADAAIQ7AACAAQh3AAAAAxDuAAAABiDcAQAADEC4AwAAGMDuuTuw3f32Xx3Ix+/5ytzdAAAATqJ93/movO6nvn/ubhwTR+4AAAAG4MjdCdpuaR4AABiTI3cAAAADEO4AAAAGINwBAAAMQLgDAAAYgHAHAAAwAOEOAABgANXdc/dhZVV1OMln5+7HEo9P8oW5O7GDGf/5GPt5Gf/5GPt5Gf95Gf/5GPt5nS7j/13dvWfZim0V7k5XVbXW3Yu5+7FTGf/5GPt5Gf/5GPt5Gf95Gf/5GPt5bYfxd1omAADAAIQ7AACAAQh3J8fVc3dghzP+8zH28zL+8zH28zL+8zL+8zH28zrtx981dwAAAANw5A4AAGAAwh0AAMAAhLsTUFUXVNXtVXWwqq6cuz+jq6qzqurvq+oTVXWgqn5lKv+tqvp8VX14+nv+3H0dVVXdVVUfm8Z5bSp7bFXdWFWfmm4fM3c/R1NV37Nhfn+4qr5SVa809x8+VXVNVd1fVbdtKNt0rlfVq6fXgtur6ifn6fU4Nhn/36+qT1bVR6vqvVX16Kn8SVX1nxv+D66ar+fb3yZjv+m+xtw/uTYZ/7/YMPZ3VdWHp3Jz/yR6iPeZ22rf75q741RVu5LckeS5SQ4luSXJxd398Vk7NrCqOiPJGd39oar6tiS3JnlRkp9N8h/d/aZZO7gDVNVdSRbd/YUNZW9M8qXufsP0IcdjuvtVc/VxdNO+5/NJfjDJL8Tcf1hU1TOT/EeSP+/up05lS+d6Ve1L8q4k5yf5ziR/k+Qp3f21mbq/7W0y/s9L8nfdfaSqfi9JpvF/UpK/frAeJ2aTsf+tLNnXmPsn37LxP2r9HyT5t+7+HXP/5HqI95k/n22073fk7vidn+Rgd9/Z3Q8kuTbJ/pn7NLTuvre7PzTd//ckn0hy5ry9Iuvz/u3T/bdnfUfIw+c5ST7d3Z+duyMj6+5/TPKlo4o3m+v7k1zb3f/d3Z9JcjDrrxEcp2Xj390f7O4j0+LNSfae8o7tAJvM/c2Y+yfZQ41/VVXWP9B+1ynt1A7xEO8zt9W+X7g7fmcmuXvD8qEIGqfM9GnVDyT556noiulUnWucFviw6iQfrKpbq+qyqeyJ3X1vsr5jTPKE2Xq3M1yUb3xhN/dPnc3muteDU+9lSd6/YfmcqvrXqvqHqvrRuTo1uGX7GnP/1PrRJPd196c2lJn7D4Oj3mduq32/cHf8akmZc1xPgar61iTvSfLK7v5Kkj9N8uQkT09yb5I/mLF7o/vh7n5GkguTvGI6fYRTpKoekeSFSf5yKjL3Tw9eD06hqnpNkiNJ3jEV3Zvk7O7+gSS/luSdVfWoufo3qM32Neb+qXVxvvHDPXP/YbDkfeamVZeUzT7/hbvjdyjJWRuW9ya5Z6a+7BhV9U1Z/4d7R3dflyTdfV93f627v57kz3IaHBIfVXffM93en+S9WR/r+6bz1B88X/3++Xo4vAuTfKi770vM/RlsNte9HpwiVXVJkhckeUlPXxownRL1xen+rUk+neQp8/VyPA+xrzH3T5Gq2p3kxUn+4sEyc//kW/Y+M9ts3y/cHb9bkpxbVedMn6ZflOT6mfs0tOlc87cl+UR3/+GG8jM2VPvpJLcd3ZYTV1XfMl1gnKr6liTPy/pYX5/kkqnaJUneN08Pd4Rv+NTW3D/lNpvr1ye5qKoeWVXnJDk3yb/M0L+hVdUFSV6V5IXd/dUN5XumLxpKVX131sf/znl6OaaH2NeY+6fOTyT5ZHcferDA3D+5NnufmW227989dwe2q+nbuq5IckOSXUmu6e4DM3drdD+c5KVJPvbg1wAn+c0kF1fV07N+KPyuJL84T/eG98Qk713f92V3knd29weq6pYk766qS5N8LsnPzNjHYVXVN2f923k3zu83mvsPj6p6V5IfS/L4qjqU5HVJ3pAlc727D1TVu5N8POunC75i7m9L2+42Gf9XJ3lkkhun/dDN3X15kmcm+Z2qOpLka0ku7+5VvxCEo2wy9j+2bF9j7p98y8a/u9+W/3+9dWLun2ybvc/cVvt+P4UAAAAwAKdlAgAADEC4AwAAGIBwBwAAMADhDgAAYADCHQAAwACEOwAAgAEIdwAAAAP4X2Fb4fe38kquAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.182000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
