{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFOuPMVjaBxi"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P59NYU98GCb9",
        "outputId": "07969b43-e398-4cc5-da5d-a2a50b5f76ae"
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.5 MB 23 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 0.4.1 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 0.4.1 which is incompatible.\n",
            "fastai 1.0.61 requires torch>=1.0.0, but you have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 24 kB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 0.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 5.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "8309c8f3-11b0-4597-8ddf-85068ff8404c"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "60118a4c-4e04-4a6b-f865-00e166daef9c"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "5950f666-ed84-4cc5-e870-bfc0e09a6e1c"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "18558419-09f7-4655-eaec-db5b7e6ffb83"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in train = 45441. Tags = {'ADP', 'DET', 'ADV', 'VERB', 'NUM', 'PRT', 'X', 'CONJ', 'ADJ', 'NOUN', '.', 'PRON'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "URC1B2nvPGFt",
        "outputId": "0846a6f9-e8a5-40f3-ebed-1d615c0b8664"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWmSToIaeAo",
        "outputId": "ea2bbfda-8cf8-478a-e4cd-54fbeac97b0b"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz_Rk0bbMyH",
        "outputId": "76008f7e-ce9b-4f31-d299-aa141dac23d7"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XCuxEBVbOY_",
        "outputId": "4a4ce1e9-a0f5-4505-a12a-f3e42a55cfb9"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XsRII5kW5x",
        "outputId": "ac6ac985-c5f8-41a9-c211-c7cc878236f2"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24, 4), (24, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # создание эмбеддингов из слов\n",
        "        self.embedding = nn.Embedding(vocab_size, # размер словаря\n",
        "                                      word_emb_dim) # желаемая размерность\n",
        "        # эмбеддинги с учётом контекста\n",
        "        self.lstm = nn.LSTM(input_size = word_emb_dim, # входная размерность - длина эмбеддинга слова\n",
        "                            hidden_size = lstm_hidden_dim, # размер скрытых слоёв задаётся пользователем\n",
        "                            num_layers = lstm_layers_count, # количество скрытых слоёв (meme STACK MORE LAYERS :))\n",
        "                            bias = True) # веса смещений\n",
        "        # предсказание тегов по готовым эмбеддингам\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, # входной размер соответствует LSTM\n",
        "                                tagset_size, # количество классов, которое надо предсказать\n",
        "                                bias=False) # без весов смещений\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_out = self.embedding(inputs) # превращаем входные данные в эмбеддинги\n",
        "        lstm_out, _ = self.lstm.forward(embedding_out) # эмбеддинги превращаем в веса\n",
        "        linear_out = self.linear.forward(lstm_out) # делаем предсказания по весам\n",
        "        return linear_out # предсказания (вероятности)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbrxsZ2mehWB",
        "outputId": "7372dd10-6fa9-4164-f966-b5bb038c4886"
      },
      "source": [
        "# разработанная модель\n",
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind), # количество разных слов\n",
        "    tagset_size=len(tag2ind) # количество разных классов\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch) # входные данные (получены выше)\n",
        "\n",
        "logits = model(X_batch) # вероятности, предсказаные моделью\n",
        "\n",
        "predicted = torch.argmax(logits, dim=-1) # выбираем самые вероятные классы\n",
        "\n",
        "correct_samples_num = float(torch.sum(predicted[y_batch!=0] == y_batch[y_batch!=0])) # количество верно предсказанных сэмплов\n",
        "print('accuracy = {:.3f}'.format(correct_samples_num / (y_batch[y_batch!=0].shape[0]))) # доля правильно предсказанных"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMUyUm1hgpe3",
        "outputId": "4f7ab875-6e9b-4825-ae4c-0a907fdd3c78"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.view(-1, logits.size(-1)), y_batch.view(-1)) # плоские списки - для каждого ответа"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5794, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm # прогресс-бар\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch) # предсказания для батча\n",
        "\n",
        "                # считаем CrossEntropy для двух плоских списков\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1) # наиболее вероятные ответы\n",
        "                #  количество верно предсказанных сэмплов\n",
        "                correct_samples_num = float(torch.sum(preds[y_batch!=0] == y_batch[y_batch!=0]))\n",
        "                # количество правильно предсказанных правильно предсказанных\n",
        "                # и общее количество предсказаний на этом шаге\n",
        "                cur_correct_count, cur_sum_count = correct_samples_num, y_batch[y_batch!=0].shape[0]\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqfbeh1ltEYa",
        "outputId": "88328e18-a0f9-49fc-9383-2970f328d7c8"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10, # уменьшил эпохи с 50 до 10\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 10] Train: Loss = 0.32319, Accuracy = 71.75%: 100%|██████████| 572/572 [00:15<00:00, 37.20it/s]\n",
            "[1 / 10]   Val: Loss = 0.10436, Accuracy = 84.88%: 100%|██████████| 13/13 [00:00<00:00, 25.17it/s]\n",
            "[2 / 10] Train: Loss = 0.10237, Accuracy = 89.86%: 100%|██████████| 572/572 [00:15<00:00, 38.09it/s]\n",
            "[2 / 10]   Val: Loss = 0.07956, Accuracy = 89.29%: 100%|██████████| 13/13 [00:00<00:00, 27.04it/s]\n",
            "[3 / 10] Train: Loss = 0.06794, Accuracy = 93.18%: 100%|██████████| 572/572 [00:15<00:00, 37.98it/s]\n",
            "[3 / 10]   Val: Loss = 0.06598, Accuracy = 91.05%: 100%|██████████| 13/13 [00:00<00:00, 25.82it/s]\n",
            "[4 / 10] Train: Loss = 0.05119, Accuracy = 94.79%: 100%|██████████| 572/572 [00:15<00:00, 37.97it/s]\n",
            "[4 / 10]   Val: Loss = 0.06350, Accuracy = 91.99%: 100%|██████████| 13/13 [00:00<00:00, 25.34it/s]\n",
            "[5 / 10] Train: Loss = 0.04109, Accuracy = 95.81%: 100%|██████████| 572/572 [00:15<00:00, 37.99it/s]\n",
            "[5 / 10]   Val: Loss = 0.06322, Accuracy = 92.53%: 100%|██████████| 13/13 [00:00<00:00, 24.98it/s]\n",
            "[6 / 10] Train: Loss = 0.03292, Accuracy = 96.59%: 100%|██████████| 572/572 [00:15<00:00, 37.77it/s]\n",
            "[6 / 10]   Val: Loss = 0.06076, Accuracy = 92.90%: 100%|██████████| 13/13 [00:00<00:00, 25.58it/s]\n",
            "[7 / 10] Train: Loss = 0.02729, Accuracy = 97.17%: 100%|██████████| 572/572 [00:15<00:00, 37.75it/s]\n",
            "[7 / 10]   Val: Loss = 0.06196, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 25.89it/s]\n",
            "[8 / 10] Train: Loss = 0.02232, Accuracy = 97.65%: 100%|██████████| 572/572 [00:15<00:00, 37.83it/s]\n",
            "[8 / 10]   Val: Loss = 0.06319, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 25.68it/s]\n",
            "[9 / 10] Train: Loss = 0.01883, Accuracy = 98.03%: 100%|██████████| 572/572 [00:14<00:00, 38.19it/s]\n",
            "[9 / 10]   Val: Loss = 0.06245, Accuracy = 93.27%: 100%|██████████| 13/13 [00:00<00:00, 24.45it/s]\n",
            "[10 / 10] Train: Loss = 0.01566, Accuracy = 98.38%: 100%|██████████| 572/572 [00:14<00:00, 38.26it/s]\n",
            "[10 / 10]   Val: Loss = 0.07171, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 25.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98wr38_rw55D",
        "outputId": "c0aedb3a-e3be-4fb4-c07e-c68a22bbe2ad"
      },
      "source": [
        "correct_count = 0 # количество правильных\n",
        "sum_count = 0 # общее количество\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64): # для каждого батча\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch) # приводим к тензору\n",
        "    model.to('cuda') # перенос на GPU, чтобы данные и модель были в одном месте\n",
        "    logits = model(X_batch) # делаем предсказание моделью\n",
        "    preds = torch.argmax(logits, dim=-1) # наиболее вероятные классы\n",
        "\n",
        "    mask = (y_batch != 0).float() # выбираем только те, где класс не 0\n",
        "    # определеям правильный класс, но учитываем только те, которые подходят по маске\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask)\n",
        "    # суммируем количество правильных ответов (и убираем инфу про device)\n",
        "    cur_correct_count = cur_correct_count.sum().item()\n",
        "    cur_sum_count = mask.sum().item() # общее количество ответов, совпадающих по маске\n",
        "                \n",
        "    correct_count += cur_correct_count # добавляем к общему количеству правильных ответов\n",
        "    sum_count += cur_sum_count # добавляем к общему количеству ответов\n",
        "\n",
        "correct_count / sum_count"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9334490083279111"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr0oY4guAaOH"
      },
      "source": [
        "Добавил Bidirectional LSTM в виде аргумента в конструкторе класса и сделал условие на количество входных узлов в полносвязном уровне"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FISw21nkaBx5"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1,\n",
        "                 bidirectional=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        # создание эмбеддингов из слов\n",
        "        self.embedding = nn.Embedding(vocab_size, # размер словаря\n",
        "                                      word_emb_dim) # желаемая размерность\n",
        "        # эмбеддинги с учётом контекста\n",
        "        self.lstm = nn.LSTM(input_size = word_emb_dim, # входная размерность - длина эмбеддинга слова\n",
        "                            hidden_size = lstm_hidden_dim, # размер скрытых слоёв задаётся пользователем\n",
        "                            num_layers = lstm_layers_count, # количество скрытых слоёв (meme STACK MORE LAYERS :))\n",
        "                            bias = True, # веса смещений\n",
        "                            bidirectional = bidirectional) # смотреть не только назад, но и вперёд\n",
        "        \n",
        "        if bidirectional: # если, по сути, две модели (матрицы)\n",
        "            lstm_hidden_dim *= 2 # то выходных весов будет в два раз больше\n",
        "        \n",
        "        # предсказание тегов по готовым эмбеддингам\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, # входной размер соответствует LSTM\n",
        "                                tagset_size, # количество классов, которое надо предсказать\n",
        "                                bias=False) # без весов смещений\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_out = self.embedding(inputs) # превращаем входные данные в эмбеддинги\n",
        "        lstm_out, _ = self.lstm.forward(embedding_out) # эмбеддинги превращаем в веса\n",
        "        linear_out = self.linear.forward(lstm_out) # делаем предсказания по весам\n",
        "        return linear_out # предсказания (вероятности)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZpY_Q1xZ18h"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsCstxiO03oT",
        "outputId": "9f46de96-182f-4742-c0fe-5f63b43cddbb"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items(): # берём слово и его индекс\n",
        "    word = word.lower() # приводим к нижнему регистру\n",
        "    if word in w2v_model.vocab: # для этого же слова в большой модели\n",
        "        embeddings[ind] = w2v_model.get_vector(word) # запоминаем эмбеддинг слова\n",
        "        known_count += 1 # и увеличиваем количество слов с известными векторами\n",
        "\n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1, bidirectional=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings) # слой эмбеддингов\n",
        "        # LSTM \n",
        "        # эмбеддинги с учётом контекста\n",
        "        self.lstm = nn.LSTM(input_size = embeddings.shape[1], # входная размерность - длина эмбеддинга слова\n",
        "                            hidden_size = lstm_hidden_dim, # размер скрытых слоёв задаётся пользователем\n",
        "                            num_layers = lstm_layers_count, # количество скрытых слоёв (meme STACK MORE LAYERS :))\n",
        "                            bias = True, # веса смещений\n",
        "                            bidirectional = bidirectional) # смотреть не только назад, но и вперёд\n",
        "        \n",
        "        if bidirectional: # если, по сути, две модели (матрицы)\n",
        "            lstm_hidden_dim *= 2 # то выходных весов будет в два раз больше\n",
        "\n",
        "        # предсказание тегов по готовым эмбеддингам\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, # входной размер соответствует LSTM\n",
        "                                tagset_size, # количество классов, которое надо предсказать\n",
        "                                bias=False) # без весов смещений\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_out = self.embedding(inputs) # превращаем входные данные в эмбеддинги\n",
        "        lstm_out, _ = self.lstm.forward(embedding_out) # эмбеддинги превращаем в веса\n",
        "        linear_out = self.linear.forward(lstm_out) # делаем предсказания по весам\n",
        "        return linear_out # предсказания (вероятности)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBtI6BDE-Fc7",
        "outputId": "a03d1080-c0f7-47ea-f281-fe628ec5451d"
      },
      "source": [
        "embeddings = torch.Tensor(embeddings) # дополнил, иначе модель не могла получить dim\n",
        "\n",
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "model.cuda() # перенос на GPU\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# уменьшил количество эпох с 50 до 10\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 10] Train: Loss = 0.78078, Accuracy = 76.79%: 100%|██████████| 572/572 [00:11<00:00, 49.49it/s]\n",
            "[1 / 10]   Val: Loss = 0.37385, Accuracy = 89.18%: 100%|██████████| 13/13 [00:00<00:00, 25.87it/s]\n",
            "[2 / 10] Train: Loss = 0.28650, Accuracy = 91.37%: 100%|██████████| 572/572 [00:11<00:00, 48.98it/s]\n",
            "[2 / 10]   Val: Loss = 0.26018, Accuracy = 92.00%: 100%|██████████| 13/13 [00:00<00:00, 25.99it/s]\n",
            "[3 / 10] Train: Loss = 0.21124, Accuracy = 93.38%: 100%|██████████| 572/572 [00:11<00:00, 48.50it/s]\n",
            "[3 / 10]   Val: Loss = 0.21190, Accuracy = 93.31%: 100%|██████████| 13/13 [00:00<00:00, 25.49it/s]\n",
            "[4 / 10] Train: Loss = 0.17545, Accuracy = 94.38%: 100%|██████████| 572/572 [00:11<00:00, 49.14it/s]\n",
            "[4 / 10]   Val: Loss = 0.18803, Accuracy = 93.95%: 100%|██████████| 13/13 [00:00<00:00, 26.43it/s]\n",
            "[5 / 10] Train: Loss = 0.15430, Accuracy = 94.99%: 100%|██████████| 572/572 [00:11<00:00, 48.69it/s]\n",
            "[5 / 10]   Val: Loss = 0.17251, Accuracy = 94.34%: 100%|██████████| 13/13 [00:00<00:00, 25.18it/s]\n",
            "[6 / 10] Train: Loss = 0.14022, Accuracy = 95.39%: 100%|██████████| 572/572 [00:11<00:00, 49.59it/s]\n",
            "[6 / 10]   Val: Loss = 0.16220, Accuracy = 94.72%: 100%|██████████| 13/13 [00:00<00:00, 25.31it/s]\n",
            "[7 / 10] Train: Loss = 0.13026, Accuracy = 95.67%: 100%|██████████| 572/572 [00:11<00:00, 47.99it/s]\n",
            "[7 / 10]   Val: Loss = 0.15462, Accuracy = 94.83%: 100%|██████████| 13/13 [00:00<00:00, 22.92it/s]\n",
            "[8 / 10] Train: Loss = 0.12273, Accuracy = 95.88%: 100%|██████████| 572/572 [00:11<00:00, 48.79it/s]\n",
            "[8 / 10]   Val: Loss = 0.15056, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 24.28it/s]\n",
            "[9 / 10] Train: Loss = 0.11668, Accuracy = 96.05%: 100%|██████████| 572/572 [00:11<00:00, 48.46it/s]\n",
            "[9 / 10]   Val: Loss = 0.14664, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 25.67it/s]\n",
            "[10 / 10] Train: Loss = 0.11212, Accuracy = 96.18%: 100%|██████████| 572/572 [00:11<00:00, 47.98it/s]\n",
            "[10 / 10]   Val: Loss = 0.14389, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 24.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUuAPGhEGVR",
        "outputId": "fe6bdc98-d1f3-4f91-f644-5af49e8526f9"
      },
      "source": [
        "loss, accuracy = do_epoch(model, criterion, (X_test, y_test), 512, optimizer=None, name='test')"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " test Loss = 0.14375, Accuracy = 95.19%: 100%|██████████| 28/28 [00:01<00:00, 26.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI0EIGvmRHxy"
      },
      "source": [
        "\"Добейтесь качества лучше прошлых моделей.\" - это когда-нибудь потом..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1q7ullTRCI8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}